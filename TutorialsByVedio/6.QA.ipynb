{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五章 基于文档的问答\n",
    "\n",
    " - [一、导入embedding模型和向量存储组件](#一、导入embedding模型和向量存储组件)\n",
    "     - [1.1 创建向量存储](#1.1-创建向量存储)\n",
    "     - [1.2 使用语言模型与文档结合使用](#1.2-使用语言模型与文档结合使用)\n",
    " - [二、 如何回答我们文档的相关问题](#二、-如何回答我们文档的相关问题)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "d:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "d:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache=None verbose=False callbacks=None callback_manager=None tags=None metadata=None client=<class 'openai.api_resources.chat_completion.ChatCompletion'> model_name='gpt-3.5-turbo' temperature=0.2 model_kwargs={} openai_api_key='sk-S0TKP0Sxk8H8VirL7a912cAf06314e46AfB0348eD40183B5' openai_api_base='https://openkey.cloud/v1' openai_organization='' openai_proxy='' request_timeout=None max_retries=6 streaming=False n=1 max_tokens=None tiktoken_model_name=None\n"
     ]
    }
   ],
   "source": [
    "# set the init envs \n",
    "import os, sys\n",
    "import openai\n",
    "import langchain\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) \n",
    "\n",
    "\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"] # 换成代理，一定要加v1\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=0.8,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "chat = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    ")\n",
    "# os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "# os.environ[\"WANDB_PROJECT\"] = \"langchainPlayGround\"\n",
    "\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    temperature=0.2\n",
    "    )\n",
    "print(chat)\n",
    "\n",
    "# llm = OpenAI(\n",
    "#     model_name = \"gpt-3.5-turbo\",\n",
    "#     temperature = 0.0\n",
    "# )\n",
    "# print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 文本向量表征模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.022206753545421832, 0.006513645422377796, -0.018233178227007405, -0.039207618777269276, -0.014360198785407882]\n"
     ]
    }
   ],
   "source": [
    "#使用OpenAIEmbedding类\n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "\n",
    "embedding_model = OpenAIEmbeddings() #初始化\n",
    "\n",
    "\n",
    "#因为文档比较短了，\n",
    "#所以这里不需要进行任何分块,\n",
    "#可以直接进行向量表征\n",
    "#使用初始化OpenAIEmbedding实例上的查询方法embed_query为文本创建向量表征\n",
    "embed = embedding_model.embed_query(\"Hi my name is Harrison\")\n",
    "\n",
    "#查看得到向量表征的长度\n",
    "print(len(embed))\n",
    "\n",
    "#每个元素都是不同的数字值，组合起来就是文本的向量表征\n",
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import some package and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA #检索QA链，在文档上进行检索\n",
    "from langchain.chat_models import ChatOpenAI #openai模型\n",
    "from langchain.document_loaders import CSVLoader #文档加载器，采用csv格式存储\n",
    "from langchain.vectorstores import DocArrayInMemorySearch #向量存储\n",
    "from IPython.display import display, Markdown #在jupyter显示信息的工具\n",
    "\n",
    "llm = chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Women's Campside Oxfords</td>\n",
       "      <td>This ultracomfortable lace-to-toe Oxford boast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recycled Waterhog Dog Mat, Chevron Weave</td>\n",
       "      <td>Protect your floors from spills and splashing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Infant and Toddler Girls' Coastal Chill Swimsu...</td>\n",
       "      <td>She'll love the bright colors, ruffles and exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Refresh Swimwear, V-Neck Tankini Contrasts</td>\n",
       "      <td>Whether you're going for a swim or heading out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EcoFlex 3L Storm Pants</td>\n",
       "      <td>Our new TEK O2 technology makes our four-seaso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                           Women's Campside Oxfords   \n",
       "1           Recycled Waterhog Dog Mat, Chevron Weave   \n",
       "2  Infant and Toddler Girls' Coastal Chill Swimsu...   \n",
       "3         Refresh Swimwear, V-Neck Tankini Contrasts   \n",
       "4                             EcoFlex 3L Storm Pants   \n",
       "\n",
       "                                         description  \n",
       "0  This ultracomfortable lace-to-toe Oxford boast...  \n",
       "1  Protect your floors from spills and splashing ...  \n",
       "2  She'll love the bright colors, ruffles and exc...  \n",
       "3  Whether you're going for a swim or heading out...  \n",
       "4  Our new TEK O2 technology makes our four-seaso...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#用Pandas导入并查看数据\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/OutdoorClothingCatalog_1000.csv',usecols=[1, 2])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 基本文档加载器创建向量存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorstore=<langchain.vectorstores.docarray.in_memory.DocArrayInMemorySearch object at 0x0000029BCC0C9F60>\n"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator #导入向量存储索引创建器\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path='./data/OutdoorClothingCatalog_1000.csv',\n",
    "    encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "# 创建指定向量存储类\n",
    "# 创建完成后，从加载器中调用, 通过文档加载器列表加载\n",
    "vector_storage = VectorstoreIndexCreator(vectorstore_cls=\n",
    "                                DocArrayInMemorySearch\n",
    "                                ).from_loaders([loader])\n",
    "\n",
    "print(vector_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 查询创建的向量存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "| Name | Description |\n",
       "| --- | --- |\n",
       "| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated, 100% polyester, wrinkle-resistant, front and back cape venting, two front bellows pockets |\n",
       "| Men's Plaid Tropic Shirt, Short-Sleeve | UPF 50+ rated, 52% polyester and 48% nylon, machine washable and dryable, front and back cape venting, two front bellows pockets |\n",
       "| Men's TropicVibe Shirt, Short-Sleeve | UPF 50+ rated, 71% Nylon, 29% Polyester, 100% Polyester knit mesh, machine wash and dry, front and back cape venting, two front bellows pockets |\n",
       "| Sun Shield Shirt by | UPF 50+ rated, 78% nylon, 22% Lycra Xtra Life fiber, handwash, line dry, wicks moisture, fits comfortably over swimsuit, abrasion resistant |\n",
       "\n",
       "All four shirts provide UPF 50+ sun protection, blocking 98% of the sun's harmful rays. The Men's Tropical Plaid Short-Sleeve Shirt is made of 100% polyester and is wrinkle-resistant"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_query =\"Please list all your shirts with sun protection \\\n",
    "in a table in markdown and summarize each one.\"\n",
    "\n",
    "\n",
    "#使用索引查询创建一个响应，并传入这个查询\n",
    "response = vector_storage.query(input_query)\n",
    "\n",
    "#查看查询返回的内容\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、 结合表征模型和向量存储\n",
    "\n",
    "使用语言模型与文档结合使用时，语言模型一次只能使用几千个单词的信息。如果我们文档比较长，如何让语言模型回答关于其中所有内容的问题呢？我们通过几个通过向量表征（Embeddings）和向量存储(Vector Store)实现\n",
    "\n",
    "文本表征（Embeddings)是对文本语义的向量表征，相似内容的文本具有相似的表征向量。这使我们可以在向量空间中比较文本的相似性。\n",
    "\n",
    "向量数据库(Vector Database)用来存储文档的文本块。对于给定的文档，我们首先将其分成较小的文本块（chunks），然后获取每个小文本块的文本表征，并将这些表征储存在向量数据库中。这个流程正是创建索引(index)的过程。将文档分成小文本快的原因在于我们可能无法将整个文档传入语言模型进行处理。\n",
    "\n",
    "索引创建完成后，我们可以使用索引来查找与传入的查询(Query)最相关的文本片段 - 首先我们为查询获取向量表征，然后我们将其与向量数据库中的所有向量进行比较并选择最相似的n个文本块，最后我们将这些相似的文本块构建提示，并输入到语言模型，从而得到最终答案。\n",
    "\n",
    "### 2.1 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\", metadata={'source': './data/OutdoorClothingCatalog_1000.csv', 'row': 0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#创建一个文档加载器，通过csv格式加载\n",
    "loader = CSVLoader(\n",
    "    file_path='./data/OutdoorClothingCatalog_1000.csv',\n",
    "    encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "#查看单个文档，每个文档对应于CSV中的一行数据\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 基于向量表征创建向量存储并进行查询\n",
    "\n",
    "### 2.4 查询创建的向量存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "page_content=': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\'t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.' metadata={'source': './data/OutdoorClothingCatalog_1000.csv', 'row': 255}\n"
     ]
    }
   ],
   "source": [
    "# import langchain\n",
    "# (langchain.__version__)\n",
    "\n",
    "# 将刚才创建文本向量表征(embeddings)存储在向量存储(vector store)中\n",
    "# 使用DocArrayInMemorySearch类的from_documents方法来实现\n",
    "# 该方法接受文档列表以及向量表征模型作为输入\n",
    "vector_db = DocArrayInMemorySearch.from_documents(docs, embedding_model)\n",
    "\n",
    "\n",
    "query = \"Please suggest a shirt with sunblocking\"\n",
    "\n",
    "#使用上面的向量存储来查找与传入查询类似的文本，得到一个相似文档列表\n",
    "docs = vector_db.similarity_search(query)\n",
    "\n",
    "#打印返回文档的个数\n",
    "print(len(docs))\n",
    "\n",
    "\n",
    "#打印返回的第一个文档\n",
    "print(docs[0])\n",
    "\n",
    "# 我们可以看到一个返回了四个结果。输出的第一结果是一件关于防晒的衬衫，满足我们查询的要求：`请推荐一件防晒功能的衬衫（Please suggest a shirt with sunblocking）`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 使用向量储存回答文档的相关问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 10.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m input_query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPlease suggest a shirt with sunblocking\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39m#使用上面的向量存储来查找与传入查询类似的文本，得到一个相似文档列表\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m obtained_docs \u001b[39m=\u001b[39m vector_db\u001b[39m.\u001b[39;49msimilarity_search(input_query)\n\u001b[0;32m      6\u001b[0m \u001b[39m#基于向量储存，创建检索器\u001b[39;00m\n\u001b[0;32m      7\u001b[0m retriever \u001b[39m=\u001b[39m vector_db\u001b[39m.\u001b[39mas_retriever() \n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\vectorstores\\docarray\\base.py:125\u001b[0m, in \u001b[0;36mDocArrayIndex.similarity_search\u001b[1;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search\u001b[39m(\n\u001b[0;32m    114\u001b[0m     \u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, k: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m    115\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m    116\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimilarity_search_with_score(query, k\u001b[39m=\u001b[39mk, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    126\u001b[0m     \u001b[39mreturn\u001b[39;00m [doc \u001b[39mfor\u001b[39;00m doc, _ \u001b[39min\u001b[39;00m results]\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\vectorstores\\docarray\\base.py:103\u001b[0m, in \u001b[0;36mDocArrayIndex.similarity_search_with_score\u001b[1;34m(self, query, k, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search_with_score\u001b[39m(\n\u001b[0;32m     90\u001b[0m     \u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, k: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[0;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[Document, \u001b[39mfloat\u001b[39m]]:\n\u001b[0;32m     92\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \n\u001b[0;32m     94\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m        Lower score represents more similarity.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     query_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding\u001b[39m.\u001b[39;49membed_query(query)\n\u001b[0;32m    104\u001b[0m     query_doc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc_cls(embedding\u001b[39m=\u001b[39mquery_embedding)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     docs, scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoc_index\u001b[39m.\u001b[39mfind(query_doc, search_field\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m, limit\u001b[39m=\u001b[39mk)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\embeddings\\openai.py:506\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed_query\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mfloat\u001b[39m]:\n\u001b[0;32m    498\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \n\u001b[0;32m    500\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39m        Embedding for the text.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_documents([text])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\embeddings\\openai.py:478\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \n\u001b[0;32m    468\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[39m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[39m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_len_safe_embeddings(texts, engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeployment)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\embeddings\\openai.py:364\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    361\u001b[0m     _iter \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(tokens), _chunk_size)\n\u001b[0;32m    363\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m _iter:\n\u001b[1;32m--> 364\u001b[0m     response \u001b[39m=\u001b[39m embed_with_retry(\n\u001b[0;32m    365\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    366\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m=\u001b[39mtokens[i : i \u001b[39m+\u001b[39m _chunk_size],\n\u001b[0;32m    367\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invocation_params,\n\u001b[0;32m    368\u001b[0m     )\n\u001b[0;32m    369\u001b[0m     batched_embeddings\u001b[39m.\u001b[39mextend(r[\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m response[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    371\u001b[0m results: List[List[List[\u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(texts))]\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\embeddings\\openai.py:107\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     response \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m _check_response(response)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m _embed_with_retry(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\tenacity\\__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    323\u001b[0m     retry_exc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreraise:\n\u001b[1;32m--> 325\u001b[0m         \u001b[39mraise\u001b[39;00m retry_exc\u001b[39m.\u001b[39;49mreraise()\n\u001b[0;32m    326\u001b[0m     \u001b[39mraise\u001b[39;00m retry_exc \u001b[39mfrom\u001b[39;00m \u001b[39mfut\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwait:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\tenacity\\__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreraise\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mNoReturn:\n\u001b[0;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_attempt\u001b[39m.\u001b[39mfailed:\n\u001b[1;32m--> 158\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_attempt\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\embeddings\\openai.py:104\u001b[0m, in \u001b[0;36membed_with_retry.<locals>._embed_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_embed_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 104\u001b[0m     response \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m _check_response(response)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\openai\\api_resources\\embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\openai\\api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    761\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    762\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    764\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    765\u001b[0m     )\n\u001b[0;32m    766\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。"
     ]
    }
   ],
   "source": [
    "input_query = \"Please suggest a shirt with sunblocking\"\n",
    "\n",
    "#使用上面的向量存储来查找与传入查询类似的文本，得到一个相似文档列表\n",
    "obtained_docs = vector_db.similarity_search(input_query)\n",
    "\n",
    "#基于向量储存，创建检索器\n",
    "retriever = vector_db.as_retriever() \n",
    "\n",
    "#导入大语言模型, 这里使用默认模型gpt-3.5-turbo会出现504服务器超时，因此使用gpt-3.5-turbo-0301\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-0301\",\n",
    "    temperature = 0.0\n",
    "                 ) \n",
    "\n",
    "\n",
    "#合并获得的相似文档内容\n",
    "result_docs = \"\".join([obtained_docs[i].page_content for i in range(len(obtained_docs))])  \n",
    "\n",
    "\n",
    "#将合并的相似文档内容后加上问题（question）输入到 `llm.call_as_llm`中\n",
    "#这里问题是：以Markdown表格的方式列出所有具有防晒功能的衬衫并总结\n",
    "response = llm.call_as_llm(f\"{result_docs} Question: Please list all your \\\n",
    "shirts with sun protection in a table in markdown and summarize each one.\") \n",
    "\n",
    "\n",
    "# 在此处打印响应，我们可以看到我们得到了一个表格，正如我们所要求的那样\n",
    "display(Markdown(response))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 8.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 10.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: 当前分组负载已饱和，请稍后再试，或升级账户以提升服务质量。.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Shirt Number | Name | Description |\n",
       "| --- | --- | --- |\n",
       "| 618 | Men's Tropical Plaid Short-Sleeve Shirt | This shirt is made of 100% polyester and is wrinkle-resistant. It has front and back cape venting that lets in cool breezes and two front bellows pockets. It is rated UPF 50+ for superior protection from the sun's UV rays. |\n",
       "| 374 | Men's Plaid Tropic Shirt, Short-Sleeve | This shirt is made with 52% polyester and 48% nylon. It is machine washable and dryable. It has front and back cape venting, two front bellows pockets, and is rated to UPF 50+. |\n",
       "| 535 | Men's TropicVibe Shirt, Short-Sleeve | This shirt is made of 71% Nylon and 29% Polyester. It has front and back cape venting that lets in cool breezes and two front bellows pockets. It is rated UPF 50+ for superior protection from the sun's UV rays. |\n",
       "| 255 | Sun Shield Shirt | This shirt is made of 78% nylon and 22% Lycra Xtra Life fiber. It is handwashable and line dry. It is rated UPF 50+ for superior protection from the sun's UV rays. It is abrasion-resistant and wicks moisture for quick-drying comfort. |\n",
       "\n",
       "All of these shirts provide UPF 50+ sun protection, blocking 98% of the sun's harmful rays. They are all designed to be lightweight and comfortable in hot weather. They all have front and back cape venting that lets in cool breezes and two front bellows pockets. The Men's Tropical Plaid Short-Sleeve Shirt is made of 100% polyester and is wrinkle-resistant. The Men's Plaid Tropic Shirt, Short-Sleeve is made with 52% polyester and 48% nylon. The Men's TropicVibe Shirt, Short-Sleeve is made of 71% Nylon and 29% Polyester. The Sun Shield Shirt is made of 78% nylon and 22% Lycra Xtra Life fiber and is abrasion-resistant and wicks moisture for quick-drying comfort."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' \n",
    "通过LangChain链封装起来\n",
    "创建一个检索QA链，对检索到的文档进行问题回答，要创建这样的链，\n",
    "我们将传入几个不同的东西\n",
    "1、语言模型，在最后进行文本生成\n",
    "2、传入链类型，这里使用stuff，将所有文档塞入上下文并对语言模型进行一次调用\n",
    "3、传入一个检索器\n",
    "'''\n",
    "\n",
    "\n",
    "RetrievalQAChain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "query =  \"Please list all your shirts with sun protection in a table \\\n",
    "in markdown and summarize each one.\"#创建一个查询并在此查询上运行链\n",
    "\n",
    "\n",
    "response = RetrievalQAChain.run(query)\n",
    "\n",
    "\n",
    "display(Markdown(response))#使用 display 和 markdown 显示它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "想在许多不同类型的块上执行相同类型的问答，该怎么办？之前的实验中只返回了4个文档，如果有多个文档，那么我们可以使用几种不同的方法\n",
    "* Map Reduce   \n",
    "将所有块与问题一起传递给语言模型，获取回复，使用另一个语言模型调用将所有单独的回复总结成最终答案，它可以在任意数量的文档上运行。可以并行处理单个问题，同时也需要更多的调用。它将所有文档视为独立的\n",
    "* Refine    \n",
    "用于循环许多文档，际上是迭代的，建立在先前文档的答案之上，非常适合前后因果信息并随时间逐步构建答案，依赖于先前调用的结果。它通常需要更长的时间，并且基本上需要与Map Reduce一样多的调用\n",
    "* Map Re-rank   \n",
    "对每个文档进行单个语言模型调用，要求它返回一个分数，选择最高分，这依赖于语言模型知道分数应该是什么，需要告诉它，如果它与文档相关，则应该是高分，并在那里精细调整说明，可以批量处理它们相对较快，但是更加昂贵\n",
    "* Stuff    \n",
    "将所有内容组合成一个文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
