{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarforever/LangChain-Tutorials/blob/main/LangChain_ChatGithub.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Lake + OpenAI实现与GitHub代码聊天 - 编程学习利器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o-rPJYwWLh8",
        "outputId": "4952e7d1-8153-44a8-a0fc-00202ea37625"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade deeplake\n",
        "# !pip install --upgrade langchain deeplake openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciO_aYtP1EA3",
        "outputId": "3e510ab6-29bf-4cf4-aeb2-b49455dda961"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/chroma-core/chroma.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nTRCqa1d0zSZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\lenovo\\Desktop\\LangChainPlayGround\\DeeperTutorials\\.env\n",
            "sk-lANo2jIeCWQt94UCCf5d16B7C32744279bF98b06C822D519\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DeepLake\n",
        "\n",
        "import os,sys\n",
        "import openai\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "# sys.path.append(\"../..\")\n",
        "\n",
        "# 读取本地/项目的环境变量。\n",
        "\n",
        "# find_dotenv()寻找并定位.env文件的路径\n",
        "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中  \n",
        "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
        "print(find_dotenv())\n",
        "_ = load_dotenv(find_dotenv())\n",
        "print(os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "from langchain.chains import LLMRequestsChain, LLMChain\n",
        "\n",
        "\n",
        "llm = OpenAI(temperature=0, model_name='gpt-3.5-turbo')\n",
        "embedding_model = OpenAIEmbeddings(disallowed_special=())\n",
        "# os.environ['ACTIVELOOP_TOKEN'] = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Cl3XkDfJ1eBl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "675\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "root_dir = './chroma'\n",
        "docs = []\n",
        "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "    for file in filenames:\n",
        "        try: \n",
        "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
        "            docs.extend(loader.load_and_split())\n",
        "        except Exception as e: \n",
        "            pass\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTMD_m6S1jpm",
        "outputId": "a0e1ab29-2fca-4c8a-e528-887b19441d02"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Created a chunk of size 1030, which is longer than the specified 1000\n",
            "Created a chunk of size 1103, which is longer than the specified 1000\n",
            "Created a chunk of size 1020, which is longer than the specified 1000\n",
            "Created a chunk of size 1540, which is longer than the specified 1000\n",
            "Created a chunk of size 2945, which is longer than the specified 1000\n",
            "Created a chunk of size 1421, which is longer than the specified 1000\n",
            "Created a chunk of size 1233, which is longer than the specified 1000\n",
            "Created a chunk of size 1030, which is longer than the specified 1000\n",
            "Created a chunk of size 1730, which is longer than the specified 1000\n",
            "Created a chunk of size 1887, which is longer than the specified 1000\n",
            "Created a chunk of size 1245, which is longer than the specified 1000\n",
            "Created a chunk of size 2383, which is longer than the specified 1000\n",
            "Created a chunk of size 1537, which is longer than the specified 1000\n",
            "Created a chunk of size 1474, which is longer than the specified 1000\n",
            "Created a chunk of size 1422, which is longer than the specified 1000\n",
            "Created a chunk of size 1038, which is longer than the specified 1000\n",
            "Created a chunk of size 1108, which is longer than the specified 1000\n",
            "Created a chunk of size 1140, which is longer than the specified 1000\n",
            "Created a chunk of size 1714, which is longer than the specified 1000\n",
            "Created a chunk of size 1021, which is longer than the specified 1000\n",
            "Created a chunk of size 1442, which is longer than the specified 1000\n",
            "Created a chunk of size 1324, which is longer than the specified 1000\n",
            "Created a chunk of size 3315, which is longer than the specified 1000\n",
            "Created a chunk of size 1970, which is longer than the specified 1000\n",
            "Created a chunk of size 1328, which is longer than the specified 1000\n",
            "Created a chunk of size 1140, which is longer than the specified 1000\n",
            "Created a chunk of size 2089, which is longer than the specified 1000\n",
            "Created a chunk of size 1117, which is longer than the specified 1000\n",
            "Created a chunk of size 1246, which is longer than the specified 1000\n",
            "Created a chunk of size 3183, which is longer than the specified 1000\n",
            "Created a chunk of size 1250, which is longer than the specified 1000\n",
            "Created a chunk of size 1704, which is longer than the specified 1000\n",
            "Created a chunk of size 1197, which is longer than the specified 1000\n",
            "Created a chunk of size 1030, which is longer than the specified 1000\n",
            "Created a chunk of size 1103, which is longer than the specified 1000\n",
            "Created a chunk of size 1214, which is longer than the specified 1000\n",
            "Created a chunk of size 1388, which is longer than the specified 1000\n",
            "Created a chunk of size 1144, which is longer than the specified 1000\n",
            "Created a chunk of size 1022, which is longer than the specified 1000\n",
            "Created a chunk of size 1488, which is longer than the specified 1000\n",
            "Created a chunk of size 1895, which is longer than the specified 1000\n",
            "Created a chunk of size 2900, which is longer than the specified 1000\n",
            "Created a chunk of size 1609, which is longer than the specified 1000\n",
            "Created a chunk of size 1590, which is longer than the specified 1000\n",
            "Created a chunk of size 1725, which is longer than the specified 1000\n",
            "Created a chunk of size 1704, which is longer than the specified 1000\n",
            "Created a chunk of size 1408, which is longer than the specified 1000\n",
            "Created a chunk of size 1340, which is longer than the specified 1000\n",
            "Created a chunk of size 1459, which is longer than the specified 1000\n",
            "Created a chunk of size 1447, which is longer than the specified 1000\n",
            "Created a chunk of size 1773, which is longer than the specified 1000\n",
            "Created a chunk of size 1001, which is longer than the specified 1000\n",
            "Created a chunk of size 1016, which is longer than the specified 1000\n",
            "Created a chunk of size 1722, which is longer than the specified 1000\n",
            "Created a chunk of size 1757, which is longer than the specified 1000\n",
            "Created a chunk of size 1716, which is longer than the specified 1000\n",
            "Created a chunk of size 1130, which is longer than the specified 1000\n",
            "Created a chunk of size 1037, which is longer than the specified 1000\n",
            "Created a chunk of size 1100, which is longer than the specified 1000\n",
            "Created a chunk of size 1076, which is longer than the specified 1000\n",
            "Created a chunk of size 1179, which is longer than the specified 1000\n",
            "Created a chunk of size 1165, which is longer than the specified 1000\n",
            "Created a chunk of size 1121, which is longer than the specified 1000\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000, \n",
        "    chunk_overlap=0\n",
        "    )\n",
        "texts = text_splitter.split_documents(docs)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVcjOx-N1oLu",
        "outputId": "2ea6a414-1e33-427c-87e2-c900b67ad07d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deep Lake Dataset in hub://wangdingiie/chroma_source already exists, loading from the storage\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset(path='hub://wangdingiie/chroma_source', tensors=['embedding', 'id', 'metadata', 'text'])\n",
            "\n",
            "  tensor      htype       shape       dtype  compression\n",
            "  -------    -------     -------     -------  ------- \n",
            " embedding  embedding  (3336, 1536)  float32   None   \n",
            "    id        text      (3336, 1)      str     None   \n",
            " metadata     json      (3336, 1)      str     None   \n",
            "   text       text      (3336, 1)      str     None   \n",
            "Deep Lake Dataset in hub://wyang14/chroma_source already exists, loading from the storage\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import DeepLake\n",
        "username = \"wangdingiie\"\n",
        "\n",
        "# init the input vector db\n",
        "db = DeepLake(dataset_path=f\"hub://{username}/chroma_source\", \n",
        "              embedding_function=embedding_model,\n",
        "              reset=True,)\n",
        "db.add_documents(texts)\n",
        "\n",
        "# load the vector db\n",
        "db = DeepLake(dataset_path=\"hub://wangdingiie/chroma_source\", \n",
        "              read_only=True, \n",
        "              embedding_function=embedding_model\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aLlxAZ4S10GE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "retriever.search_kwargs['distance_metric'] = 'cos'\n",
        "retriever.search_kwargs['fetch_k'] = 100\n",
        "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
        "retriever.search_kwargs['k'] = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zsKsATuG190J"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "model = ChatOpenAI(model='gpt-3.5-turbo')\n",
        "qa = ConversationalRetrievalChain.from_llm(model,retriever=retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xhQH_VG2BZf",
        "outputId": "992d296f-a3eb-4041-e7de-a56e392be261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            " What does Chroma do? \n",
            "\n",
            "Answer:\n",
            " Chroma is an open-source embedding database that allows you to build LLM (Language Learning Model) apps by making knowledge, facts, and skills pluggable. It provides a JavaScript/TypeScript interface to communicate with a backend Chroma DB over REST. Chroma allows you to store embeddings and perform nearest neighbor searches, making it easy to work with vector databases. It also supports various embedding models such as Sentence Transformers, OpenAI embeddings, Cohere embeddings, or your own custom embeddings. \n",
            "\n",
            "\n",
            "Question:\n",
            " How to use Chroma? \n",
            "\n",
            "Answer:\n",
            " To utilize Chroma, you can follow these steps:\n",
            "\n",
            "1. Install the Chroma package: You can install the Chroma package for your preferred programming language (Python or JavaScript) using package managers like pip or npm.\n",
            "\n",
            "2. Start Chroma: Chroma needs to be running in order for the client to communicate with it. You can follow the instructions in the [Chroma Usage Guide](https://docs.trychroma.com/usage-guide) to quickly set up and start Chroma.\n",
            "\n",
            "3. Create a Chroma client: Once Chroma is running, you can create a Chroma client in your code. The client allows you to interact with the Chroma database.\n",
            "\n",
            "4. Create a collection: Use the Chroma client to create a collection. A collection is a container for storing and querying embeddings.\n",
            "\n",
            "5. Add data to the collection: Use the `add` method of the collection to add data with corresponding embeddings. You can provide an ID, embedding, and optional metadata for each data entry.\n",
            "\n",
            "6. Query the collection: Use the `query` method of the collection to perform nearest neighbor searches based on a query embedding. You can specify the number of nearest neighbors to retrieve and provide optional metadata filters.\n",
            "\n",
            "These are the basic steps to utilize Chroma. You can refer to the Chroma documentation for more details and advanced usage examples. \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"What does Chroma do?\",\n",
        "    \"How to use Chroma?\"\n",
        "] \n",
        "chat_history = []\n",
        "\n",
        "for question in questions:  \n",
        "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
        "    chat_history.append((question, result['answer']))\n",
        "    print(f\"Question:\\n {question} \\n\")\n",
        "    print(f\"Answer:\\n {result['answer']} \\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q_DYKLNmBrPK"
      },
      "outputs": [],
      "source": [
        "def ask(question, chat_history):\n",
        "  response = qa({\"question\": question, \"chat_history\": chat_history})\n",
        "  print(f\"Question:\\n {question}\\n\")\n",
        "  print(f\"Answer:\\n {response['answer']}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ6d_NJx4bsD",
        "outputId": "a7118a55-a206-4117-9323-b483a5d1f9b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            " What's the main programming language used in Chroma?\n",
            "\n",
            "Answer:\n",
            " The main programming language used in Chroma is Python.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask(\"What's the main programming language used in Chroma?\", chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMNXbAS64xiy",
        "outputId": "a827dbf9-ee9c-48d4-8a7f-5ccf78ad03c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            " Summarize the storage part of Chroma\n",
            "\n",
            "Answer:\n",
            " The storage component of Chroma is a persistent backend that stores the embeddings and metadata associated with your data. It can save your database and indexes to a directory on your local machine or to a remote server. Chroma supports different storage implementations, such as DuckDB with Parquet or a REST API server.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask('Summarize the storage part of Chroma', chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpcokdnz4_7R",
        "outputId": "027ee649-b309-4b6c-aea7-541ccd92153c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            " Tell me more about Sentence Transformers\n",
            "\n",
            "Answer:\n",
            " Sentence Transformers is a Python library that provides pre-trained models for generating sentence embeddings. These embeddings are numerical representations of sentences that capture their meaning and semantic similarity. Sentence Transformers make it easier to compare and search for similar sentences in large text collections.\n",
            "\n",
            "The library offers a variety of pre-trained models that can be used to generate sentence embeddings. These models are trained on large datasets and can encode sentences into high-dimensional vectors. The embeddings can then be used for various natural language processing tasks such as clustering, classification, semantic search, and more.\n",
            "\n",
            "Sentence Transformers are widely used in applications like information retrieval, question-answering systems, chatbots, and text summarization. They are designed to capture the semantic meaning of sentences and enable efficient similarity search.\n",
            "\n",
            "Chroma, the project mentioned in the context, uses Sentence Transformers as the default embedding function for its vector databases. It allows users to store and search for embeddings using nearest neighbor search algorithms, making it easier to find similar sentences or documents based on their embeddings.\n",
            "\n",
            "If you want to learn more about Sentence Transformers, you can refer to the [official documentation](https://www.sbert.net/docs/pretrained_models.html).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask('Tell me more about Sentence Transformers', chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrLDPlJB9LxB",
        "outputId": "3b22cd38-f510-4591-a884-deec3c3b07ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            " Show me some example code on how to use Chroma to store embeddings\n",
            "\n",
            "Answer:\n",
            " Sure! Here's an example code on how to use Chroma to store embeddings:\n",
            "\n",
            "```python\n",
            "import chromadb\n",
            "\n",
            "# Create a client\n",
            "client = chromadb.Client()\n",
            "\n",
            "# Create a collection\n",
            "collection = client.create_collection(name=\"my_collection\")\n",
            "\n",
            "# Add documents with embeddings to the collection\n",
            "documents = [\"doc1\", \"doc2\", \"doc3\"]\n",
            "embeddings = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
            "collection.add(documents, embeddings)\n",
            "\n",
            "# Query the collection\n",
            "results = collection.query(query_embeddings=[[1, 2, 3]], n_results=1)\n",
            "print(results)\n",
            "```\n",
            "\n",
            "In this example, we first create a client and then create a collection called \"my_collection\". We then add documents (with their corresponding embeddings) to the collection using the `add` method. Finally, we query the collection using a query embedding and retrieve the nearest matching document.\n",
            "\n",
            "Remember to install the Chroma library (`pip install chromadb`) and have a running instance of Chroma in order to run this code successfully.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask('Show me some example code on how to use Chroma to store embeddings', chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23vpHyrY-sMv",
        "outputId": "dafa707e-8857-4487-e6bc-9fac4ae80bd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            " What is the Python class for Chroma query interface?\n",
            "\n",
            "Answer:\n",
            " The Python class used for the Chroma query interface is `chromadb.Client`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask('What is the Python class for Chroma query interface?', chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw2V0-pa_AAp",
        "outputId": "ad403141-77d2-4c7c-9383-e0f373e1045f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            " Show me the public functions of class Client\n",
            "\n",
            "Answer:\n",
            " The public functions of the Client class are:\n",
            "\n",
            "- reset(): Resets the client.\n",
            "- version(): Retrieves the version of the client.\n",
            "- heartbeat(): Retrieves the heartbeat of the client.\n",
            "- persist(): Throws an error as it is not implemented in the JS client.\n",
            "- createCollection(name: string, metadata?: object, embeddingFunction?: CallableFunction): Creates a new collection with the specified name, metadata, and embedding function. Returns the newly created Collection object.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask('Show me the public functions of class Client', chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDG3CTK5AftH",
        "outputId": "15e696d2-86bf-434a-be50-1dc4eff5899e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            " What are the underlying databases used by Chroma?\n",
            "\n",
            "Answer:\n",
            " Chroma can use different databases as its backend. By default, Chroma uses DuckDB as its in-memory database. However, it also supports using DuckDB with Parquet for persistence. Additionally, Chroma can be configured to use ClickHouse as its backend for large-scale deployments.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask('What are the underlying databases used by Chroma?', chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reZAo8xPAvQg",
        "outputId": "d1f1ebaf-52fb-4274-93ee-ffd131d41052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            " Which class implements the DuckDB support?\n",
            "\n",
            "Answer:\n",
            " The class that implements DuckDB support in Chroma is `DuckDB`.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ask('Which class implements the DuckDB support?', chat_history)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNnHgC0INV5gC+75pBfvvrl",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
