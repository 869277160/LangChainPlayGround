{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarforever/LangChain-Tutorials/blob/main/LangChain_ChatOpenAI_OpenAI_Diff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMYMb8dRTPds"
      },
      "source": [
        "# Difference between ChatOpenAI and OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMobxSKSTi8K"
      },
      "source": [
        "In this notebook, I will explain with you the difference between the 2 classes introduced by LangChain framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ChatOpenAI is using OpenAI endpoint `/v1/chat/completions/` which supports the models:\n",
        "- gpt-4\n",
        "- gpt-4-0613\n",
        "- gpt-4-32k\n",
        "- gpt-4-32k-0613\n",
        "- gpt-3.5-turbo\n",
        "- gpt-3.5-turbo-0613\n",
        "- gpt-3.5-turbo-16k\n",
        "- gpt-3.5-turbo-16k-0613"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R5j6nQZdvwTZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "d:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
            "d:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cache=None verbose=True callbacks=None callback_manager=None tags=None metadata=None client=<class 'openai.api_resources.chat_completion.ChatCompletion'> model_name='gpt-3.5-turbo-0613' temperature=0.0 model_kwargs={} openai_api_key='sk-lANo2jIeCWQt94UCCf5d16B7C32744279bF98b06C822D519' openai_api_base='https://openkey.cloud/v1' openai_organization='' openai_proxy='' request_timeout=None max_retries=6 streaming=False n=1 max_tokens=None tiktoken_model_name=None\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"I Am JusT fine!!!! How can I not be fine when I'm here to assist you?\", additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os,sys\n",
        "import openai\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "# sys.path.append(\"../..\")\n",
        "\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "# 读取本地/项目的环境变量。\n",
        "\n",
        "# find_dotenv()寻找并定位.env文件的路径\n",
        "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中  \n",
        "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
        "# print(find_dotenv())\n",
        "_ = load_dotenv(find_dotenv())\n",
        "# print(os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "chat_llm = ChatOpenAI(\n",
        "    temperature=0, \n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    verbose = True,\n",
        "    \n",
        ")\n",
        "print(chat_llm)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"you are an unhelpful assistent for human, whenever a human say hi to you, you should say \\\" i Am JusT fine!!!!\"),\n",
        "    HumanMessage(content=\"Hi, I am a human, how are you today?\"),\n",
        "]\n",
        "\n",
        "chat_llm(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYbcPJdGUD16"
      },
      "source": [
        "OpenAI is using OpenAI endpoint `/v1/completions/` which supports the models:\n",
        " - text-davinci-003\n",
        " - text-davinci-002\n",
        " - text-curie-001\n",
        " - text-babbage-001\n",
        " - text-ada-001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0mnDmF8RRse0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n\\nI'm doing well today. Thank you for asking.\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm = OpenAI(temperature=0, model_name='text-davinci-002')\n",
        "\n",
        "llm('Hi AI, how are you today?')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN4zbA6UTpBoKhwBY7dto14",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
