{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarforever/LangChain-Tutorials/blob/main/Langchain_Memory_Persistent_Store.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0ApeCaVrZhV"
      },
      "source": [
        "# 简介\n",
        "\n",
        "该Python Notebook演示LangChain框架所提供的Memory持久化能力。LangChain通过langchain.memory.chat_message_histories包中提供的一系列组件支持多种形式的历史消息存储，包括文件，数据库等。\n",
        "\n",
        "本示例演示文件存储形式的持久化。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RdSvP1Oi7LCc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "d:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
            "d:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\lenovo\\Desktop\\LangChainPlayGround\\DeeperTutorials\\.env\n",
            "sk-lANo2jIeCWQt94UCCf5d16B7C32744279bF98b06C822D519\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Author: error: error: git config user.name & please set dead value or install git && error: git config user.email & please set dead value or install git & please set dead value or install git\n",
        "Date: 2023-08-10 10:04:44\n",
        "LastEditors: error: error: git config user.name & please set dead value or install git && error: git config user.email & please set dead value or install git & please set dead value or install git\n",
        "LastEditTime: 2023-08-10 11:06:16\n",
        "FilePath: \\LangChain-Tutorials-main\\Langchain_Memory_Persistent_Store.ipynb\n",
        "Description: \n",
        "\n",
        "Copyright (c) 2023 by ${git_name_email}, All Rights Reserved. \n",
        "'''\n",
        "# !pip install langchain openai --quiet --upgrade\n",
        "\n",
        "import os,sys\n",
        "import openai\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "# sys.path.append(\"../..\")\n",
        "\n",
        "# 读取本地/项目的环境变量。\n",
        "\n",
        "# find_dotenv()寻找并定位.env文件的路径\n",
        "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中  \n",
        "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
        "print(find_dotenv())\n",
        "_ = load_dotenv(find_dotenv())\n",
        "print(os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "import langchain \n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVqmb_UYrif9"
      },
      "source": [
        "1. 准备**OpenAI API Key**，**track_tokens_usage**辅助函数（调用`chain.run`函数，并统计token开销），以及LLM实例"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CcSMVHBz6zrj"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.callbacks import get_openai_callback\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "def track_tokens_usage(chain, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.run(query)\n",
        "        print(f'Total tokens: {cb.total_tokens}')\n",
        "\n",
        "    return result\n",
        "\n",
        "llm = OpenAI(\n",
        "  temperature=0,\n",
        "\tmodel_name=\"text-davinci-003\"\n",
        ")\n",
        "\n",
        "chat_llm = ChatOpenAI()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU9QeEcZr-x1"
      },
      "source": [
        "2. 使用Memory组件**ConversationBufferMemory**的默认配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "czgOmwfsr9Y7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n",
            "Total tokens: 110\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' Langchain is a blockchain-based language learning platform that allows users to learn new languages in a secure and decentralized environment. It uses a combination of blockchain technology and artificial intelligence to provide users with an immersive language learning experience.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation = ConversationChain(llm=llm, memory = ConversationBufferMemory())\n",
        "\n",
        "print(conversation.prompt.template)\n",
        "\n",
        "track_tokens_usage(conversation, \"What is Langchain?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnAq2L8rsLLy"
      },
      "source": [
        "3. 引入消息历史持久化组件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TymjTQMWiXAx"
      },
      "outputs": [],
      "source": [
        "from langchain.memory.chat_message_histories import FileChatMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EL-gMHvdjLHD"
      },
      "outputs": [],
      "source": [
        "message_history = FileChatMessageHistory(file_path = 'conversation_20230620.txt')\n",
        "\n",
        "memory = ConversationBufferMemory(chat_memory=message_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9K799NAZjVUT"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(llm=llm, memory = memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "D1kDz1EjjYcr",
        "outputId": "bdd9c106-7999-48f8-e4f1-815c13925bc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 133\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' ChatGPT is a natural language processing (NLP) model that enables coherent conversation by understanding the context of a conversation and providing relevant responses. It uses a transformer-based architecture to generate responses that are based on the context of the conversation. It also uses a large corpus of data to generate more accurate responses.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "track_tokens_usage(conversation, \"How does ChatGPT enable coherent conversation?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8rrP-T3kOS0",
        "outputId": "c03707e3-0a90-4e2b-bd47-571372d1f148"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'cat' 不是内部或外部命令，也不是可运行的程序\n",
            "或批处理文件。\n"
          ]
        }
      ],
      "source": [
        "!cat conversation_20230620.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "oPBsl3jUyNhP",
        "outputId": "a2e54fdc-c9ca-4c2f-e67f-9c159552bf36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 156\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "' Bye! It was nice talking to you. Have a great day!'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "track_tokens_usage(conversation, \"Bye now!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjGRBf5DyO7I",
        "outputId": "557083c1-4c41-4515-b685-90e0b269d153"
      },
      "outputs": [],
      "source": [
        "!cat conversation_20230620.txt"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMSoUMKic3BwTFLpQ1YCwEm",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
