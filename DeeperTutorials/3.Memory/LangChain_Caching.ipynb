{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarforever/LangChain-Tutorials/blob/main/LangChain_Caching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA823Xm-LG0c"
      },
      "source": [
        "# Understanding LangChain Caching\n",
        "\n",
        "In this notebook, we will see:\n",
        "1. How LangChain framework uses caching mechanism to improve LLM interaction efficiency.\n",
        "2. The caching algorithms of 2 different underlying storages, In-Memory and SQLite.\n",
        "\n",
        "Hope it will help you understand if and when you should use CACHE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VXeDv_iNy4Yt"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain openai --quiet --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFr7TU4yMINW"
      },
      "source": [
        "## Get your ChatOpenAI instance ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EOakFOzKzApQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\lenovo\\Desktop\\LangChainPlayGround\\DeeperTutorials\\.env\n",
            "sk-lANo2jIeCWQt94UCCf5d16B7C32744279bF98b06C822D519\n"
          ]
        }
      ],
      "source": [
        "import os,sys\n",
        "import openai\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "# sys.path.append(\"../..\")\n",
        "\n",
        "# 读取本地/项目的环境变量。\n",
        "\n",
        "# find_dotenv()寻找并定位.env文件的路径\n",
        "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中  \n",
        "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
        "print(find_dotenv())\n",
        "_ = load_dotenv(find_dotenv())\n",
        "print(os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "from langchain import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "import langchain \n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "chat_llm = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9LI6MsOMYDz"
      },
      "source": [
        "## 1. In Memory Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pZkMY2T1zRJ4"
      },
      "outputs": [],
      "source": [
        "from langchain.cache import InMemoryCache\n",
        "langchain.llm_cache = InMemoryCache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_0EEZ6BMgHf"
      },
      "source": [
        "### Ask a question and measure how long it takes for LLM to respond."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "EOxV7tZwz0Jz",
        "outputId": "9bb819c6-78f1-46e2-c7df-8ebcd1cc6f6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 188 ms\n",
            "Wall time: 8.93 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"OpenAI is an artificial intelligence research laboratory consisting of the for-profit OpenAI LP and its non-profit parent company, OpenAI Inc. It was founded in December 2015 by Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, John Schulman, and Wojciech Zaremba. OpenAI's mission is to ensure that artificial general intelligence (AGI) benefits all of humanity. AGI refers to highly autonomous systems that outperform humans in most economically valuable work. OpenAI conducts research in AI and develops AI technologies with a focus on long-term safety, technical leadership, and cooperative orientation. They aim to create safe and beneficial AI systems while also promoting the broad distribution of AI benefits. OpenAI has produced various notable AI models, including GPT-3, which is a state-of-the-art language model capable of natural language processing and generation.\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "chat_llm.predict(\"What is OpenAI?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNVz-A70OB65"
      },
      "source": [
        "#### How the cache stores data\n",
        "\n",
        "**source code**: [cache.py](https://github.com/hwchase17/langchain/blob/v0.0.219/langchain/cache.py#L102)\n",
        "```python\n",
        "class InMemoryCache(BaseCache):\n",
        "    \"\"\"Cache that stores things in memory.\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"Initialize with empty cache.\"\"\"\n",
        "        self._cache: Dict[Tuple[str, str], RETURN_VAL_TYPE] = {}\n",
        "```\n",
        "\n",
        "This is the implementation of InMemoryCache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "795SesMkKsx1",
        "outputId": "6602dee3-af40-4211-9ea6-c957c565babb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is OpenAI?\"}}]'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# First element of the tuple\n",
        "list(langchain.llm_cache._cache.keys())[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "cKoCMgV1J0Qy",
        "outputId": "a3ae41b4-2983-458f-ee38-9d5390df660a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"chat_models\", \"openai\", \"ChatOpenAI\"], \"kwargs\": {\"openai_api_key\": {\"lc\": 1, \"type\": \"secret\", \"id\": [\"OPENAI_API_KEY\"]}}}---[(\\'stop\\', None)]'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Second element of the tuple\n",
        "list(langchain.llm_cache._cache.keys())[0][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0HhgOmWMomV"
      },
      "source": [
        "### Ask same question again and see the quicker response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GDykW5yiMw-b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 0 ns\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"OpenAI is an artificial intelligence research laboratory consisting of the for-profit OpenAI LP and its non-profit parent company, OpenAI Inc. It was founded in December 2015 by Elon Musk, Sam Altman, Greg Brockman, Ilya Sutskever, John Schulman, and Wojciech Zaremba. OpenAI's mission is to ensure that artificial general intelligence (AGI) benefits all of humanity. AGI refers to highly autonomous systems that outperform humans in most economically valuable work. OpenAI conducts research in AI and develops AI technologies with a focus on long-term safety, technical leadership, and cooperative orientation. They aim to create safe and beneficial AI systems while also promoting the broad distribution of AI benefits. OpenAI has produced various notable AI models, including GPT-3, which is a state-of-the-art language model capable of natural language processing and generation.\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "chat_llm.predict(\"What is OpenAI?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYonfxGfMx3t"
      },
      "source": [
        "## 2. SQLite as Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EP3QRaPy0mp1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'rm' 不是内部或外部命令，也不是可运行的程序\n",
            "或批处理文件。\n"
          ]
        }
      ],
      "source": [
        "# !rm -f .cache.db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yRFlThqU0tfU"
      },
      "outputs": [],
      "source": [
        "from langchain.cache import SQLiteCache\n",
        "langchain.llm_cache = SQLiteCache(database_path=\".cache.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j1dHYmGM5WK"
      },
      "source": [
        "### Ask the same question twice and measure the performance difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "WsfcYsU40yFR",
        "outputId": "e0128db0-d992-4037-c91d-b63847cf905b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 4.96 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'OpenAI is an artificial intelligence research laboratory and company that aims to ensure that artificial general intelligence (AGI) benefits all of humanity. AGI refers to highly autonomous systems that outperform humans at most economically valuable work. OpenAI conducts research, develops AI models, and promotes responsible deployment of AI technology. It also focuses on creating AI systems that are safe, transparent, and aligned with human values. OpenAI has developed various models, including the GPT-3 language model, and actively contributes to the open-source AI community.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "chat_llm.predict(\"What is OpenAI?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "m_HFoa-Z052V",
        "outputId": "728a714a-83d4-42fc-d289-c376915c0152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 2 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'OpenAI is an artificial intelligence research laboratory and company that aims to ensure that artificial general intelligence (AGI) benefits all of humanity. AGI refers to highly autonomous systems that outperform humans at most economically valuable work. OpenAI conducts research, develops AI models, and promotes responsible deployment of AI technology. It also focuses on creating AI systems that are safe, transparent, and aligned with human values. OpenAI has developed various models, including the GPT-3 language model, and actively contributes to the open-source AI community.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "chat_llm.predict(\"What is OpenAI?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHIjReJUM_gD"
      },
      "source": [
        "### Add some space in the sentence and ask again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hdD1CpzSNFzo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 78.1 ms\n",
            "Wall time: 5.17 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'OpenAI is an artificial intelligence research laboratory and company. It aims to ensure that artificial general intelligence (AGI) benefits all of humanity. OpenAI conducts research, develops AI models and technologies, and promotes the principles of safe and responsible AI development. It has created various AI models, including GPT-3, which is a powerful language model capable of generating human-like text. OpenAI also provides an API for developers to access its models and integrate them into their applications.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "chat_llm.predict(\"What is  OpenAI?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TvI7KFBLGfTn"
      },
      "outputs": [],
      "source": [
        "import sqlalchemy\n",
        "from sqlalchemy import create_engine, text\n",
        "engine = create_engine(\"sqlite:///.cache.db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4LAKj76NJiZ"
      },
      "source": [
        "### **Why does the extra space cause the cache miss??**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlKSAcOOOSqA"
      },
      "source": [
        "#### How SQLite stores cache data\n",
        "\n",
        "**source code**: [cache.py](https://github.com/hwchase17/langchain/blob/v0.0.219/langchain/cache.py#L128)\n",
        "```python\n",
        "class FullLLMCache(Base):  # type: ignore\n",
        "    \"\"\"SQLite table for full LLM Cache (all generations).\"\"\"\n",
        "\n",
        "    __tablename__ = \"full_llm_cache\"\n",
        "    prompt = Column(String, primary_key=True)\n",
        "    llm = Column(String, primary_key=True)\n",
        "    idx = Column(Integer, primary_key=True)\n",
        "    response = Column(String)\n",
        "\n",
        "\n",
        "class SQLAlchemyCache(BaseCache):\n",
        "    \"\"\"Cache that uses SQAlchemy as a backend.\"\"\"\n",
        "\n",
        "    def __init__(self, engine: Engine, cache_schema: Type[FullLLMCache] = FullLLMCache):\n",
        "        \"\"\"Initialize by creating all tables.\"\"\"\n",
        "        self.engine = engine\n",
        "        self.cache_schema = cache_schema\n",
        "        self.cache_schema.metadata.create_all(self.engine)\n",
        "```\n",
        "\n",
        "This is the schema of cache table `full_llm_cache`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK8cQdbkGrNk",
        "outputId": "4c986b1d-1dfb-49d8-caf3-66e9f03f62a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMKeyView(['prompt', 'llm', 'idx', 'response'])\n",
            "('[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is OpenAI?\"}}]', '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"chat_models\", \"openai\", \"ChatOpenAI\"], \"kwargs\": {\"openai_api_key\": {\"lc\": 1, \"type\": \"secret\", \"id\": [\"OPENAI_API_KEY\"]}}}---[(\\'stop\\', None)]', 0, '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"output\", \"ChatGeneration\"], \"kwargs\": {\"message\": {\"lc\": 1, \"type\": \"constructor\", \"i ... (589 characters truncated) ... language model, and actively contributes to the open-source AI community.\", \"additional_kwargs\": {}}}, \"generation_info\": {\"finish_reason\": \"stop\"}}}')\n",
            "('[{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"messages\", \"HumanMessage\"], \"kwargs\": {\"content\": \"What is  OpenAI?\"}}]', '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"chat_models\", \"openai\", \"ChatOpenAI\"], \"kwargs\": {\"openai_api_key\": {\"lc\": 1, \"type\": \"secret\", \"id\": [\"OPENAI_API_KEY\"]}}}---[(\\'stop\\', None)]', 0, '{\"lc\": 1, \"type\": \"constructor\", \"id\": [\"langchain\", \"schema\", \"output\", \"ChatGeneration\"], \"kwargs\": {\"message\": {\"lc\": 1, \"type\": \"constructor\", \"i ... (524 characters truncated) ... velopers to access its models and integrate them into their applications.\", \"additional_kwargs\": {}}}, \"generation_info\": {\"finish_reason\": \"stop\"}}}')\n"
          ]
        }
      ],
      "source": [
        "with engine.connect() as connection:\n",
        "\n",
        "    rs = connection.exec_driver_sql('select * from full_llm_cache')\n",
        "    print(rs.keys())\n",
        "    for row in rs:\n",
        "        print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a46Ty0dmfhRH"
      },
      "source": [
        "## Semantic Cache\n",
        "\n",
        "Semantic cache stores prompts and responses, and evaluate hits based on semantic similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5tV0sRSqIUT"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain openai --quiet --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3IOojuZqcUy"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ['OPENAI_API_KEY'] = 'your openai api key'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwC6ItJngOoo"
      },
      "source": [
        "### Follow [Redis official doc](https://redis.com/blog/running-redis-on-google-colab/) to install and start redis server on google colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHH23I9ngMGy",
        "outputId": "ce6c1022-0d52-4a7d-d13f-56cdc7cf287a"
      },
      "outputs": [],
      "source": [
        "# !curl -fsSL https://packages.redis.io/redis-stack/redis-stack-server-6.2.6-v7.focal.x86_64.tar.gz -o redis-stack-server.tar.gz\n",
        "# !tar -xvf redis-stack-server.tar.gz\n",
        "# !pip install redis\n",
        "\n",
        "# !./redis-stack-server-6.2.6-v7/bin/redis-stack-server --daemonize yes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "flJ_q0ymfyEb"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# To make the caching really obvious, lets use a slower model.\n",
        "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3GDztDVpUGQ"
      },
      "source": [
        "### Initialize the Redis semantic cache with default score threshold 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YylPXZ2dgiHc"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.cache import RedisSemanticCache\n",
        "\n",
        "\n",
        "langchain.llm_cache = RedisSemanticCache(redis_url=\"redis://localhost:6379\", \n",
        "                                         embedding=OpenAIEmbeddings(), \n",
        "                                         score_threshold=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "r3c0garCgp-9",
        "outputId": "39c63d6f-dc8a-4e65-88c0-07052f6e9130"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Redis cannot be used as a vector database without RediSearch >=2.4Please head to https://redis.io/docs/stack/search/quick_start/to know more about installing the RediSearch module within Redis Stack.\n",
            "Redis cannot be used as a vector database without RediSearch >=2.4Please head to https://redis.io/docs/stack/search/quick_start/to know more about installing the RediSearch module within Redis Stack.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Redis failed to connect: Redis cannot be used as a vector database without RediSearch >=2.4Please head to https://redis.io/docs/stack/search/quick_start/to know more about installing the RediSearch module within Redis Stack.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\vectorstores\\redis.py:590\u001b[0m, in \u001b[0;36mRedis.from_existing_index\u001b[1;34m(cls, embedding, index_name, content_key, metadata_key, vector_key, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39m# check if redis has redisearch module installed\u001b[39;00m\n\u001b[1;32m--> 590\u001b[0m _check_redis_module_exist(client, REDIS_REQUIRED_MODULES)\n\u001b[0;32m    591\u001b[0m \u001b[39m# ensure that the index already exists\u001b[39;00m\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\vectorstores\\redis.py:70\u001b[0m, in \u001b[0;36m_check_redis_module_exist\u001b[1;34m(client, required_modules)\u001b[0m\n\u001b[0;32m     69\u001b[0m logger\u001b[39m.\u001b[39merror(error_message)\n\u001b[1;32m---> 70\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_message)\n",
            "\u001b[1;31mValueError\u001b[0m: Redis cannot be used as a vector database without RediSearch >=2.4Please head to https://redis.io/docs/stack/search/quick_start/to know more about installing the RediSearch module within Redis Stack.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\cache.py:348\u001b[0m, in \u001b[0;36mRedisSemanticCache._get_llm_cache\u001b[1;34m(self, llm_string)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 348\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dict[index_name] \u001b[39m=\u001b[39m RedisVectorstore\u001b[39m.\u001b[39;49mfrom_existing_index(\n\u001b[0;32m    349\u001b[0m         embedding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding,\n\u001b[0;32m    350\u001b[0m         index_name\u001b[39m=\u001b[39;49mindex_name,\n\u001b[0;32m    351\u001b[0m         redis_url\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mredis_url,\n\u001b[0;32m    352\u001b[0m     )\n\u001b[0;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\vectorstores\\redis.py:596\u001b[0m, in \u001b[0;36mRedis.from_existing_index\u001b[1;34m(cls, embedding, index_name, content_key, metadata_key, vector_key, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 596\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRedis failed to connect: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    598\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[0;32m    599\u001b[0m     redis_url,\n\u001b[0;32m    600\u001b[0m     index_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    606\u001b[0m )\n",
            "\u001b[1;31mValueError\u001b[0m: Redis failed to connect: Redis cannot be used as a vector database without RediSearch >=2.4Please head to https://redis.io/docs/stack/search/quick_start/to know more about installing the RediSearch module within Redis Stack.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\vectorstores\\redis.py:153\u001b[0m, in \u001b[0;36mRedis.__init__\u001b[1;34m(self, redis_url, index_name, embedding_function, content_key, metadata_key, vector_key, relevance_score_fn, distance_metric, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[39m# check if redis has redisearch module installed\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     _check_redis_module_exist(redis_client, REDIS_REQUIRED_MODULES)\n\u001b[0;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\vectorstores\\redis.py:70\u001b[0m, in \u001b[0;36m_check_redis_module_exist\u001b[1;34m(client, required_modules)\u001b[0m\n\u001b[0;32m     69\u001b[0m logger\u001b[39m.\u001b[39merror(error_message)\n\u001b[1;32m---> 70\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_message)\n",
            "\u001b[1;31mValueError\u001b[0m: Redis cannot be used as a vector database without RediSearch >=2.4Please head to https://redis.io/docs/stack/search/quick_start/to know more about installing the RediSearch module within Redis Stack.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\llms\\base.py:802\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(prompt, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    796\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    797\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    798\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prompt)\u001b[39m}\u001b[39;00m\u001b[39m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    799\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`generate` instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    800\u001b[0m     )\n\u001b[0;32m    801\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    803\u001b[0m         [prompt],\n\u001b[0;32m    804\u001b[0m         stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    805\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    806\u001b[0m         tags\u001b[39m=\u001b[39mtags,\n\u001b[0;32m    807\u001b[0m         metadata\u001b[39m=\u001b[39mmetadata,\n\u001b[0;32m    808\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    809\u001b[0m     )\n\u001b[0;32m    810\u001b[0m     \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    811\u001b[0m     \u001b[39m.\u001b[39mtext\n\u001b[0;32m    812\u001b[0m )\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\llms\\base.py:582\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    575\u001b[0m params[\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m stop\n\u001b[0;32m    576\u001b[0m options \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m: stop}\n\u001b[0;32m    577\u001b[0m (\n\u001b[0;32m    578\u001b[0m     existing_prompts,\n\u001b[0;32m    579\u001b[0m     llm_string,\n\u001b[0;32m    580\u001b[0m     missing_prompt_idxs,\n\u001b[0;32m    581\u001b[0m     missing_prompts,\n\u001b[1;32m--> 582\u001b[0m ) \u001b[39m=\u001b[39m get_prompts(params, prompts)\n\u001b[0;32m    583\u001b[0m disregard_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache\n\u001b[0;32m    584\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m )\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\llms\\base.py:131\u001b[0m, in \u001b[0;36mget_prompts\u001b[1;34m(params, prompts)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mfor\u001b[39;00m i, prompt \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(prompts):\n\u001b[0;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m langchain\u001b[39m.\u001b[39mllm_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m         cache_val \u001b[39m=\u001b[39m langchain\u001b[39m.\u001b[39;49mllm_cache\u001b[39m.\u001b[39;49mlookup(prompt, llm_string)\n\u001b[0;32m    132\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(cache_val, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    133\u001b[0m             existing_prompts[i] \u001b[39m=\u001b[39m cache_val\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\cache.py:376\u001b[0m, in \u001b[0;36mRedisSemanticCache.lookup\u001b[1;34m(self, prompt, llm_string)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlookup\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m, llm_string: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[RETURN_VAL_TYPE]:\n\u001b[0;32m    375\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Look up based on prompt and llm_string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m     llm_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_llm_cache(llm_string)\n\u001b[0;32m    377\u001b[0m     generations \u001b[39m=\u001b[39m []\n\u001b[0;32m    378\u001b[0m     \u001b[39m# Read from a Hash\u001b[39;00m\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\cache.py:354\u001b[0m, in \u001b[0;36mRedisSemanticCache._get_llm_cache\u001b[1;34m(self, llm_string)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dict[index_name] \u001b[39m=\u001b[39m RedisVectorstore\u001b[39m.\u001b[39mfrom_existing_index(\n\u001b[0;32m    349\u001b[0m         embedding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding,\n\u001b[0;32m    350\u001b[0m         index_name\u001b[39m=\u001b[39mindex_name,\n\u001b[0;32m    351\u001b[0m         redis_url\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mredis_url,\n\u001b[0;32m    352\u001b[0m     )\n\u001b[0;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m     redis \u001b[39m=\u001b[39m RedisVectorstore(\n\u001b[0;32m    355\u001b[0m         embedding_function\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding\u001b[39m.\u001b[39;49membed_query,\n\u001b[0;32m    356\u001b[0m         index_name\u001b[39m=\u001b[39;49mindex_name,\n\u001b[0;32m    357\u001b[0m         redis_url\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mredis_url,\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    359\u001b[0m     _embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding\u001b[39m.\u001b[39membed_query(text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    360\u001b[0m     redis\u001b[39m.\u001b[39m_create_index(dim\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(_embedding))\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\vectorstores\\redis.py:155\u001b[0m, in \u001b[0;36mRedis.__init__\u001b[1;34m(self, redis_url, index_name, embedding_function, content_key, metadata_key, vector_key, relevance_score_fn, distance_metric, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m     _check_redis_module_exist(redis_client, REDIS_REQUIRED_MODULES)\n\u001b[0;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRedis failed to connect: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39m=\u001b[39m redis_client\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent_key \u001b[39m=\u001b[39m content_key\n",
            "\u001b[1;31mValueError\u001b[0m: Redis failed to connect: Redis cannot be used as a vector database without RediSearch >=2.4Please head to https://redis.io/docs/stack/search/quick_start/to know more about installing the RediSearch module within Redis Stack."
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "llm(\"Please translate 'this is Monday' into Chinese\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIu-A7Wxn0sT"
      },
      "source": [
        "Notice that, the query below is 1 word different from the previous one. Cache got similarily hit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "bm_QBd4gnw_w",
        "outputId": "a8a4835d-e0c1-449b-f64b-d1372ce524da"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "llm(\"Please translate 'this is Tuesday' into Chinese\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "KB27Wo5ihC-C",
        "outputId": "50487775-f2a4-4f65-8d65-ecc33db0c4b0"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "llm(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Xbp5si6tpb7E",
        "outputId": "d02dfad8-9d7d-4672-f25e-b5c2f3938548"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "llm(\"Tell me 2 jokes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvggtYaDpiV1"
      },
      "source": [
        "### Initialize the Redis semantic cache with default score threshold 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIjlyavcpk9F"
      },
      "outputs": [],
      "source": [
        "langchain.llm_cache = RedisSemanticCache(redis_url=\"redis://localhost:6379\", embedding=OpenAIEmbeddings(), score_threshold=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "ybsRNjYhhIs5",
        "outputId": "9f40ba16-85dc-4bec-d6b6-344495160a4f"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "llm(\"Give me a peach\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "zfq2rErdk5zZ",
        "outputId": "6bf6dfdb-f6e8-4772-d7f3-289e6aedd96a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "llm(\"Give me 2 peaches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p25RH2EioFmv"
      },
      "source": [
        "### Deep dive into Redis semantic cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nimL_nl7oK1s"
      },
      "source": [
        "#### Find the keys in the cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4eGNhNIng9K",
        "outputId": "42c7433e-e993-45a8-feef-44a622b925d3"
      },
      "outputs": [],
      "source": [
        "langchain.llm_cache._cache_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-GsORdkoQOG"
      },
      "source": [
        "#### Manually execute similarity search to fetch the similar documents with scores\n",
        "\n",
        "You should expect that the more similar the document is, the smaller the score will be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqSiGnAsmC2p",
        "outputId": "fa393c10-7835-41ea-ce06-454d2128e48f"
      },
      "outputs": [],
      "source": [
        "langchain.llm_cache._cache_dict['cache:bf6f6d9ebdf492e28cb8bf4878a4b951'].similarity_search_with_score(query='Give me 2 peaches')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VspGA_wSokS4"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "The score threshold is the key factor in using Redis semantic cache for similarity cache."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBXKgATeWMVZ"
      },
      "source": [
        "## Semantic Cache with GPTCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwFaSeFWWe9r"
      },
      "source": [
        "### What is GPTCache?\n",
        "\n",
        "An open source project dedicated to building a semantic cache for storing LLM responses.\n",
        "\n",
        "Two use cases:\n",
        "1. Exact match\n",
        "2. Similar match\n",
        "\n",
        "GPTCache addressed the following questions:\n",
        "1. How to generate embeddings for the queries? (via embedding function)\n",
        "2. How to cache the data? (via cache store of data manager, such as SQLite, MySQL, and PostgreSQL. More NoSQL databases will be added in the future)\n",
        "3. How to store and search vector embeddings? (via vector store of data manager, such as FAISS or vector databases such as Milvus. More vector databases and cloud services will be added in the future.)\n",
        "4. How to determine eviction policy? (LRU or FIFO)\n",
        "5. How to determine cache hit or miss? (via evaluation function)\n",
        "\n",
        "Please refer to the following Cache class definition for better understanding of how above questions are addressed:\n",
        "\n",
        "```python\n",
        "class Cache:\n",
        "   def init(self,\n",
        "            cache_enable_func=cache_all,\n",
        "            pre_embedding_func=last_content,\n",
        "            embedding_func=string_embedding,\n",
        "            data_manager: DataManager = get_data_manager(),\n",
        "            similarity_evaluation=ExactMatchEvaluation(),\n",
        "            post_process_messages_func=first,\n",
        "            config=Config(),\n",
        "            next_cache=None,\n",
        "            **kwargs\n",
        "            ):\n",
        "       self.has_init = True\n",
        "       self.cache_enable_func = cache_enable_func\n",
        "       self.pre_embedding_func = pre_embedding_func\n",
        "       self.embedding_func = embedding_func\n",
        "       self.data_manager: DataManager = data_manager\n",
        "       self.similarity_evaluation = similarity_evaluation\n",
        "       self.post_process_messages_func = post_process_messages_func\n",
        "       self.data_manager.init(**kwargs)\n",
        "       self.config = config\n",
        "       self.next_cache = next_cache\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sp9gZNC2WXA7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gptcache\n",
            "  Obtaining dependency information for gptcache from https://files.pythonhosted.org/packages/5a/ec/1a83bfea7a4a8c1844bcc97f1c6046fe9e14b54c243156308e6374283bae/gptcache-0.1.39.1-py3-none-any.whl.metadata\n",
            "  Downloading gptcache-0.1.39.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\llm\\lib\\site-packages (from gptcache) (1.23.5)\n",
            "Requirement already satisfied: cachetools in d:\\anaconda3\\envs\\llm\\lib\\site-packages (from gptcache) (5.3.1)\n",
            "Requirement already satisfied: requests in d:\\anaconda3\\envs\\llm\\lib\\site-packages (from gptcache) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->gptcache) (2.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->gptcache) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->gptcache) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->gptcache) (2023.7.22)\n",
            "Downloading gptcache-0.1.39.1-py3-none-any.whl (122 kB)\n",
            "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
            "   ---------- ----------------------------- 30.7/122.3 kB 1.3 MB/s eta 0:00:01\n",
            "   ------------- ------------------------- 41.0/122.3 kB 653.6 kB/s eta 0:00:01\n",
            "   ----------------------------- --------- 92.2/122.3 kB 744.7 kB/s eta 0:00:01\n",
            "   ---------------------------------- --- 112.6/122.3 kB 726.2 kB/s eta 0:00:01\n",
            "   -------------------------------------- 122.3/122.3 kB 651.7 kB/s eta 0:00:00\n",
            "Installing collected packages: gptcache\n",
            "Successfully installed gptcache-0.1.39.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -angchain (d:\\anaconda3\\envs\\llm\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -angchain (d:\\anaconda3\\envs\\llm\\lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "!pip install gptcache "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zy5CCHJl2yla"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-002\", n=2, best_of=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SHDJ27DW5ny"
      },
      "source": [
        "### Exact Match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oS_kGsN1XCfm"
      },
      "outputs": [],
      "source": [
        "from gptcache import Cache\n",
        "from gptcache.manager.factory import manager_factory\n",
        "from gptcache.processor.pre import get_prompt\n",
        "from gptcache.adapter.api import init_similar_cache\n",
        "from langchain.cache import GPTCache\n",
        "import hashlib\n",
        "\n",
        "def get_hashed_name(name):\n",
        "    return hashlib.sha256(name.encode()).hexdigest()\n",
        "\n",
        "\n",
        "def init_gptcache(cache_obj: Cache, llm: str):\n",
        "    hashed_llm = get_hashed_name(llm)\n",
        "    cache_obj.init(\n",
        "        pre_embedding_func=get_prompt,\n",
        "        data_manager=manager_factory(\n",
        "            manager=\"map\", \n",
        "            data_dir=f\"map_cache_{hashed_llm}\"),\n",
        "    )\n",
        "\n",
        "\n",
        "langchain.llm_cache = GPTCache(init_gptcache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KlldiTqFrzrm"
      },
      "outputs": [],
      "source": [
        "question = \"What is cache eviction policy?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "HNF18bP6XE_c",
        "outputId": "8b3df86b-7e52-4cd4-a55e-923a25ee937f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 31.2 ms\n",
            "Wall time: 6.16 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\nA cache eviction policy is a set of rules that determine which items in a cache should be removed when the cache becomes full and new items need to be added.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "llm(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "nnXOyMwWrtla",
        "outputId": "2be6a860-35f8-4089-85c4-37a47bcb61ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 998 µs\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\nA cache eviction policy is a set of rules that determine which items in a cache should be removed when the cache becomes full and new items need to be added.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "llm(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Vqp-2H-sBp64",
        "outputId": "8893f8e3-6913-4e1c-ee50-d6dcfcc06883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 2.54 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\nThere are several cache eviction policies, including least recently used (LRU), first in first out (FIFO), and random.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "llm(\"What is cache eviction   policy?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BixAYE1ysCda"
      },
      "source": [
        "### Similar Match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "847yNhrlsCEt"
      },
      "outputs": [],
      "source": [
        "from gptcache import Cache\n",
        "from gptcache.adapter.api import init_similar_cache\n",
        "from langchain.cache import GPTCache\n",
        "import hashlib\n",
        "\n",
        "\n",
        "def get_hashed_name(name):\n",
        "    return hashlib.sha256(name.encode()).hexdigest()\n",
        "\n",
        "\n",
        "def init_gptcache(cache_obj: Cache, llm: str):\n",
        "    hashed_llm = get_hashed_name(llm)\n",
        "    init_similar_cache(cache_obj=cache_obj, data_dir=f\"similar_cache_{hashed_llm}\")\n",
        "\n",
        "\n",
        "langchain.llm_cache = GPTCache(init_gptcache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "MssW4YLXsL4B",
        "outputId": "43ad7c5c-8621-41bc-eb81-c125ee08bcb1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76a4a2fdafc841a8bd68966d0c9ea49b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lenovo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5818c3a4c52f49889dad625ea2ef16c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/827 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "721dd2259e90494aa82e1de24c7f8b7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da7c84dd0a664044b1420dc573f7b340",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e16cc916300b4847a854c395f6f17aef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/245 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29e77caa6adc44c89e08e5f9c14d6c34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.onnx:   0%|          | 0.00/46.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 7.22 s\n",
            "Wall time: 1min 43s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\nA cache eviction policy is a set of rules that determine when and how often cached data is removed from the cache.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "llm(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "t0sIlm0xsYf2",
        "outputId": "33b72b52-4ad7-487e-90e0-a9cc2b6b547e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 3.47 s\n",
            "Wall time: 459 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\nA cache eviction policy is a set of rules that determine when and how often cached data is removed from the cache.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "llm(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Wnq0CDSBsd2y",
        "outputId": "ae860031-f9ce-4912-dac9-28e54ec4baca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 5.59 s\n",
            "Wall time: 925 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\nA cache eviction policy is a set of rules that determine when and how often cached data is removed from the cache.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "llm(\"What is cache eviction   policy?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "1MYGLb8GCiEB",
        "outputId": "90663424-ec3a-4c85-9ca8-374b517566ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 6.22 s\n",
            "Wall time: 1.98 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\",\\n\\nAnd I'll give you a kiss.\""
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "llm(\"Give me a peach\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "e_PX54xiCi7g",
        "outputId": "982e42bd-edcd-45b5-fc47-cd15383168d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 3.27 s\n",
            "Wall time: 400 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\",\\n\\nAnd I'll give you a kiss.\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "llm(\"Give me 2 peaches\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMWORqfJY8aBvCdSDqE4xwH",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
