{"cells":[{"cell_type":"markdown","id":"0d647f70","metadata":{},"source":["# 第四章 向量数据库与词向量(Vectorstores and Embeddings)\n","\n"," - [一、环境配置](#一、环境配置)\n"," - [二、读取文档](#二、读取文档)\n"," - [三、Embeddings](#三、Embeddings)\n"," - [四、Vectorstores](#四、Vectorstores)\n","     - [4.1 初始化Chroma](#4.1-初始化Chroma)\n","     - [4.2 相似性搜索(Similarity Search)](#4.2-相似性搜索(Similarity-Search))\n"," - [五、失败的情况(Failure modes)](#五、失败的情况(Failure-modes))\n"]},{"cell_type":"markdown","id":"ea9da9ec","metadata":{},"source":["回顾一下检索增强生成（RAG）的整体工作流程："]},{"attachments":{"overview.jpeg":{"image/jpeg":"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"}},"cell_type":"markdown","id":"125247c2","metadata":{},"source":["![overview.jpeg](attachment:overview.jpeg)"]},{"cell_type":"markdown","id":"88e5ba0b","metadata":{},"source":["## 一、环境配置"]},{"cell_type":"markdown","id":"f3d033b6","metadata":{},"source":["在当前文件夹下新建`.env`文件，内容为`OPENAI_API_KEY = \"sk-...\"`\n","\n","由于本章节需要使用`PyPDFLoader`、`Chroma`，故需要安装依赖包`pypdf`、`chromadb`"]},{"cell_type":"code","execution_count":5,"id":"8ed9c2eb","metadata":{},"outputs":[],"source":["# !pip install -Uq pypdf\n","# !pip install -Uq chromadb"]},{"cell_type":"code","execution_count":6,"id":"7f8d2266-4a35-4904-ae9d-c89790c5ae61","metadata":{"height":166,"tags":[]},"outputs":[],"source":["import os\n","import openai\n","import sys\n","sys.path.append('../..')\n","\n","from dotenv import load_dotenv, find_dotenv\n","_ = load_dotenv(find_dotenv()) \n","\n","\n","# openai.api_key  = os.environ['OPENAI_API_KEY']"]},{"cell_type":"markdown","id":"460a54b0","metadata":{},"source":["前两节课我们讨论了`Document Loading`（文档加载）和`Splitting`（分割）。\n","\n","下面我们将使用前两节课的知识对文档进行加载分割。"]},{"cell_type":"markdown","id":"18e077a0","metadata":{},"source":["## 二、读取文档"]},{"cell_type":"markdown","id":"0c1e123e","metadata":{},"source":["下面文档的课程链接 https://see.stanford.edu/Course/CS229 ，可在该网站上下载对应的课程讲义"]},{"cell_type":"code","execution_count":7,"id":"2437469e","metadata":{"height":249,"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["209\n","[Document(page_content=\"MachineLearning-Lecture01  \\nInstructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \\nlearning class. So what I wanna do today is ju st spend a little time going over the logistics \\nof the class, and then we'll start to  talk a bit about machine learning.  \\nBy way of introduction, my name's  Andrew Ng and I'll be instru ctor for this class. And so \\nI personally work in machine learning, and I' ve worked on it for about 15 years now, and \\nI actually think that machine learning is th e most exciting field of all the computer \\nsciences. So I'm actually always excited about  teaching this class. Sometimes I actually \\nthink that machine learning is not only the most exciting thin g in computer science, but \\nthe most exciting thing in all of human e ndeavor, so maybe a little bias there.  \\nI also want to introduce the TAs, who are all graduate students doing research in or \\nrelated to the machine learni ng and all aspects of machin e learning. Paul Baumstarck \\nworks in machine learning and computer vision.  Catie Chang is actually a neuroscientist \\nwho applies machine learning algorithms to try to understand the human brain. Tom Do \\nis another PhD student, works in computa tional biology and in sort of the basic \\nfundamentals of human learning. Zico Kolter is  the head TA — he's head TA two years \\nin a row now — works in machine learning a nd applies them to a bunch of robots. And \\nDaniel Ramage is — I guess he's not here  — Daniel applies l earning algorithms to\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}), Document(page_content='Daniel Ramage is — I guess he\\'s not here  — Daniel applies l earning algorithms to \\nproblems in natural language processing.  \\nSo you\\'ll get to know the TAs and me much be tter throughout this quarter, but just from \\nthe sorts of things the TA\\'s do, I hope you can  already tell that machine learning is a \\nhighly interdisciplinary topic in which just the TAs find l earning algorithms to problems \\nin computer vision and biology and robots a nd language. And machine learning is one of \\nthose things that has and is having a large impact on many applications.  \\nSo just in my own daily work, I actually frequently end up talking to people like \\nhelicopter pilots to biologists to people in  computer systems or databases to economists \\nand sort of also an unending stream of  people from industry coming to Stanford \\ninterested in applying machine learni ng methods to their own problems.  \\nSo yeah, this is fun. A couple of weeks ago, a student actually forwar ded to me an article \\nin \"Computer World\" about the 12 IT skills th at employers can\\'t say no to. So it\\'s about \\nsort of the 12 most desirabl e skills in all of IT and all of information technology, and \\ntopping the list was actually machine lear ning. So I think this is a good time to be \\nlearning this stuff and learning algorithms and having a large impact on many segments \\nof science and industry.  \\nI\\'m actually curious about something. Learni ng algorithms is one of the things that', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}), Document(page_content=\"of science and industry.  \\nI'm actually curious about something. Learni ng algorithms is one of the things that \\ntouches many areas of science and industrie s, and I'm just kind of curious. How many \\npeople here are computer science majors, are in the computer science department? Okay. \\nAbout half of you. How many people are from  EE? Oh, okay, maybe about a fifth. How\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}), Document(page_content=\"many biologers are there here? Wow, just a few, not many. I'm surprised. Anyone from \\nstatistics? Okay, a few. So where are the rest of you from?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : Say again?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : iCME. Cool.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Civi and what else?  \\nStudent : [Inaudible]  \\nInstructor (Andrew Ng) : Synthesis, [inaudible] systems. Yeah, cool.  \\nStudent : Chemi.  \\nInstructor (Andrew Ng) : Chemi. Cool.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Aero/astro. Yes, right. Yeah, okay, cool. Anyone else?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon? MSNE. All ri ght. Cool. Yeah.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Endo —  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Oh, I see, industry. Okay. Cool. Great, great. So as you can \\ntell from a cross-section of th is class, I think we're a very diverse audience in this room, \\nand that's one of the things that makes this class fun to teach and fun to be in, I think.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 1}), Document(page_content=\"So in this class, we've tried to convey to you a broad set of principl es and tools that will \\nbe useful for doing many, many things. And ev ery time I teach this class, I can actually \\nvery confidently say that af ter December, no matter what yo u're going to do after this \\nDecember when you've sort of completed this  class, you'll find the things you learn in \\nthis class very useful, and these things will be useful pretty much no matter what you end \\nup doing later in your life.  \\nSo I have more logistics to go over later, but let's say a few more words about machine \\nlearning. I feel that machine learning grew out of  early work in AI, early work in artificial \\nintelligence. And over the last — I wanna say last 15 or last 20 years or so, it's been viewed as a sort of growing new capability for computers. And in particular, it turns out \\nthat there are many programs or there are many applications that you can't program by \\nhand.  \\nFor example, if you want to get a computer to read handwritten characters, to read sort of \\nhandwritten digits, that actual ly turns out to be amazingly difficult to write a piece of \\nsoftware to take this input, an image of some thing that I wrote and to  figure out just what \\nit is, to translate my cursive handwriting into — to extract the characters I wrote out in \\nlonghand. And other things: One thing that my students and I do is autonomous flight. It \\nturns out to be extremely difficult to sit dow n and write a program to  fly a helicopter.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 2}), Document(page_content=\"turns out to be extremely difficult to sit dow n and write a program to  fly a helicopter.  \\nBut in contrast, if you want to do things like to get software to fl y a helicopter or have \\nsoftware recognize handwritten digits, one very  successful approach is to use a learning \\nalgorithm and have a computer learn by its elf how to, say, recognize your handwriting. \\nAnd in fact, handwritten digit recognition, this is pretty much the only approach that \\nworks well. It uses applications that are hard to program by hand.  \\nLearning algorithms has also made I guess sign ificant inroads in what's sometimes called \\ndatabase mining. So, for example, with the growth of IT and computers, increasingly \\nmany hospitals are keeping around medical reco rds of what sort of patients, what \\nproblems they had, what their prognoses was,  what the outcome was. And taking all of \\nthese medical records, which started to be digitized only about maybe 15 years, applying \\nlearning algorithms to them can turn raw medi cal records into what I might loosely call \\nmedical knowledge in which we start to detect trends in medical practice and even start to \\nalter medical practice as a result of me dical knowledge that's derived by applying \\nlearning algorithms to the sorts of medical r ecords that hospitals have just been building \\nover the last 15, 20 years in an electronic format.  \\nTurns out that most of you probably use learning algorithms — I don't know — I think\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 2}), Document(page_content=\"over the last 15, 20 years in an electronic format.  \\nTurns out that most of you probably use learning algorithms — I don't know — I think \\nhalf a dozen times a day or maybe a dozen  times a day or more, and often without \\nknowing it. So, for example, every time you se nd mail via the US Postal System, turns \\nout there's an algorithm that tries to automa tically read the zip code you wrote on your \\nenvelope, and that's done by a learning al gorithm. So every time you send US mail, you \\nare using a learning algorithm, perhap s without even being aware of it.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 2}), Document(page_content=\"Similarly, every time you write a check, I ac tually don't know the number for this, but a \\nsignificant fraction of checks that you write are processed by a learning algorithm that's \\nlearned to read the digits, so the dolla r amount that you wrote down on your check. So \\nevery time you write a check, there's anot her learning algorithm that you're probably \\nusing without even being aware of it.  \\nIf you use a credit card, or I know at least one phone compan y was doing this, and lots of \\ncompanies like eBay as well that do electr onic transactions, there's a good chance that \\nthere's a learning algorithm in the backgr ound trying to figure out if, say, your credit \\ncard's been stolen or if someone's engaging in a fraudulent transaction.  \\nIf you use a website like Amazon or Netflix that will often recommend books for you to \\nbuy or movies for you to rent or whatever , these are other examples of learning \\nalgorithms that have learned what sorts of th ings you like to buy or what sorts of movies \\nyou like to watch and can therefore give  customized recommendations to you.  \\nJust about a week ago, I had my car serviced, and even there, my car mechanic was trying \\nto explain to me some learning algorithm in th e innards of my car th at's sort of doing its \\nbest to optimize my driving performan ce for fuel efficiency or something.  \\nSo, see, most of us use learning algorithms half a dozen, a dozen, maybe dozens of times \\nwithout even knowing it.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 3}), Document(page_content=\"So, see, most of us use learning algorithms half a dozen, a dozen, maybe dozens of times \\nwithout even knowing it.  \\nAnd of course, learning algorithms are also  doing things like giving us a growing \\nunderstanding of the human genome. So if so meday we ever find a cure for cancer, I bet \\nlearning algorithms will have had a large role in that. That's sort of the thing that Tom \\nworks on, yes?  \\nSo in teaching this class, I sort of have thre e goals. One of them is just to I hope convey \\nsome of my own excitement a bout machine learning to you.  \\nThe second goal is by the end of this class, I hope all of you will be ab le to apply state-of-\\nthe-art machine learning algorithms to whatev er problems you're interested in. And if you \\never need to build a system for reading zi p codes, you'll know how to do that by the end \\nof this class.  \\nAnd lastly, by the end of this class, I reali ze that only a subset of  you are interested in \\ndoing research in machine learning, but by the c onclusion of this class,  I hope that all of \\nyou will actually be well qualified to star t doing research in machine learning, okay?  \\nSo let's say a few words about logistics. The prerequisites of this class are written on one \\nof the handouts, are as follows: In this class, I'm going to assume that all of you have sort \\nof basic knowledge of computer science and kn owledge of the basic computer skills and \\nprinciples. So I assume all of you know what big?O notation, that all of you know about\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 3}), Document(page_content='principles. So I assume all of you know what big?O notation, that all of you know about \\nsort of data structures like  queues, stacks, binary trees , and that all of you know enough \\nprogramming skills to, like, write a simple co mputer program. And it turns out that most', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 3}), Document(page_content=\"of this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 4}), Document(page_content=\"But if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.  \\nSo there are a couple more logisti cal things I should deal with in  this class. One is that, as \\nmost of you know, CS229 is a televised cla ss. And in fact, I guess many of you are \\nprobably watching this at home on TV, so I' m gonna say hi to our home viewers.  \\nSo earlier this year, I appro ached SCPD, which televises th ese classes, about trying to \\nmake a small number of Stanford classes publ icly available or posting the videos on the \\nweb. And so this year, Stanford is actually starting a small pilot program in which we'll \\npost videos of a small number of classes onlin e, so on the Internet in a way that makes it \\npublicly accessible to everyone. I'm very exc ited about that because machine learning in \\nschool, let's get the word out there.  \\nOne of the consequences of this is that — let's see — so videos  or pictures of the students \\nin this classroom will not be posted online, so your images — so don't worry about being \\nby seeing your own face appear on YouTube one day. But the microphones may pick up your voices, so I guess the consequence of that is that because microphones may pick up your voices, no matter how irritated you are at  me, don't yell out swear words in the \\nmiddle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 4}), Document(page_content=\"middle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?  \\nLet's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 4}), Document(page_content=\"So all right, online resources. The class has a home page, so it's in on the handouts. I \\nwon't write on the chalkboard — http:// cs229.stanford.edu. And so when there are \\nhomework assignments or things like that, we  usually won't sort of — in the mission of \\nsaving trees, we will usually not give out many handouts in class. So homework \\nassignments, homework solutions will be posted online at the course home page.  \\nAs far as this class, I've also written, a nd I guess I've also revised every year a set of \\nfairly detailed lecture notes that cover the te chnical content of this  class. And so if you \\nvisit the course homepage, you'll also find the detailed lecture notes that go over in detail \\nall the math and equations and so on  that I'll be doing in class.  \\nThere's also a newsgroup, su.class.cs229, also written on the handout. This is a \\nnewsgroup that's sort of a forum for people in  the class to get to  know each other and \\nhave whatever discussions you want to ha ve amongst yourselves. So the class newsgroup \\nwill not be monitored by the TAs and me. But this is a place for you to form study groups \\nor find project partners or discuss homework problems and so on, and it's not monitored \\nby the TAs and me. So feel free to ta lk trash about this class there.  \\nIf you want to contact the teaching staff, pl ease use the email address written down here, \\ncs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 5}), Document(page_content=\"cs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So \\nrather than sending us email individually, if you send email to this account, it will \\nactually let us get back to you maximally quickly with answers to your questions.  \\nIf you're asking questions about homework probl ems, please say in the subject line which \\nassignment and which question the email refers to, since that will also help us to route \\nyour question to the appropriate TA or to me  appropriately and get the response back to \\nyou quickly.  \\nLet's see. Skipping ahead — let's see — for homework, one midterm, one open and term \\nproject. Notice on the honor code. So one thi ng that I think will help you to succeed and \\ndo well in this class and even help you to enjoy this cla ss more is if you form a study \\ngroup.  \\nSo start looking around where you' re sitting now or at the end of class today, mingle a \\nlittle bit and get to know your classmates. I strongly encourage you to form study groups \\nand sort of have a group of people to study with and have a group of your fellow students \\nto talk over these concepts with. You can also  post on the class news group if you want to \\nuse that to try to form a study group.  \\nBut some of the problems sets in this cla ss are reasonably difficult.  People that have \\ntaken the class before may tell you they were very difficult. And just I bet it would be \\nmore fun for you, and you'd probably have a be tter learning experience if you form a\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 5}), Document(page_content=\"more fun for you, and you'd probably have a be tter learning experience if you form a \\nstudy group of people to work with. So I definitely encourage you to do that.  \\nAnd just to say a word on the honor code, whic h is I definitely en courage you to form a \\nstudy group and work together, discuss homew ork problems together. But if you discuss\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 5}), Document(page_content=\"homework problems with other students, then  I'll ask you to sort of go home and write \\ndown your own solutions independe ntly without referring to note s that were taken in any \\nof your joint study sessions.  \\nSo in other words, when you turn in a hom ework problem, what you turn in should be \\nsomething that was reconstructed independe ntly by yourself and w ithout referring to \\nnotes that you took during your  study sessions with other people, okay? And obviously, \\nshowing your solutions to othe rs or copying other solutions  directly is right out.  \\nWe occasionally also reuse problem set questions from previous years so that the \\nproblems are a bit more debugged and work more  smoothly. And as a result of that, I also \\nask you not to look at solutions from previous ye ars, and this includes both sort of official \\nsolutions that we've given out to previous gene rations of this class and previous solutions \\nthat people that have taken this class in previous years may have written out by \\nthemselves, okay?  \\nSadly, in this class, there are usually — sadly, in previous y ears, there have often been a \\nfew honor code violations in this class. And last year, I think I pr osecuted five honor code \\nviolations, which I think is a ridiculously large number. And so just don't work without \\nsolutions, and hopefully there'll be zero honor code  violations this year. I'd love for that \\nto happen.  \\nThe section here on the late homework polic y if you ever want to hand in a homework\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 6}), Document(page_content=\"to happen.  \\nThe section here on the late homework polic y if you ever want to hand in a homework \\nlate, I'll leave you to r ead that yourself.  \\nWe also have a midterm, which is scheduled for Thursday, 8th of November at 6:00 p.m., \\nso please keep that evening free.  \\nAnd let's see. And one more administrative thing I wanted to sa y is about the class \\nproject. So part of the goal of this cla ss is to leave you well eq uipped to apply machine \\nlearning algorithms to a problem or to do rese arch in machine learning. And so as part of \\nthis class, I'll ask you to execute a small resear ch project sort of as a small term project.  \\nAnd what most students do for this is either  apply machine learning to a problem that you \\nfind interesting or investigate some aspect of  machine learning. So to those of you that \\nare either already doing research or to those of you who are in industry, you're taking this \\nfrom a company, one fantastic sort of way to do a class project would be if you apply \\nmachine learning algorithms to a problem that  you're interested in, to a problem that \\nyou're already working on, whether it be a scien ce research problem or sort of a problem \\nin industry where you're trying to get a syst em to work using a learning algorithm.  \\nTo those of you that are not currently doing re search, one great way to do a project would \\nbe if you apply learning algorithms to just pick a problem that you care about. Pick a\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 6}), Document(page_content='be if you apply learning algorithms to just pick a problem that you care about. Pick a \\nproblem that you find interesting, and apply lear ning algorithms to that  and play with the \\nideas and see what happens.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 6}), Document(page_content=\"And let's see. Oh, and the goal of the projec t should really be for you to do a publishable \\npiece of research in machine learning, okay?  \\nAnd if you go to the course website, you'll actuall y find a list of the projects that students \\nhad done last year. And so I'm holding the li st in my hand. You can  go home later and \\ntake a look at it online.  \\nBut reading down this list, I see that last year, there were st udents that ap plied learning \\nalgorithms to control a snake robot. Ther e was a few projects on improving learning \\nalgorithms. There's a project on flying autonomous  aircraft. There was a project actually \\ndone by our TA Paul on improvi ng computer vision algorithms  using machine learning.  \\nThere are a couple of project s on Netflix rankings using learning algorithms; a few \\nmedical robots; ones on segmenting [inaudibl e] to segmenting pieces of the body using \\nlearning algorithms; one on musical instrume nt detection; anot her on irony sequence \\nalignment; and a few algorithms on understandin g the brain neuroscience, actually quite a \\nfew projects on neuroscience; a couple of projects on unde scending fMRI data on brain \\nscans, and so on; another project on market makings, the financial trading. There was an \\ninteresting project on trying to use learning algorithms to decide what is it that makes a \\nperson's face physically attractive. There's a learning algorithm on op tical illusions, and \\nso on.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 7}), Document(page_content=\"person's face physically attractive. There's a learning algorithm on op tical illusions, and \\nso on.  \\nAnd it goes on, so lots of fun projects. A nd take a look, then come up with your own \\nideas. But whatever you find cool and interest ing, I hope you'll be able to make machine \\nlearning a project out of it. Yeah, question?  \\nStudent : Are these gro up projects?  \\nInstructor (Andrew Ng): Oh, yes, thank you.  \\nStudent : So how many people can be in a group?  \\nInstructor (Andrew Ng): Right. So projects can be done in  groups of up to three people. \\nSo as part of forming study groups, later t oday as you get to know your classmates, I \\ndefinitely also encourage you to grab two ot her people and form a group of up to three \\npeople for your project, okay? And just start brainstorming ideas for now amongst \\nyourselves. You can also come and talk to me or the TAs if you want to brainstorm ideas \\nwith us.  \\nOkay. So one more organizational ques tion. I'm curious, how many of you know \\nMATLAB? Wow, cool, quite a lot. Okay. So as part of the — act ually how many of you \\nknow Octave or have used Octave ? Oh, okay, much smaller number.  \\nSo as part of this class, especially in the homeworks, we'll ask you to implement a few \\nprograms, a few machine learning algorithms as  part of the homeworks. And most of\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 7}), Document(page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 8}), Document(page_content='into his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televi sed. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 8}), Document(page_content=\"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 8}), Document(page_content=\"So later this quarter, we'll use the discussion sections to talk about things like convex \\noptimization, to talk a little bit about hidde n Markov models, which is a type of machine \\nlearning algorithm for modeling time series and a few other things, so  extensions to the \\nmaterials that I'll be covering in the main  lectures. And attend ance at the discussion \\nsections is optional, okay?  \\nSo that was all I had from l ogistics. Before we move on to start talking a bit about \\nmachine learning, let me check what questions you have. Yeah?  \\nStudent : [Inaudible] R or something like that?  \\nInstructor (Andrew Ng) : Oh, yeah, let's see, right. So our policy has been that you're \\nwelcome to use R, but I would strongly advi se against it, mainly because in the last \\nproblem set, we actually supply some code th at will run in Octave  but that would be \\nsomewhat painful for you to translate into R yourself. So for your other assignments, if \\nyou wanna submit a solution in R, that's fi ne. But I think MATLAB is actually totally \\nworth learning. I know R and MATLAB, and I personally end up using MATLAB quite a \\nbit more often for various reasons. Yeah?  \\nStudent : For the [inaudible] pr oject [inaudible]?  \\nInstructor (Andrew Ng) : So for the term project, you're welcome to do it in smaller \\ngroups of three, or you're welcome to do it by yo urself or in groups of two. Grading is the \\nsame regardless of the group size, so with  a larger group, you probably — I recommend\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 9}), Document(page_content=\"same regardless of the group size, so with  a larger group, you probably — I recommend \\ntrying to form a team, but it's actually totally fine to do it in a sma ller group if you want.  \\nStudent : [Inaudible] what language [inaudible]?  \\nInstructor (Andrew Ng): So let's see. There is no C programming in this class other \\nthan any that you may choose to do yourself in your project. So all the homeworks can be \\ndone in MATLAB or Octave, and let's see. A nd I guess the program prerequisites is more \\nthe ability to understand big?O notation and know ledge of what a data structure, like a \\nlinked list or a queue or bina ry treatments, more so than  your knowledge of C or Java \\nspecifically. Yeah?  \\nStudent : Looking at the end semester project, I mean, what exactly will you be testing \\nover there? [Inaudible]?  \\nInstructor (Andrew Ng) : Of the project?  \\nStudent : Yeah.  \\nInstructor (Andrew Ng) : Yeah, let me answer that later.  In a couple of weeks, I shall \\ngive out a handout with guidelines for the pr oject. But for now, we should think of the \\ngoal as being to do a cool piec e of machine learning work that  will let you experience the\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 9}), Document(page_content=\"joys of machine learning firs thand and really try to think about doing a publishable piece \\nof work.  \\nSo many students will try to build a cool machine learning application. That's probably \\nthe most common project. Some students will try to improve state-of-the-art machine \\nlearning. Some of those projects are also very  successful. It's a littl e bit harder to do. And \\nthere's also a smaller minority of students th at will sometimes try to prove — develop the \\ntheory of machine learning further or try to  prove theorems about machine learning. So \\nthey're usually great projects of all of those types with applications and machine learning \\nbeing the most common. Anything else? Okay, cool.  \\nSo that was it for logistics. Let's talk about  learning algorithms. So can I have the laptop \\ndisplay, please, or the projector? Actually, co uld you lower the big sc reen? Cool. This is \\namazing customer service. Thank you. I see. Okay, cool. Okay. No, that's fine. I see. \\nOkay. That's cool. Thanks. Okay.  \\nBig screen isn't working toda y, but I hope you can read things  on the smaller screens out \\nthere. Actually, [inaudible] I think this room just got a new projector that — someone \\nsent you an excited email — was it just on Frid ay? — saying we just got a new projector \\nand they said 4,000-to-1 something or othe r brightness ratio. I don't know. Someone was \\nvery excited about the new projector in this room, but I guess we'll see that in operation \\non Wednesday.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 10}), Document(page_content='very excited about the new projector in this room, but I guess we\\'ll see that in operation \\non Wednesday.  \\nSo start by talking about what machine learni ng is. What is machine learning? Actually, \\ncan you read the text out there? Raise your hand if the text on the small screens is legible. \\nOh, okay, cool, mostly legible. Okay. So I\\'ll just read it out.  \\nSo what is machine learning? Way back in  about 1959, Arthur Samuel defined machine \\nlearning informally as the [inaudible] that gives computers to learn — [inaudible] that \\ngives computers the ability to learn without  being explicitly programmed. So Arthur \\nSamuel, so way back in the history of m achine learning, actually did something very \\ncool, which was he wrote a checkers progr am, which would play games of checkers \\nagainst itself.  \\nAnd so because a computer can play thousands  of games against itself relatively quickly, \\nArthur Samuel had his program play thousands  of games against itself, and over time it \\nwould start to learn to rec ognize patterns which led to wi ns and patterns which led to \\nlosses. So over time it learned things like that , \"Gee, if I get a lot of pieces taken by the \\nopponent, then I\\'m more likely to lose than win,\" or, \"Gee, if I get my pieces into a \\ncertain position, then I\\'m especially li kely to win rather than lose.\"  \\nAnd so over time, Arthur Samuel had a check ers program that woul d actually learn to', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 10}), Document(page_content='And so over time, Arthur Samuel had a check ers program that woul d actually learn to \\nplay checkers by learning what are the sort of  board positions that tend to be associated \\nwith wins and what are the boa rd positions that tend to be associated with losses. And \\nway back around 1959, the amazing thing about this was that his program actually \\nlearned to play checkers much better than Arthur Samuel  himself could.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 10}), Document(page_content=\"So even today, there are some people that say, well, computers can't do anything that \\nthey're not explicitly programmed to. And Ar thur Samuel's checkers program was maybe \\nthe first I think really convi ncing refutation of this clai m. Namely, Arthur Samuel \\nmanaged to write a checkers program that could play checkers much better than he \\npersonally could, and this is an instance of maybe computers learning to do things that \\nthey were not programmed explicitly to do.  \\nHere's a more recent, a more modern, more formal definition of machine learning due to \\nTom Mitchell, who says that a well-posed le arning problem is defined as follows: He \\nsays that a computer program is set to lear n from an experience E with respect to some \\ntask T and some performance measure P if  its performance on T as measured by P \\nimproves with experience E. Okay. So not  only is it a definition, it even rhymes.  \\nSo, for example, in the case of checkers, th e experience E that a program has would be \\nthe experience of playing lots of games of checkers against itself, say. The task T is the \\ntask of playing checkers, a nd the performance measure P will be something like the \\nfraction of games it wins against a cert ain set of human opponents. And by this \\ndefinition, we'll say that Arthur Samuel's ch eckers program has learned to play checkers, \\nokay?  \\nSo as an overview of what we're going to do in this class, this class is sort of organized\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 11}), Document(page_content=\"okay?  \\nSo as an overview of what we're going to do in this class, this class is sort of organized \\ninto four major sections. We're gonna talk about four major topics in this class, the first \\nof which is supervised learning. So le t me give you an example of that.  \\nSo suppose you collect a data set of housing prices. And one of the TAs, Dan Ramage, \\nactually collected a data set for me last week to use in the example later. But suppose that \\nyou go to collect statistics about how much hous es cost in a certain geographic area. And \\nDan, the TA, collected data from housing pr ices in Portland, Oregon. So what you can do \\nis let's say plot the square footage of the house against the list price of  the house, right, so \\nyou collect data on a bunch of houses. And let' s say you get a data set like this with \\nhouses of different sizes that are li sted for different amounts of money.  \\nNow, let's say that I'm trying to sell a hous e in the same area as Portland, Oregon as \\nwhere the data comes from. Let's say I have a hou se that's this size in square footage, and \\nI want an algorithm to tell me about how much should I expect my house to sell for. So there are lots of ways to do this, and some of you may have seen elements of what I'm \\nabout to say before.  \\nSo one thing you could do is look at this data and maybe put a straight  line to it. And then \\nif this is my house, you may then look at th e straight line and predict that my house is\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 11}), Document(page_content=\"if this is my house, you may then look at th e straight line and predict that my house is \\ngonna go for about that much money, right? Ther e are other decisions that we can make, \\nwhich we'll talk about later, which is, well, what if I don' t wanna put a straight line? \\nMaybe I should put a quadratic function to it. Ma ybe that fits the data a little bit better. \\nYou notice if you do that, the price of my house goes up a bit, so that'd be nice.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 11}), Document(page_content=\"And this sort of learning pr oblem of learning to predict hous ing prices is an example of \\nwhat's called a supervised learning problem. And the reason that it's called supervised \\nlearning is because we're providing the al gorithm a data set of a bunch of square \\nfootages, a bunch of housing sizes, and as well as sort of the right answer of what the \\nactual prices of a number  of houses were, right?  \\nSo we call this supervised learning because we're supervising the algorithm or, in other \\nwords, we're giving the algorithm the, quote,  right answer for a number of houses. And \\nthen we want the algorithm to learn the a ssociation between the inputs and the outputs \\nand to sort of give us more of the right answers, okay?  \\nIt turns out this specific exam ple that I drew here is an example of something called a \\nregression problem. And the term regression sort of refers to the fact that the variable \\nyou're trying to predict is a continuous value and price.  \\nThere's another class of supervised learning problems which we'll talk about, which are \\nclassification problems. And so, in a classifi cation problem, the variab le you're trying to \\npredict is discreet rather than continuous . So as one specific example — so actually a \\nstandard data set you can download online [i naudible] that lots of machine learning \\npeople have played with. Let's say you collect  a data set on breast cancer tumors, and you\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 12}), Document(page_content=\"people have played with. Let's say you collect  a data set on breast cancer tumors, and you \\nwant to learn the algorithm to predict wh ether or not a certai n tumor is malignant. \\nMalignant is the opposite of benign, right, so ma lignancy is a sort of harmful, bad tumor. \\nSo we collect some number of features, some  number of properties of these tumors, and \\nfor the sake of sort of having a simple [inaudi ble] explanation, let's just say that we're \\ngoing to look at the size of the tumor and depe nding on the size of the tumor, we'll try to \\nfigure out whether or not the tu mor is malignant or benign.  \\nSo the tumor is either malignant or benign, and so  the variable in the Y axis is either zero \\nor 1, and so your data set ma y look something like that, righ t? And that's 1 and that's \\nzero, okay? And so this is an example of a classification problem where the variable \\nyou're trying to predict is a discreet value. It 's either zero or 1.  \\nAnd in fact, more generally, there will be many learning problems where we'll have more \\nthan one input variable, more than one input f eature and use more than one variable to try \\nto predict, say, whether a tumor is malignant  or benign. So, for example, continuing with \\nthis, you may instead have a data  set that looks like this. I'm gonna part this data set in a \\nslightly different way now. And I'm making this  data set look much cleaner than it really \\nis in reality for illustration, okay?\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 12}), Document(page_content='slightly different way now. And I\\'m making this  data set look much cleaner than it really \\nis in reality for illustration, okay?  \\nFor example, maybe the crosses indicate ma lignant tumors and the \"O\"s may indicate \\nbenign tumors. And so you may have a data se t comprising patients of  different ages and \\nwho have different tumor sizes and where a cross indicates a mali gnant tumor, and an \\n\"O\" indicates a benign tumor. And you may want  an algorithm to learn to predict, given a \\nnew patient, whether their tumo r is malignant or benign.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 12}), Document(page_content=\"So, for example, what a learning algorithm ma y do is maybe come in and decide that a \\nstraight line like that separates the two classes of tumors really well, and so if you have a \\nnew patient who's age and tumor size fall over there, then the algorithm may predict that \\nthe tumor is benign rather than malignant, oka y? So this is just another example of \\nanother supervised learning problem and another classification problem.  \\nAnd so it turns out that one of the issues we' ll talk about later in this class is in this \\nspecific example, we're going to try to predic t whether a tumor is malignant or benign \\nbased on two features or based on two inputs, namely the age of the patient and the tumor \\nsize. It turns out that when you look at a real  data set, you find th at learning algorithms \\noften use other sets of features . In the breast cancer data ex ample, you also use properties \\nof the tumors, like clump thic kness, uniformity of cell size, uniformity of cell shape, \\n[inaudible] adhesion and so on, so va rious other medical properties.  \\nAnd one of the most interesting things we'll ta lk about later this quarter is what if your \\ndata doesn't lie in a two-dimensional or th ree-dimensional or sort of even a finite \\ndimensional space, but is it possible — what if your data actually lies in an infinite \\ndimensional space? Our plots here are two-dime nsional space. I can't plot you an infinite\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 13}), Document(page_content=\"dimensional space? Our plots here are two-dime nsional space. I can't plot you an infinite \\ndimensional space, right? And so it turns out that one of the most successful classes of \\nmachine learning algorithms — some may call support vector machines — actually takes \\ndata and maps data to an infinite dimensi onal space and then does classification using not \\ntwo features like I've done  here, but an infinite number of features.  \\nAnd that will actually be one of the most fa scinating things we talk about when we go \\ndeeply into classification al gorithms. And it's actually an in teresting question, right, so \\nthink about how do you even represent an in finite dimensional vector in computer \\nmemory? You don't have an infinite amount of computers. How do you even represent a \\npoint that lies in an infinite dimensional sp ace? We'll talk about that when we get to \\nsupport vector machines, okay?  \\nSo let's see. So that was supervised learning. The second of the four major topics of this \\nclass will be learning theory. So I have a friend who teaches math at a different \\nuniversity, not at Stanford, and when you talk to  him about his work and what he's really \\nout to do, this friend of mine will — he's a ma th professor, right? — this friend of mine \\nwill sort of get the look of wonder in his eyes, and he'll tell you about how in his \\nmathematical work, he feels like he's disc overing truth and beauty in the universe. And\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 13}), Document(page_content=\"mathematical work, he feels like he's disc overing truth and beauty in the universe. And \\nhe says it in sort of a real ly touching, sincere way, and then  he has this — you can see it \\nin his eyes — he has this deep appreciation of the truth and beauty  in the universe as \\nrevealed to him by the math he does.  \\nIn this class, I'm not gonna do any truth and beauty. In this class,  I'm gonna talk about \\nlearning theory to try to convey to y ou an understanding of how and why learning \\nalgorithms work so that we can apply these lear ning algorithms as effectively as possible.  \\nSo, for example, it turns out you can prove su rprisingly deep theorems on when you can \\nguarantee that a learning algorithm will wo rk, all right? So think about a learning\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 13}), Document(page_content=\"algorithm for reading zip codes. When can  you prove a theorem guaranteeing that a \\nlearning algorithm will be at least 99.9 per cent accurate on reading zip codes? This is \\nactually somewhat surprising. We actually prove theorems showing when you can expect \\nthat to hold.  \\nWe'll also sort of delve into learning theo ry to try to understand what algorithms can \\napproximate different functions well and also  try to understand things like how much \\ntraining data do you need? So how many exampl es of houses do I need in order for your \\nlearning algorithm to recognize the pattern be tween the square footage of a house and its \\nhousing price? And this will help  us answer questions like if you're trying to design a \\nlearning algorithm, should you be spending more time collecting more data or is it a case \\nthat you already have enough data; it would be  a waste of time to try to collect more. \\nOkay?  \\nSo I think learning algorithms are a very powerful tool that  as I walk around sort of \\nindustry in Silicon Valley or as I work with  various businesses in CS and outside CS, I \\nfind that there's often a huge differen ce between how well so meone who really \\nunderstands this stuff can appl y a learning algorithm versus so meone who sort of gets it \\nbut sort of doesn't.  \\nThe analogy I like to think of  is imagine you were going to a carpentry school instead of \\na machine learning class, right? If you go to a carpentry school, they can give you the\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 14}), Document(page_content=\"a machine learning class, right? If you go to a carpentry school, they can give you the \\ntools of carpentry. They'll give you a hamme r, a bunch of nails, a screwdriver or \\nwhatever. But a master carpenter will be able to  use those tools far better than most of us \\nin this room. I know a carpen ter can do things with a hammer and nail that I couldn't \\npossibly. And it's actually a littl e bit like that in machine learning, too. One thing that's \\nsadly not taught in many courses on machine l earning is how to take the tools of machine \\nlearning and really, really apply them well.  \\nSo in the same way, so the tools of machin e learning are I wanna say quite a bit more \\nadvanced than the tools of carpentry. Maybe a carpenter will disagree . But a large part of \\nthis class will be just givi ng you the raw tools of machine learning, just the algorithms \\nand so on. But what I plan to do throughout this entire quarter, not just in the segment of \\nlearning theory, but actually as a theme r unning through everything I do this quarter, will \\nbe to try to convey to you the skills to real ly take the learning al gorithm ideas and really \\nto get them to work on a problem.  \\nIt's sort of hard for me to stand here and say how big a deal that is, but when I walk \\naround companies in Silicon Valley, it's co mpletely not uncommon for me to see \\nsomeone using some machine learning algorith m and then explain to me what they've\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 14}), Document(page_content=\"someone using some machine learning algorith m and then explain to me what they've \\nbeen doing for the last six months, and I go, oh, gee, it should have been obvious from \\nthe start that the last six months, you've been wa sting your time, right?  \\nAnd so my goal in this class, running th rough the entire quarter , not just on learning \\ntheory, is actually not only to give you the tools of m achine learning, but to teach you \\nhow to use them well. And I've noticed this  is something that really not many other\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 14}), Document(page_content='classes teach. And this is something I\\'m rea lly convinced is a huge deal, and so by the \\nend of this class, I hope all of you will be master carpenters. I hope all of you will be \\nreally good at applying these learning algor ithms and getting them to work amazingly \\nwell in many problems. Okay?  \\nLet\\'s see. So [inaudible] the board. After lear ning theory, there\\'s a nother class of learning \\nalgorithms that I then want to teach you a bout, and that\\'s unsupervised learning. So you \\nrecall, right, a little ea rlier I drew an example like this , right, where you have a couple of \\nfeatures, a couple of input vari ables and sort of malignant tumors and benign tumors or \\nwhatever. And that was an example of a s upervised learning problem because the data \\nyou have gives you the right answer for each of your patients. The data tells you this \\npatient has a malignant tumor;  this patient has a benign tumor. So it had the right \\nanswers, and you wanted the algorithm to just produce more of the same.  \\nIn contrast, in an unsupervised learning problem , this is the sort of data you get, okay? \\nWhere speaking loosely, you\\'re given a data se t, and I\\'m not gonna tell you what the right \\nanswer is on any of your data. I\\'m just gonna give you a data set and I\\'m gonna say, \"Would you please find interesting structure in this data set?\" So that\\'s the unsupervised \\nlearning problem where you\\'re sort of not given the right answer for everything.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 15}), Document(page_content=\"learning problem where you're sort of not given the right answer for everything.  \\nSo, for example, an algorithm may find structure in the data in the form of the data being \\npartitioned into two clusters, or clustering is  sort of one example of an unsupervised \\nlearning problem.  \\nSo I hope you can see this. It turns out that th ese sort of unsupervised  learning algorithms \\nare also used in many problems. This is a scr een shot — this is a picture I got from Sue \\nEmvee, who's a PhD student here, who is a pplying unsupervised learning algorithms to \\ntry to understand gene data, so is trying to  look at genes as individuals and group them \\ninto clusters based on properties of what ge nes they respond to — based on properties of \\nhow the genes respond to different experiments.  \\nAnother interesting application of [inaudible] sorts of clus tering algorithms is actually \\nimage processing, this which I got from Steve Gules, who's another PhD student. It turns \\nout what you can do is if you give this sort of  data, say an image, to certain unsupervised \\nlearning algorithms, they will then learn to group pixels together and say, gee, this sort of \\npixel seems to belong together , and that sort of pixel seems to belong together.  \\nAnd so the images you see on the bottom — I guess you can just barely see them on there \\n— so the images you see on the bottom are groupings — are what the algorithm has done\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 15}), Document(page_content='— so the images you see on the bottom are groupings — are what the algorithm has done \\nto group certain pixels together. On a small di splay, it might be easier to just look at the \\nimage on the right. The two images on the botto m are two sort of identical visualizations \\nof the same grouping of the pixe ls into [inaudible] regions.  \\nAnd so it turns out that this sort of clustering algorithm or this sort of unsupervised \\nlearning algorithm, which learns  to group pixels together, it turns out to be useful for \\nmany applications in vision, in co mputer vision image processing.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 15}), Document(page_content=\"I'll just show you one example, and this is a rather cool one that two students, Ashutosh \\nSaxena and Min Sun here did, wh ich is given an image like this, right? This is actually a \\npicture taken of the Stanford campus. You can apply that sort of cl ustering algorithm and \\ngroup the picture into regions. Let me actually blow that up so that you can see it more \\nclearly. Okay. So in the middle, you see the lines sort of groupi ng the image together, \\ngrouping the image into [inaudible] regions.  \\nAnd what Ashutosh and Min did was they then  applied the learning algorithm to say can \\nwe take this clustering and us e it to build a 3D model of the world? And so using the \\nclustering, they then had a lear ning algorithm try to learn what the 3D structure of the \\nworld looks like so that they could come up with a 3D model that you can sort of fly \\nthrough, okay? Although many people used to th ink it's not possible to take a single \\nimage and build a 3D model, but using a lear ning algorithm and that sort of clustering \\nalgorithm is the first step. They were able to.  \\nI'll just show you one more example. I like this  because it's a picture of Stanford with our \\nbeautiful Stanford campus. So again, taking th e same sort of clustering algorithms, taking \\nthe same sort of unsupervised learning algor ithm, you can group the pixels into different\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 16}), Document(page_content=\"the same sort of unsupervised learning algor ithm, you can group the pixels into different \\nregions. And using that as a pre-processing step, they eventually built this sort of 3D model of Stanford campus in a single picture.  You can sort of walk  into the ceiling, look \\naround the campus. Okay? This actually turned out to be a mix of supervised and \\nunsupervised learning, but the unsupervised lear ning, this sort of cl ustering was the first \\nstep.  \\nSo it turns out these sorts of unsupervised — clustering algorithms are actually routinely \\nused for many different problems, things like organizing computing clusters, social \\nnetwork analysis, market segmentation, so if you're a marketer and you want to divide your market into different segments or diffe rent groups of people to market to them \\nseparately; even for astronomical data an alysis and understanding how galaxies are \\nformed. These are just a sort of small sample  of the applications of unsupervised learning \\nalgorithms and clustering algorithms that we 'll talk about later in this class.  \\nJust one particularly cool example of an uns upervised learning algorithm that I want to \\ntell you about. And to motivate that, I'm gonna  tell you about what's called the cocktail \\nparty problem, which is imagine that you're at  some cocktail party a nd there are lots of \\npeople standing all over. And you know how it is, right, if you're at a large party,\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 16}), Document(page_content=\"people standing all over. And you know how it is, right, if you're at a large party, \\neveryone's talking, it can be sometimes very hard  to hear even the person in front of you. \\nSo imagine a large cocktail party with lots of people. So the problem is, is that all of these people talking, can you separate out the voice of just the person you're interested in \\ntalking to with all this  loud background noise?  \\nSo I'll show you a specific example in a second, but here's a cocktail party that's I guess \\nrather sparsely attended by just two people.  But what we're gonna do is we'll put two \\nmicrophones in the room, okay? And so becau se the microphones are just at slightly \\ndifferent distances to the two people, and th e two people may speak in slightly different \\nvolumes, each microphone will pick up an overl apping combination of these two people's\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 16}), Document(page_content=\"voices, so slightly different overlapping voice s. So Speaker 1's voice may be more loud \\non Microphone 1, and Speaker 2's voice may be louder on Microphone 2, whatever.  \\nBut the question is, given these microphone reco rdings, can you separate out the original \\nspeaker's voices? So I'm gonna play some audi o clips that were collected by Tai Yuan \\nLee at UCSD. I'm gonna actually play for you the original raw microphone recordings \\nfrom this cocktail party. So this is the Microphone 1:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\nUno, dos, tres, cuatro, cinco, seis, siete, ocho, nueve, diez.  \\nInstructor (Andrew Ng) : So it's a fascinating cocktail party with people counting from \\none to ten. This is the second microphone:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\nUno, dos, tres, cuatro, cinco, seis, siete, ocho, nueve, diez.  \\nInstructor (Andrew Ng) : Okay. So in supervised learning, we don't know what the \\nright answer is, right? So what we're goi ng to do is take exactly the two microphone \\nrecordings you just heard and give it to an  unsupervised learning algorithm and tell the \\nalgorithm which of these discover structure in the data [inaudible] or  what structure is \\nthere in this data? And we actually don't know what the right answer is offhand.  \\nSo give this data to an unsupervised lear ning algorithm, and what the algorithm does in\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 17}), Document(page_content=\"So give this data to an unsupervised lear ning algorithm, and what the algorithm does in \\nthis case, it will discover that this data can actually be explained by two independent \\nspeakers speaking at the same time, and it can  further separate out the two speakers for \\nyou. So here's Output 1 of the algorithm:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nInstructor (Andrew Ng) : And there's the second algorithm:  \\nMicrophone 2:\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 17}), Document(page_content=\"Uno, dos, tres, cuatro, cinco, seis, siete, ocho, nueve, diez.  \\nInstructor (Andrew Ng): And so the algorithm discovers  that, gee, the structure \\nunderlying the data is really th at there are two sources of so und, and here they are. I'll \\nshow you one more example. This is a, well, th is is a second sort of different pair of \\nmicrophone recordings:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\n[Music playing.]  Instructor (Andrew Ng): So the poor guy is not at a cocktail party. He's talking to his \\nradio. There's the second recording:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\n[Music playing.]  \\nInstructor (Andrew Ng) : Right. And we get this data. It's the same unsupervised \\nlearning algorithm. The algorithm is actually called independent component analysis, and \\nlater in this quarter, you'll see why. And then output's the following:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nInstructor (Andrew Ng): And that's the second one:  \\nMicrophone 2:  \\n[Music playing.]  \\nInstructor (Andrew Ng): Okay. So it turns out that be yond solving the cocktail party \\nalgorithm, this specific cla ss of unsupervised learning algor ithms are also applied to a \\nbunch of other problems, like in text proces sing or understanding f unctional grading and \\nmachine data, like the magneto-encephalogram would be an EEG data. We'll talk about\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 18}), Document(page_content=\"machine data, like the magneto-encephalogram would be an EEG data. We'll talk about \\nthat more when we go and describe ICA or independent component analysis algorithms, \\nwhich is what you just saw.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 18}), Document(page_content=\"And as an aside, this algorithm I just showed you, it seems like it must be a pretty \\ncomplicated algorithm, right, to take this overlapping audio streams and separate them \\nout. It sounds like a pretty complicated thi ng to do. So you're gonna ask how complicated \\nis it really to implement an  algorithm like this? It turns out if you do it in MATLAB, you \\ncan do it in one line of code.  \\nSo I got this from Samuel Wyse at Toront o, U of Toronto, and the example I showed you \\nactually used a more complicated ICA algorithm than this. But nonetheless, I guess this is \\nwhy for this class I'm going to ask you to  do most of your programming in MATLAB and \\nOctave because if you try to implement the sa me algorithm in C or Java or something, I \\ncan tell you from personal, painful experien ce, you end up writing pages and pages of \\ncode rather than relatively few lines of code. I'll also mention that it did take researchers \\nmany, many years to come up with that one line of code, so this is not easy.  \\nSo that was unsupervised learning, and then the last of the four major topics I wanna tell \\nyou about is reinforcement learning. And this  refers to problems where you don't do one-\\nshot decision-making. So, for example, in  the supervised learning cancer prediction \\nproblem, you have a patient come in, you predict that the cancer is malignant or benign. \\nAnd then based on your prediction, maybe the pa tient lives or dies, and then that's it,\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 19}), Document(page_content=\"And then based on your prediction, maybe the pa tient lives or dies, and then that's it, \\nright? So you make a decision and then there's a consequence. You either got it right or \\nwrong. In reinforcement learning problems, you are usually asked to make a sequence of \\ndecisions over time.  \\nSo, for example, this is something that my students and I work on. If I give you the keys \\nto an autonomous helicopter — we actually ha ve this helicopter here at Stanford, — how \\ndo you write a program to make it fly, ri ght? You notice that if you make a wrong \\ndecision on a helicopter, the consequence of crashing it may not happen until much later. And in fact, usually you need to make a w hole sequence of bad decisions to crash a \\nhelicopter. But conversely, you al so need to make a whole sequence of good decisions in \\norder to fly a helic opter really well.  \\nSo I'm gonna show you some fun videos of lear ning algorithms flying helicopters. This is \\na video of our helicopter at Stanford flying using a contro ller that was learned using a \\nreinforcement learning algorithm. So this wa s done on the Stanford football field, and \\nwe'll zoom out the camera in a second. You'll sort of see th e trees planted in the sky. So \\nmaybe this is one of the most difficult aer obatic maneuvers flown on any helicopter under \\ncomputer control. And this controller, which is very, very hard for a human to sit down \\nand write out, was learned using one of these reinforcement learning algorithms.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 19}), Document(page_content='and write out, was learned using one of these reinforcement learning algorithms.  \\nJust a word about that: The basic idea behi nd a reinforcement learning algorithm is this \\nidea of what\\'s called a reward  function. What we have to think about is imagine you\\'re \\ntrying to train a dog. So every time y our dog does something good, you say, \"Good dog,\" \\nand you reward the dog. Every time your dog does something bad, you go, \"Bad dog,\" right? And hopefully, over time, your dog will lear n to do the right things to get more of \\nthe positive rewards, to get mo re of the \"Good dogs\" and to ge t fewer of the \"Bad dogs.”', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 19}), Document(page_content='So the way we teach a helicopter to fly or any of these robots is sort of the same thing. \\nEvery time the helicopter crashes, we go, \"B ad helicopter,\" and every time it does the \\nright thing, we go, \"Good helicopter, \" and over time it learns how to control itself so as to \\nget more of these positive rewards.  \\nSo reinforcement learning is — I think of it as a way for you to specify what you want \\ndone, so you have to specify what is a \"good dog\" and what is a \"bad dog\" behavior. And \\nthen it\\'s up to the learning algorithm to  figure out how to maximize the \"good dog\" \\nreward signals and minimize the \"bad dog\" punishments.  \\nSo it turns out reinforcement learning is applie d to other problems in robotics. It\\'s applied \\nto things in web crawling and so on. But it\\'s just  cool to show videos, so let me just show \\na bunch of them. This learning algorithm was actually implemented by our head TA, Zico, of programming a four-legged dog. I guess Sam Shriver in this class also worked \\non the project and Peter Renfrew and Mike and a few others. But I guess this really is a \\ngood dog/bad dog since it\\'s a robot dog.  \\nThe second video on the right, some of the st udents, I guess Peter, Zico, Tonca working \\non a robotic snake, again using learning algorith ms to teach a snake robot to climb over \\nobstacles.  \\nBelow that, this is kind of a fun example.  Ashutosh Saxena and Jeff Michaels used \\nlearning algorithms to teach a car how to  drive at reasonably high speeds off roads', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 20}), Document(page_content=\"learning algorithms to teach a car how to  drive at reasonably high speeds off roads \\navoiding obstacles.  \\nAnd on the lower right, that's a robot program med by PhD student Eva Roshen to teach a \\nsort of somewhat strangely configured robot how to get on top of an obstacle, how to get \\nover an obstacle. Sorry. I know the video's kind of small. I hope you can sort of see it. \\nOkay?  \\nSo I think all of these are robots that I thi nk are very difficult to hand-code a controller \\nfor by learning these sorts of l earning algorithms. You can in relatively short order get a \\nrobot to do often pretty amazing things.  \\nOkay. So that was most of what I wanted to say today. Just a couple more last things, but \\nlet me just check what questions you have righ t now. So if there are no questions, I'll just \\nclose with two reminders, which are after class today or as you start to talk with other \\npeople in this class, I just encourage you again to start to form project partners, to try to \\nfind project partners to do your project with. And also, this is a good time to start forming \\nstudy groups, so either talk to your friends  or post in the newsgroup, but we just \\nencourage you to try to star t to do both of those today, okay? Form study groups, and try \\nto find two other project partners.  \\nSo thank you. I'm looking forward to teaching this class, and I'll see you in a couple of \\ndays.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 20}), Document(page_content='[End of Audio]  \\nDuration: 69 minutes', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 21}), Document(page_content=\"MachineLearning-Lecture01  \\nInstructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \\nlearning class. So what I wanna do today is ju st spend a little time going over the logistics \\nof the class, and then we'll start to  talk a bit about machine learning.  \\nBy way of introduction, my name's  Andrew Ng and I'll be instru ctor for this class. And so \\nI personally work in machine learning, and I' ve worked on it for about 15 years now, and \\nI actually think that machine learning is th e most exciting field of all the computer \\nsciences. So I'm actually always excited about  teaching this class. Sometimes I actually \\nthink that machine learning is not only the most exciting thin g in computer science, but \\nthe most exciting thing in all of human e ndeavor, so maybe a little bias there.  \\nI also want to introduce the TAs, who are all graduate students doing research in or \\nrelated to the machine learni ng and all aspects of machin e learning. Paul Baumstarck \\nworks in machine learning and computer vision.  Catie Chang is actually a neuroscientist \\nwho applies machine learning algorithms to try to understand the human brain. Tom Do \\nis another PhD student, works in computa tional biology and in sort of the basic \\nfundamentals of human learning. Zico Kolter is  the head TA — he's head TA two years \\nin a row now — works in machine learning a nd applies them to a bunch of robots. And \\nDaniel Ramage is — I guess he's not here  — Daniel applies l earning algorithms to\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}), Document(page_content='Daniel Ramage is — I guess he\\'s not here  — Daniel applies l earning algorithms to \\nproblems in natural language processing.  \\nSo you\\'ll get to know the TAs and me much be tter throughout this quarter, but just from \\nthe sorts of things the TA\\'s do, I hope you can  already tell that machine learning is a \\nhighly interdisciplinary topic in which just the TAs find l earning algorithms to problems \\nin computer vision and biology and robots a nd language. And machine learning is one of \\nthose things that has and is having a large impact on many applications.  \\nSo just in my own daily work, I actually frequently end up talking to people like \\nhelicopter pilots to biologists to people in  computer systems or databases to economists \\nand sort of also an unending stream of  people from industry coming to Stanford \\ninterested in applying machine learni ng methods to their own problems.  \\nSo yeah, this is fun. A couple of weeks ago, a student actually forwar ded to me an article \\nin \"Computer World\" about the 12 IT skills th at employers can\\'t say no to. So it\\'s about \\nsort of the 12 most desirabl e skills in all of IT and all of information technology, and \\ntopping the list was actually machine lear ning. So I think this is a good time to be \\nlearning this stuff and learning algorithms and having a large impact on many segments \\nof science and industry.  \\nI\\'m actually curious about something. Learni ng algorithms is one of the things that', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}), Document(page_content=\"of science and industry.  \\nI'm actually curious about something. Learni ng algorithms is one of the things that \\ntouches many areas of science and industrie s, and I'm just kind of curious. How many \\npeople here are computer science majors, are in the computer science department? Okay. \\nAbout half of you. How many people are from  EE? Oh, okay, maybe about a fifth. How\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}), Document(page_content=\"many biologers are there here? Wow, just a few, not many. I'm surprised. Anyone from \\nstatistics? Okay, a few. So where are the rest of you from?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : Say again?  \\nStudent : iCME.  \\nInstructor (Andrew Ng) : iCME. Cool.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Civi and what else?  \\nStudent : [Inaudible]  \\nInstructor (Andrew Ng) : Synthesis, [inaudible] systems. Yeah, cool.  \\nStudent : Chemi.  \\nInstructor (Andrew Ng) : Chemi. Cool.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Aero/astro. Yes, right. Yeah, okay, cool. Anyone else?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon? MSNE. All ri ght. Cool. Yeah.  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Pardon?  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Endo —  \\nStudent : [Inaudible].  \\nInstructor (Andrew Ng) : Oh, I see, industry. Okay. Cool. Great, great. So as you can \\ntell from a cross-section of th is class, I think we're a very diverse audience in this room, \\nand that's one of the things that makes this class fun to teach and fun to be in, I think.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 1}), Document(page_content=\"So in this class, we've tried to convey to you a broad set of principl es and tools that will \\nbe useful for doing many, many things. And ev ery time I teach this class, I can actually \\nvery confidently say that af ter December, no matter what yo u're going to do after this \\nDecember when you've sort of completed this  class, you'll find the things you learn in \\nthis class very useful, and these things will be useful pretty much no matter what you end \\nup doing later in your life.  \\nSo I have more logistics to go over later, but let's say a few more words about machine \\nlearning. I feel that machine learning grew out of  early work in AI, early work in artificial \\nintelligence. And over the last — I wanna say last 15 or last 20 years or so, it's been viewed as a sort of growing new capability for computers. And in particular, it turns out \\nthat there are many programs or there are many applications that you can't program by \\nhand.  \\nFor example, if you want to get a computer to read handwritten characters, to read sort of \\nhandwritten digits, that actual ly turns out to be amazingly difficult to write a piece of \\nsoftware to take this input, an image of some thing that I wrote and to  figure out just what \\nit is, to translate my cursive handwriting into — to extract the characters I wrote out in \\nlonghand. And other things: One thing that my students and I do is autonomous flight. It \\nturns out to be extremely difficult to sit dow n and write a program to  fly a helicopter.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 2}), Document(page_content=\"turns out to be extremely difficult to sit dow n and write a program to  fly a helicopter.  \\nBut in contrast, if you want to do things like to get software to fl y a helicopter or have \\nsoftware recognize handwritten digits, one very  successful approach is to use a learning \\nalgorithm and have a computer learn by its elf how to, say, recognize your handwriting. \\nAnd in fact, handwritten digit recognition, this is pretty much the only approach that \\nworks well. It uses applications that are hard to program by hand.  \\nLearning algorithms has also made I guess sign ificant inroads in what's sometimes called \\ndatabase mining. So, for example, with the growth of IT and computers, increasingly \\nmany hospitals are keeping around medical reco rds of what sort of patients, what \\nproblems they had, what their prognoses was,  what the outcome was. And taking all of \\nthese medical records, which started to be digitized only about maybe 15 years, applying \\nlearning algorithms to them can turn raw medi cal records into what I might loosely call \\nmedical knowledge in which we start to detect trends in medical practice and even start to \\nalter medical practice as a result of me dical knowledge that's derived by applying \\nlearning algorithms to the sorts of medical r ecords that hospitals have just been building \\nover the last 15, 20 years in an electronic format.  \\nTurns out that most of you probably use learning algorithms — I don't know — I think\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 2}), Document(page_content=\"over the last 15, 20 years in an electronic format.  \\nTurns out that most of you probably use learning algorithms — I don't know — I think \\nhalf a dozen times a day or maybe a dozen  times a day or more, and often without \\nknowing it. So, for example, every time you se nd mail via the US Postal System, turns \\nout there's an algorithm that tries to automa tically read the zip code you wrote on your \\nenvelope, and that's done by a learning al gorithm. So every time you send US mail, you \\nare using a learning algorithm, perhap s without even being aware of it.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 2}), Document(page_content=\"Similarly, every time you write a check, I ac tually don't know the number for this, but a \\nsignificant fraction of checks that you write are processed by a learning algorithm that's \\nlearned to read the digits, so the dolla r amount that you wrote down on your check. So \\nevery time you write a check, there's anot her learning algorithm that you're probably \\nusing without even being aware of it.  \\nIf you use a credit card, or I know at least one phone compan y was doing this, and lots of \\ncompanies like eBay as well that do electr onic transactions, there's a good chance that \\nthere's a learning algorithm in the backgr ound trying to figure out if, say, your credit \\ncard's been stolen or if someone's engaging in a fraudulent transaction.  \\nIf you use a website like Amazon or Netflix that will often recommend books for you to \\nbuy or movies for you to rent or whatever , these are other examples of learning \\nalgorithms that have learned what sorts of th ings you like to buy or what sorts of movies \\nyou like to watch and can therefore give  customized recommendations to you.  \\nJust about a week ago, I had my car serviced, and even there, my car mechanic was trying \\nto explain to me some learning algorithm in th e innards of my car th at's sort of doing its \\nbest to optimize my driving performan ce for fuel efficiency or something.  \\nSo, see, most of us use learning algorithms half a dozen, a dozen, maybe dozens of times \\nwithout even knowing it.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 3}), Document(page_content=\"So, see, most of us use learning algorithms half a dozen, a dozen, maybe dozens of times \\nwithout even knowing it.  \\nAnd of course, learning algorithms are also  doing things like giving us a growing \\nunderstanding of the human genome. So if so meday we ever find a cure for cancer, I bet \\nlearning algorithms will have had a large role in that. That's sort of the thing that Tom \\nworks on, yes?  \\nSo in teaching this class, I sort of have thre e goals. One of them is just to I hope convey \\nsome of my own excitement a bout machine learning to you.  \\nThe second goal is by the end of this class, I hope all of you will be ab le to apply state-of-\\nthe-art machine learning algorithms to whatev er problems you're interested in. And if you \\never need to build a system for reading zi p codes, you'll know how to do that by the end \\nof this class.  \\nAnd lastly, by the end of this class, I reali ze that only a subset of  you are interested in \\ndoing research in machine learning, but by the c onclusion of this class,  I hope that all of \\nyou will actually be well qualified to star t doing research in machine learning, okay?  \\nSo let's say a few words about logistics. The prerequisites of this class are written on one \\nof the handouts, are as follows: In this class, I'm going to assume that all of you have sort \\nof basic knowledge of computer science and kn owledge of the basic computer skills and \\nprinciples. So I assume all of you know what big?O notation, that all of you know about\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 3}), Document(page_content='principles. So I assume all of you know what big?O notation, that all of you know about \\nsort of data structures like  queues, stacks, binary trees , and that all of you know enough \\nprogramming skills to, like, write a simple co mputer program. And it turns out that most', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 3}), Document(page_content=\"of this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 4}), Document(page_content=\"But if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.  \\nSo there are a couple more logisti cal things I should deal with in  this class. One is that, as \\nmost of you know, CS229 is a televised cla ss. And in fact, I guess many of you are \\nprobably watching this at home on TV, so I' m gonna say hi to our home viewers.  \\nSo earlier this year, I appro ached SCPD, which televises th ese classes, about trying to \\nmake a small number of Stanford classes publ icly available or posting the videos on the \\nweb. And so this year, Stanford is actually starting a small pilot program in which we'll \\npost videos of a small number of classes onlin e, so on the Internet in a way that makes it \\npublicly accessible to everyone. I'm very exc ited about that because machine learning in \\nschool, let's get the word out there.  \\nOne of the consequences of this is that — let's see — so videos  or pictures of the students \\nin this classroom will not be posted online, so your images — so don't worry about being \\nby seeing your own face appear on YouTube one day. But the microphones may pick up your voices, so I guess the consequence of that is that because microphones may pick up your voices, no matter how irritated you are at  me, don't yell out swear words in the \\nmiddle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 4}), Document(page_content=\"middle of class, but because there won't be video you can safely sit there and make faces \\nat me, and that won't show, okay?  \\nLet's see. I also handed out this — ther e were two handouts I hope most of you have, \\ncourse information handout. So let me just sa y a few words about parts of these. On the \\nthird page, there's a section that says Online Resources.  \\nOh, okay. Louder? Actually, could you turn up the volume? Testing. Is this better? \\nTesting, testing. Okay, cool. Thanks.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 4}), Document(page_content=\"So all right, online resources. The class has a home page, so it's in on the handouts. I \\nwon't write on the chalkboard — http:// cs229.stanford.edu. And so when there are \\nhomework assignments or things like that, we  usually won't sort of — in the mission of \\nsaving trees, we will usually not give out many handouts in class. So homework \\nassignments, homework solutions will be posted online at the course home page.  \\nAs far as this class, I've also written, a nd I guess I've also revised every year a set of \\nfairly detailed lecture notes that cover the te chnical content of this  class. And so if you \\nvisit the course homepage, you'll also find the detailed lecture notes that go over in detail \\nall the math and equations and so on  that I'll be doing in class.  \\nThere's also a newsgroup, su.class.cs229, also written on the handout. This is a \\nnewsgroup that's sort of a forum for people in  the class to get to  know each other and \\nhave whatever discussions you want to ha ve amongst yourselves. So the class newsgroup \\nwill not be monitored by the TAs and me. But this is a place for you to form study groups \\nor find project partners or discuss homework problems and so on, and it's not monitored \\nby the TAs and me. So feel free to ta lk trash about this class there.  \\nIf you want to contact the teaching staff, pl ease use the email address written down here, \\ncs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 5}), Document(page_content=\"cs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So \\nrather than sending us email individually, if you send email to this account, it will \\nactually let us get back to you maximally quickly with answers to your questions.  \\nIf you're asking questions about homework probl ems, please say in the subject line which \\nassignment and which question the email refers to, since that will also help us to route \\nyour question to the appropriate TA or to me  appropriately and get the response back to \\nyou quickly.  \\nLet's see. Skipping ahead — let's see — for homework, one midterm, one open and term \\nproject. Notice on the honor code. So one thi ng that I think will help you to succeed and \\ndo well in this class and even help you to enjoy this cla ss more is if you form a study \\ngroup.  \\nSo start looking around where you' re sitting now or at the end of class today, mingle a \\nlittle bit and get to know your classmates. I strongly encourage you to form study groups \\nand sort of have a group of people to study with and have a group of your fellow students \\nto talk over these concepts with. You can also  post on the class news group if you want to \\nuse that to try to form a study group.  \\nBut some of the problems sets in this cla ss are reasonably difficult.  People that have \\ntaken the class before may tell you they were very difficult. And just I bet it would be \\nmore fun for you, and you'd probably have a be tter learning experience if you form a\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 5}), Document(page_content=\"more fun for you, and you'd probably have a be tter learning experience if you form a \\nstudy group of people to work with. So I definitely encourage you to do that.  \\nAnd just to say a word on the honor code, whic h is I definitely en courage you to form a \\nstudy group and work together, discuss homew ork problems together. But if you discuss\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 5}), Document(page_content=\"homework problems with other students, then  I'll ask you to sort of go home and write \\ndown your own solutions independe ntly without referring to note s that were taken in any \\nof your joint study sessions.  \\nSo in other words, when you turn in a hom ework problem, what you turn in should be \\nsomething that was reconstructed independe ntly by yourself and w ithout referring to \\nnotes that you took during your  study sessions with other people, okay? And obviously, \\nshowing your solutions to othe rs or copying other solutions  directly is right out.  \\nWe occasionally also reuse problem set questions from previous years so that the \\nproblems are a bit more debugged and work more  smoothly. And as a result of that, I also \\nask you not to look at solutions from previous ye ars, and this includes both sort of official \\nsolutions that we've given out to previous gene rations of this class and previous solutions \\nthat people that have taken this class in previous years may have written out by \\nthemselves, okay?  \\nSadly, in this class, there are usually — sadly, in previous y ears, there have often been a \\nfew honor code violations in this class. And last year, I think I pr osecuted five honor code \\nviolations, which I think is a ridiculously large number. And so just don't work without \\nsolutions, and hopefully there'll be zero honor code  violations this year. I'd love for that \\nto happen.  \\nThe section here on the late homework polic y if you ever want to hand in a homework\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 6}), Document(page_content=\"to happen.  \\nThe section here on the late homework polic y if you ever want to hand in a homework \\nlate, I'll leave you to r ead that yourself.  \\nWe also have a midterm, which is scheduled for Thursday, 8th of November at 6:00 p.m., \\nso please keep that evening free.  \\nAnd let's see. And one more administrative thing I wanted to sa y is about the class \\nproject. So part of the goal of this cla ss is to leave you well eq uipped to apply machine \\nlearning algorithms to a problem or to do rese arch in machine learning. And so as part of \\nthis class, I'll ask you to execute a small resear ch project sort of as a small term project.  \\nAnd what most students do for this is either  apply machine learning to a problem that you \\nfind interesting or investigate some aspect of  machine learning. So to those of you that \\nare either already doing research or to those of you who are in industry, you're taking this \\nfrom a company, one fantastic sort of way to do a class project would be if you apply \\nmachine learning algorithms to a problem that  you're interested in, to a problem that \\nyou're already working on, whether it be a scien ce research problem or sort of a problem \\nin industry where you're trying to get a syst em to work using a learning algorithm.  \\nTo those of you that are not currently doing re search, one great way to do a project would \\nbe if you apply learning algorithms to just pick a problem that you care about. Pick a\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 6}), Document(page_content='be if you apply learning algorithms to just pick a problem that you care about. Pick a \\nproblem that you find interesting, and apply lear ning algorithms to that  and play with the \\nideas and see what happens.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 6}), Document(page_content=\"And let's see. Oh, and the goal of the projec t should really be for you to do a publishable \\npiece of research in machine learning, okay?  \\nAnd if you go to the course website, you'll actuall y find a list of the projects that students \\nhad done last year. And so I'm holding the li st in my hand. You can  go home later and \\ntake a look at it online.  \\nBut reading down this list, I see that last year, there were st udents that ap plied learning \\nalgorithms to control a snake robot. Ther e was a few projects on improving learning \\nalgorithms. There's a project on flying autonomous  aircraft. There was a project actually \\ndone by our TA Paul on improvi ng computer vision algorithms  using machine learning.  \\nThere are a couple of project s on Netflix rankings using learning algorithms; a few \\nmedical robots; ones on segmenting [inaudibl e] to segmenting pieces of the body using \\nlearning algorithms; one on musical instrume nt detection; anot her on irony sequence \\nalignment; and a few algorithms on understandin g the brain neuroscience, actually quite a \\nfew projects on neuroscience; a couple of projects on unde scending fMRI data on brain \\nscans, and so on; another project on market makings, the financial trading. There was an \\ninteresting project on trying to use learning algorithms to decide what is it that makes a \\nperson's face physically attractive. There's a learning algorithm on op tical illusions, and \\nso on.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 7}), Document(page_content=\"person's face physically attractive. There's a learning algorithm on op tical illusions, and \\nso on.  \\nAnd it goes on, so lots of fun projects. A nd take a look, then come up with your own \\nideas. But whatever you find cool and interest ing, I hope you'll be able to make machine \\nlearning a project out of it. Yeah, question?  \\nStudent : Are these gro up projects?  \\nInstructor (Andrew Ng): Oh, yes, thank you.  \\nStudent : So how many people can be in a group?  \\nInstructor (Andrew Ng): Right. So projects can be done in  groups of up to three people. \\nSo as part of forming study groups, later t oday as you get to know your classmates, I \\ndefinitely also encourage you to grab two ot her people and form a group of up to three \\npeople for your project, okay? And just start brainstorming ideas for now amongst \\nyourselves. You can also come and talk to me or the TAs if you want to brainstorm ideas \\nwith us.  \\nOkay. So one more organizational ques tion. I'm curious, how many of you know \\nMATLAB? Wow, cool, quite a lot. Okay. So as part of the — act ually how many of you \\nknow Octave or have used Octave ? Oh, okay, much smaller number.  \\nSo as part of this class, especially in the homeworks, we'll ask you to implement a few \\nprograms, a few machine learning algorithms as  part of the homeworks. And most of\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 7}), Document(page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 8}), Document(page_content='into his office and he said, \"Oh, professo r, professor, thank you so much for your \\nmachine learning class. I learned so much from it. There\\'s this stuff that I learned in your \\nclass, and I now use every day. And it\\'s help ed me make lots of money, and here\\'s a \\npicture of my big house.\"  \\nSo my friend was very excited. He said, \"W ow. That\\'s great. I\\'m glad to hear this \\nmachine learning stuff was actually useful. So what was it that you learned? Was it \\nlogistic regression? Was it the PCA? Was it the data ne tworks? What was it that you \\nlearned that was so helpful?\" And the student said, \"Oh, it was the MATLAB.\"  \\nSo for those of you that don\\'t know MATLAB yet, I hope you do learn it. It\\'s not hard, \\nand we\\'ll actually have a short MATLAB tutori al in one of the discussion sections for \\nthose of you that don\\'t know it.  \\nOkay. The very last piece of logistical th ing is the discussion s ections. So discussion \\nsections will be taught by the TAs, and atte ndance at discussion sections is optional, \\nalthough they\\'ll also be recorded and televi sed. And we\\'ll use the discussion sections \\nmainly for two things. For the next two or th ree weeks, we\\'ll use the discussion sections \\nto go over the prerequisites to this class or if some of you haven\\'t seen probability or \\nstatistics for a while or maybe algebra, we\\'ll go over those in the discussion sections as a \\nrefresher for those of you that want one.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 8}), Document(page_content=\"statistics for a while or maybe algebra, we'll go over those in the discussion sections as a \\nrefresher for those of you that want one.  \\nLater in this quarter, we'll also use the disc ussion sections to go over extensions for the \\nmaterial that I'm teaching in the main lectur es. So machine learning is a huge field, and \\nthere are a few extensions that we really want  to teach but didn't have time in the main \\nlectures for.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 8}), Document(page_content=\"So later this quarter, we'll use the discussion sections to talk about things like convex \\noptimization, to talk a little bit about hidde n Markov models, which is a type of machine \\nlearning algorithm for modeling time series and a few other things, so  extensions to the \\nmaterials that I'll be covering in the main  lectures. And attend ance at the discussion \\nsections is optional, okay?  \\nSo that was all I had from l ogistics. Before we move on to start talking a bit about \\nmachine learning, let me check what questions you have. Yeah?  \\nStudent : [Inaudible] R or something like that?  \\nInstructor (Andrew Ng) : Oh, yeah, let's see, right. So our policy has been that you're \\nwelcome to use R, but I would strongly advi se against it, mainly because in the last \\nproblem set, we actually supply some code th at will run in Octave  but that would be \\nsomewhat painful for you to translate into R yourself. So for your other assignments, if \\nyou wanna submit a solution in R, that's fi ne. But I think MATLAB is actually totally \\nworth learning. I know R and MATLAB, and I personally end up using MATLAB quite a \\nbit more often for various reasons. Yeah?  \\nStudent : For the [inaudible] pr oject [inaudible]?  \\nInstructor (Andrew Ng) : So for the term project, you're welcome to do it in smaller \\ngroups of three, or you're welcome to do it by yo urself or in groups of two. Grading is the \\nsame regardless of the group size, so with  a larger group, you probably — I recommend\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 9}), Document(page_content=\"same regardless of the group size, so with  a larger group, you probably — I recommend \\ntrying to form a team, but it's actually totally fine to do it in a sma ller group if you want.  \\nStudent : [Inaudible] what language [inaudible]?  \\nInstructor (Andrew Ng): So let's see. There is no C programming in this class other \\nthan any that you may choose to do yourself in your project. So all the homeworks can be \\ndone in MATLAB or Octave, and let's see. A nd I guess the program prerequisites is more \\nthe ability to understand big?O notation and know ledge of what a data structure, like a \\nlinked list or a queue or bina ry treatments, more so than  your knowledge of C or Java \\nspecifically. Yeah?  \\nStudent : Looking at the end semester project, I mean, what exactly will you be testing \\nover there? [Inaudible]?  \\nInstructor (Andrew Ng) : Of the project?  \\nStudent : Yeah.  \\nInstructor (Andrew Ng) : Yeah, let me answer that later.  In a couple of weeks, I shall \\ngive out a handout with guidelines for the pr oject. But for now, we should think of the \\ngoal as being to do a cool piec e of machine learning work that  will let you experience the\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 9}), Document(page_content=\"joys of machine learning firs thand and really try to think about doing a publishable piece \\nof work.  \\nSo many students will try to build a cool machine learning application. That's probably \\nthe most common project. Some students will try to improve state-of-the-art machine \\nlearning. Some of those projects are also very  successful. It's a littl e bit harder to do. And \\nthere's also a smaller minority of students th at will sometimes try to prove — develop the \\ntheory of machine learning further or try to  prove theorems about machine learning. So \\nthey're usually great projects of all of those types with applications and machine learning \\nbeing the most common. Anything else? Okay, cool.  \\nSo that was it for logistics. Let's talk about  learning algorithms. So can I have the laptop \\ndisplay, please, or the projector? Actually, co uld you lower the big sc reen? Cool. This is \\namazing customer service. Thank you. I see. Okay, cool. Okay. No, that's fine. I see. \\nOkay. That's cool. Thanks. Okay.  \\nBig screen isn't working toda y, but I hope you can read things  on the smaller screens out \\nthere. Actually, [inaudible] I think this room just got a new projector that — someone \\nsent you an excited email — was it just on Frid ay? — saying we just got a new projector \\nand they said 4,000-to-1 something or othe r brightness ratio. I don't know. Someone was \\nvery excited about the new projector in this room, but I guess we'll see that in operation \\non Wednesday.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 10}), Document(page_content='very excited about the new projector in this room, but I guess we\\'ll see that in operation \\non Wednesday.  \\nSo start by talking about what machine learni ng is. What is machine learning? Actually, \\ncan you read the text out there? Raise your hand if the text on the small screens is legible. \\nOh, okay, cool, mostly legible. Okay. So I\\'ll just read it out.  \\nSo what is machine learning? Way back in  about 1959, Arthur Samuel defined machine \\nlearning informally as the [inaudible] that gives computers to learn — [inaudible] that \\ngives computers the ability to learn without  being explicitly programmed. So Arthur \\nSamuel, so way back in the history of m achine learning, actually did something very \\ncool, which was he wrote a checkers progr am, which would play games of checkers \\nagainst itself.  \\nAnd so because a computer can play thousands  of games against itself relatively quickly, \\nArthur Samuel had his program play thousands  of games against itself, and over time it \\nwould start to learn to rec ognize patterns which led to wi ns and patterns which led to \\nlosses. So over time it learned things like that , \"Gee, if I get a lot of pieces taken by the \\nopponent, then I\\'m more likely to lose than win,\" or, \"Gee, if I get my pieces into a \\ncertain position, then I\\'m especially li kely to win rather than lose.\"  \\nAnd so over time, Arthur Samuel had a check ers program that woul d actually learn to', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 10}), Document(page_content='And so over time, Arthur Samuel had a check ers program that woul d actually learn to \\nplay checkers by learning what are the sort of  board positions that tend to be associated \\nwith wins and what are the boa rd positions that tend to be associated with losses. And \\nway back around 1959, the amazing thing about this was that his program actually \\nlearned to play checkers much better than Arthur Samuel  himself could.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 10}), Document(page_content=\"So even today, there are some people that say, well, computers can't do anything that \\nthey're not explicitly programmed to. And Ar thur Samuel's checkers program was maybe \\nthe first I think really convi ncing refutation of this clai m. Namely, Arthur Samuel \\nmanaged to write a checkers program that could play checkers much better than he \\npersonally could, and this is an instance of maybe computers learning to do things that \\nthey were not programmed explicitly to do.  \\nHere's a more recent, a more modern, more formal definition of machine learning due to \\nTom Mitchell, who says that a well-posed le arning problem is defined as follows: He \\nsays that a computer program is set to lear n from an experience E with respect to some \\ntask T and some performance measure P if  its performance on T as measured by P \\nimproves with experience E. Okay. So not  only is it a definition, it even rhymes.  \\nSo, for example, in the case of checkers, th e experience E that a program has would be \\nthe experience of playing lots of games of checkers against itself, say. The task T is the \\ntask of playing checkers, a nd the performance measure P will be something like the \\nfraction of games it wins against a cert ain set of human opponents. And by this \\ndefinition, we'll say that Arthur Samuel's ch eckers program has learned to play checkers, \\nokay?  \\nSo as an overview of what we're going to do in this class, this class is sort of organized\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 11}), Document(page_content=\"okay?  \\nSo as an overview of what we're going to do in this class, this class is sort of organized \\ninto four major sections. We're gonna talk about four major topics in this class, the first \\nof which is supervised learning. So le t me give you an example of that.  \\nSo suppose you collect a data set of housing prices. And one of the TAs, Dan Ramage, \\nactually collected a data set for me last week to use in the example later. But suppose that \\nyou go to collect statistics about how much hous es cost in a certain geographic area. And \\nDan, the TA, collected data from housing pr ices in Portland, Oregon. So what you can do \\nis let's say plot the square footage of the house against the list price of  the house, right, so \\nyou collect data on a bunch of houses. And let' s say you get a data set like this with \\nhouses of different sizes that are li sted for different amounts of money.  \\nNow, let's say that I'm trying to sell a hous e in the same area as Portland, Oregon as \\nwhere the data comes from. Let's say I have a hou se that's this size in square footage, and \\nI want an algorithm to tell me about how much should I expect my house to sell for. So there are lots of ways to do this, and some of you may have seen elements of what I'm \\nabout to say before.  \\nSo one thing you could do is look at this data and maybe put a straight  line to it. And then \\nif this is my house, you may then look at th e straight line and predict that my house is\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 11}), Document(page_content=\"if this is my house, you may then look at th e straight line and predict that my house is \\ngonna go for about that much money, right? Ther e are other decisions that we can make, \\nwhich we'll talk about later, which is, well, what if I don' t wanna put a straight line? \\nMaybe I should put a quadratic function to it. Ma ybe that fits the data a little bit better. \\nYou notice if you do that, the price of my house goes up a bit, so that'd be nice.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 11}), Document(page_content=\"And this sort of learning pr oblem of learning to predict hous ing prices is an example of \\nwhat's called a supervised learning problem. And the reason that it's called supervised \\nlearning is because we're providing the al gorithm a data set of a bunch of square \\nfootages, a bunch of housing sizes, and as well as sort of the right answer of what the \\nactual prices of a number  of houses were, right?  \\nSo we call this supervised learning because we're supervising the algorithm or, in other \\nwords, we're giving the algorithm the, quote,  right answer for a number of houses. And \\nthen we want the algorithm to learn the a ssociation between the inputs and the outputs \\nand to sort of give us more of the right answers, okay?  \\nIt turns out this specific exam ple that I drew here is an example of something called a \\nregression problem. And the term regression sort of refers to the fact that the variable \\nyou're trying to predict is a continuous value and price.  \\nThere's another class of supervised learning problems which we'll talk about, which are \\nclassification problems. And so, in a classifi cation problem, the variab le you're trying to \\npredict is discreet rather than continuous . So as one specific example — so actually a \\nstandard data set you can download online [i naudible] that lots of machine learning \\npeople have played with. Let's say you collect  a data set on breast cancer tumors, and you\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 12}), Document(page_content=\"people have played with. Let's say you collect  a data set on breast cancer tumors, and you \\nwant to learn the algorithm to predict wh ether or not a certai n tumor is malignant. \\nMalignant is the opposite of benign, right, so ma lignancy is a sort of harmful, bad tumor. \\nSo we collect some number of features, some  number of properties of these tumors, and \\nfor the sake of sort of having a simple [inaudi ble] explanation, let's just say that we're \\ngoing to look at the size of the tumor and depe nding on the size of the tumor, we'll try to \\nfigure out whether or not the tu mor is malignant or benign.  \\nSo the tumor is either malignant or benign, and so  the variable in the Y axis is either zero \\nor 1, and so your data set ma y look something like that, righ t? And that's 1 and that's \\nzero, okay? And so this is an example of a classification problem where the variable \\nyou're trying to predict is a discreet value. It 's either zero or 1.  \\nAnd in fact, more generally, there will be many learning problems where we'll have more \\nthan one input variable, more than one input f eature and use more than one variable to try \\nto predict, say, whether a tumor is malignant  or benign. So, for example, continuing with \\nthis, you may instead have a data  set that looks like this. I'm gonna part this data set in a \\nslightly different way now. And I'm making this  data set look much cleaner than it really \\nis in reality for illustration, okay?\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 12}), Document(page_content='slightly different way now. And I\\'m making this  data set look much cleaner than it really \\nis in reality for illustration, okay?  \\nFor example, maybe the crosses indicate ma lignant tumors and the \"O\"s may indicate \\nbenign tumors. And so you may have a data se t comprising patients of  different ages and \\nwho have different tumor sizes and where a cross indicates a mali gnant tumor, and an \\n\"O\" indicates a benign tumor. And you may want  an algorithm to learn to predict, given a \\nnew patient, whether their tumo r is malignant or benign.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 12}), Document(page_content=\"So, for example, what a learning algorithm ma y do is maybe come in and decide that a \\nstraight line like that separates the two classes of tumors really well, and so if you have a \\nnew patient who's age and tumor size fall over there, then the algorithm may predict that \\nthe tumor is benign rather than malignant, oka y? So this is just another example of \\nanother supervised learning problem and another classification problem.  \\nAnd so it turns out that one of the issues we' ll talk about later in this class is in this \\nspecific example, we're going to try to predic t whether a tumor is malignant or benign \\nbased on two features or based on two inputs, namely the age of the patient and the tumor \\nsize. It turns out that when you look at a real  data set, you find th at learning algorithms \\noften use other sets of features . In the breast cancer data ex ample, you also use properties \\nof the tumors, like clump thic kness, uniformity of cell size, uniformity of cell shape, \\n[inaudible] adhesion and so on, so va rious other medical properties.  \\nAnd one of the most interesting things we'll ta lk about later this quarter is what if your \\ndata doesn't lie in a two-dimensional or th ree-dimensional or sort of even a finite \\ndimensional space, but is it possible — what if your data actually lies in an infinite \\ndimensional space? Our plots here are two-dime nsional space. I can't plot you an infinite\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 13}), Document(page_content=\"dimensional space? Our plots here are two-dime nsional space. I can't plot you an infinite \\ndimensional space, right? And so it turns out that one of the most successful classes of \\nmachine learning algorithms — some may call support vector machines — actually takes \\ndata and maps data to an infinite dimensi onal space and then does classification using not \\ntwo features like I've done  here, but an infinite number of features.  \\nAnd that will actually be one of the most fa scinating things we talk about when we go \\ndeeply into classification al gorithms. And it's actually an in teresting question, right, so \\nthink about how do you even represent an in finite dimensional vector in computer \\nmemory? You don't have an infinite amount of computers. How do you even represent a \\npoint that lies in an infinite dimensional sp ace? We'll talk about that when we get to \\nsupport vector machines, okay?  \\nSo let's see. So that was supervised learning. The second of the four major topics of this \\nclass will be learning theory. So I have a friend who teaches math at a different \\nuniversity, not at Stanford, and when you talk to  him about his work and what he's really \\nout to do, this friend of mine will — he's a ma th professor, right? — this friend of mine \\nwill sort of get the look of wonder in his eyes, and he'll tell you about how in his \\nmathematical work, he feels like he's disc overing truth and beauty in the universe. And\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 13}), Document(page_content=\"mathematical work, he feels like he's disc overing truth and beauty in the universe. And \\nhe says it in sort of a real ly touching, sincere way, and then  he has this — you can see it \\nin his eyes — he has this deep appreciation of the truth and beauty  in the universe as \\nrevealed to him by the math he does.  \\nIn this class, I'm not gonna do any truth and beauty. In this class,  I'm gonna talk about \\nlearning theory to try to convey to y ou an understanding of how and why learning \\nalgorithms work so that we can apply these lear ning algorithms as effectively as possible.  \\nSo, for example, it turns out you can prove su rprisingly deep theorems on when you can \\nguarantee that a learning algorithm will wo rk, all right? So think about a learning\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 13}), Document(page_content=\"algorithm for reading zip codes. When can  you prove a theorem guaranteeing that a \\nlearning algorithm will be at least 99.9 per cent accurate on reading zip codes? This is \\nactually somewhat surprising. We actually prove theorems showing when you can expect \\nthat to hold.  \\nWe'll also sort of delve into learning theo ry to try to understand what algorithms can \\napproximate different functions well and also  try to understand things like how much \\ntraining data do you need? So how many exampl es of houses do I need in order for your \\nlearning algorithm to recognize the pattern be tween the square footage of a house and its \\nhousing price? And this will help  us answer questions like if you're trying to design a \\nlearning algorithm, should you be spending more time collecting more data or is it a case \\nthat you already have enough data; it would be  a waste of time to try to collect more. \\nOkay?  \\nSo I think learning algorithms are a very powerful tool that  as I walk around sort of \\nindustry in Silicon Valley or as I work with  various businesses in CS and outside CS, I \\nfind that there's often a huge differen ce between how well so meone who really \\nunderstands this stuff can appl y a learning algorithm versus so meone who sort of gets it \\nbut sort of doesn't.  \\nThe analogy I like to think of  is imagine you were going to a carpentry school instead of \\na machine learning class, right? If you go to a carpentry school, they can give you the\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 14}), Document(page_content=\"a machine learning class, right? If you go to a carpentry school, they can give you the \\ntools of carpentry. They'll give you a hamme r, a bunch of nails, a screwdriver or \\nwhatever. But a master carpenter will be able to  use those tools far better than most of us \\nin this room. I know a carpen ter can do things with a hammer and nail that I couldn't \\npossibly. And it's actually a littl e bit like that in machine learning, too. One thing that's \\nsadly not taught in many courses on machine l earning is how to take the tools of machine \\nlearning and really, really apply them well.  \\nSo in the same way, so the tools of machin e learning are I wanna say quite a bit more \\nadvanced than the tools of carpentry. Maybe a carpenter will disagree . But a large part of \\nthis class will be just givi ng you the raw tools of machine learning, just the algorithms \\nand so on. But what I plan to do throughout this entire quarter, not just in the segment of \\nlearning theory, but actually as a theme r unning through everything I do this quarter, will \\nbe to try to convey to you the skills to real ly take the learning al gorithm ideas and really \\nto get them to work on a problem.  \\nIt's sort of hard for me to stand here and say how big a deal that is, but when I walk \\naround companies in Silicon Valley, it's co mpletely not uncommon for me to see \\nsomeone using some machine learning algorith m and then explain to me what they've\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 14}), Document(page_content=\"someone using some machine learning algorith m and then explain to me what they've \\nbeen doing for the last six months, and I go, oh, gee, it should have been obvious from \\nthe start that the last six months, you've been wa sting your time, right?  \\nAnd so my goal in this class, running th rough the entire quarter , not just on learning \\ntheory, is actually not only to give you the tools of m achine learning, but to teach you \\nhow to use them well. And I've noticed this  is something that really not many other\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 14}), Document(page_content='classes teach. And this is something I\\'m rea lly convinced is a huge deal, and so by the \\nend of this class, I hope all of you will be master carpenters. I hope all of you will be \\nreally good at applying these learning algor ithms and getting them to work amazingly \\nwell in many problems. Okay?  \\nLet\\'s see. So [inaudible] the board. After lear ning theory, there\\'s a nother class of learning \\nalgorithms that I then want to teach you a bout, and that\\'s unsupervised learning. So you \\nrecall, right, a little ea rlier I drew an example like this , right, where you have a couple of \\nfeatures, a couple of input vari ables and sort of malignant tumors and benign tumors or \\nwhatever. And that was an example of a s upervised learning problem because the data \\nyou have gives you the right answer for each of your patients. The data tells you this \\npatient has a malignant tumor;  this patient has a benign tumor. So it had the right \\nanswers, and you wanted the algorithm to just produce more of the same.  \\nIn contrast, in an unsupervised learning problem , this is the sort of data you get, okay? \\nWhere speaking loosely, you\\'re given a data se t, and I\\'m not gonna tell you what the right \\nanswer is on any of your data. I\\'m just gonna give you a data set and I\\'m gonna say, \"Would you please find interesting structure in this data set?\" So that\\'s the unsupervised \\nlearning problem where you\\'re sort of not given the right answer for everything.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 15}), Document(page_content=\"learning problem where you're sort of not given the right answer for everything.  \\nSo, for example, an algorithm may find structure in the data in the form of the data being \\npartitioned into two clusters, or clustering is  sort of one example of an unsupervised \\nlearning problem.  \\nSo I hope you can see this. It turns out that th ese sort of unsupervised  learning algorithms \\nare also used in many problems. This is a scr een shot — this is a picture I got from Sue \\nEmvee, who's a PhD student here, who is a pplying unsupervised learning algorithms to \\ntry to understand gene data, so is trying to  look at genes as individuals and group them \\ninto clusters based on properties of what ge nes they respond to — based on properties of \\nhow the genes respond to different experiments.  \\nAnother interesting application of [inaudible] sorts of clus tering algorithms is actually \\nimage processing, this which I got from Steve Gules, who's another PhD student. It turns \\nout what you can do is if you give this sort of  data, say an image, to certain unsupervised \\nlearning algorithms, they will then learn to group pixels together and say, gee, this sort of \\npixel seems to belong together , and that sort of pixel seems to belong together.  \\nAnd so the images you see on the bottom — I guess you can just barely see them on there \\n— so the images you see on the bottom are groupings — are what the algorithm has done\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 15}), Document(page_content='— so the images you see on the bottom are groupings — are what the algorithm has done \\nto group certain pixels together. On a small di splay, it might be easier to just look at the \\nimage on the right. The two images on the botto m are two sort of identical visualizations \\nof the same grouping of the pixe ls into [inaudible] regions.  \\nAnd so it turns out that this sort of clustering algorithm or this sort of unsupervised \\nlearning algorithm, which learns  to group pixels together, it turns out to be useful for \\nmany applications in vision, in co mputer vision image processing.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 15}), Document(page_content=\"I'll just show you one example, and this is a rather cool one that two students, Ashutosh \\nSaxena and Min Sun here did, wh ich is given an image like this, right? This is actually a \\npicture taken of the Stanford campus. You can apply that sort of cl ustering algorithm and \\ngroup the picture into regions. Let me actually blow that up so that you can see it more \\nclearly. Okay. So in the middle, you see the lines sort of groupi ng the image together, \\ngrouping the image into [inaudible] regions.  \\nAnd what Ashutosh and Min did was they then  applied the learning algorithm to say can \\nwe take this clustering and us e it to build a 3D model of the world? And so using the \\nclustering, they then had a lear ning algorithm try to learn what the 3D structure of the \\nworld looks like so that they could come up with a 3D model that you can sort of fly \\nthrough, okay? Although many people used to th ink it's not possible to take a single \\nimage and build a 3D model, but using a lear ning algorithm and that sort of clustering \\nalgorithm is the first step. They were able to.  \\nI'll just show you one more example. I like this  because it's a picture of Stanford with our \\nbeautiful Stanford campus. So again, taking th e same sort of clustering algorithms, taking \\nthe same sort of unsupervised learning algor ithm, you can group the pixels into different\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 16}), Document(page_content=\"the same sort of unsupervised learning algor ithm, you can group the pixels into different \\nregions. And using that as a pre-processing step, they eventually built this sort of 3D model of Stanford campus in a single picture.  You can sort of walk  into the ceiling, look \\naround the campus. Okay? This actually turned out to be a mix of supervised and \\nunsupervised learning, but the unsupervised lear ning, this sort of cl ustering was the first \\nstep.  \\nSo it turns out these sorts of unsupervised — clustering algorithms are actually routinely \\nused for many different problems, things like organizing computing clusters, social \\nnetwork analysis, market segmentation, so if you're a marketer and you want to divide your market into different segments or diffe rent groups of people to market to them \\nseparately; even for astronomical data an alysis and understanding how galaxies are \\nformed. These are just a sort of small sample  of the applications of unsupervised learning \\nalgorithms and clustering algorithms that we 'll talk about later in this class.  \\nJust one particularly cool example of an uns upervised learning algorithm that I want to \\ntell you about. And to motivate that, I'm gonna  tell you about what's called the cocktail \\nparty problem, which is imagine that you're at  some cocktail party a nd there are lots of \\npeople standing all over. And you know how it is, right, if you're at a large party,\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 16}), Document(page_content=\"people standing all over. And you know how it is, right, if you're at a large party, \\neveryone's talking, it can be sometimes very hard  to hear even the person in front of you. \\nSo imagine a large cocktail party with lots of people. So the problem is, is that all of these people talking, can you separate out the voice of just the person you're interested in \\ntalking to with all this  loud background noise?  \\nSo I'll show you a specific example in a second, but here's a cocktail party that's I guess \\nrather sparsely attended by just two people.  But what we're gonna do is we'll put two \\nmicrophones in the room, okay? And so becau se the microphones are just at slightly \\ndifferent distances to the two people, and th e two people may speak in slightly different \\nvolumes, each microphone will pick up an overl apping combination of these two people's\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 16}), Document(page_content=\"voices, so slightly different overlapping voice s. So Speaker 1's voice may be more loud \\non Microphone 1, and Speaker 2's voice may be louder on Microphone 2, whatever.  \\nBut the question is, given these microphone reco rdings, can you separate out the original \\nspeaker's voices? So I'm gonna play some audi o clips that were collected by Tai Yuan \\nLee at UCSD. I'm gonna actually play for you the original raw microphone recordings \\nfrom this cocktail party. So this is the Microphone 1:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\nUno, dos, tres, cuatro, cinco, seis, siete, ocho, nueve, diez.  \\nInstructor (Andrew Ng) : So it's a fascinating cocktail party with people counting from \\none to ten. This is the second microphone:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\nUno, dos, tres, cuatro, cinco, seis, siete, ocho, nueve, diez.  \\nInstructor (Andrew Ng) : Okay. So in supervised learning, we don't know what the \\nright answer is, right? So what we're goi ng to do is take exactly the two microphone \\nrecordings you just heard and give it to an  unsupervised learning algorithm and tell the \\nalgorithm which of these discover structure in the data [inaudible] or  what structure is \\nthere in this data? And we actually don't know what the right answer is offhand.  \\nSo give this data to an unsupervised lear ning algorithm, and what the algorithm does in\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 17}), Document(page_content=\"So give this data to an unsupervised lear ning algorithm, and what the algorithm does in \\nthis case, it will discover that this data can actually be explained by two independent \\nspeakers speaking at the same time, and it can  further separate out the two speakers for \\nyou. So here's Output 1 of the algorithm:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nInstructor (Andrew Ng) : And there's the second algorithm:  \\nMicrophone 2:\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 17}), Document(page_content=\"Uno, dos, tres, cuatro, cinco, seis, siete, ocho, nueve, diez.  \\nInstructor (Andrew Ng): And so the algorithm discovers  that, gee, the structure \\nunderlying the data is really th at there are two sources of so und, and here they are. I'll \\nshow you one more example. This is a, well, th is is a second sort of different pair of \\nmicrophone recordings:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\n[Music playing.]  Instructor (Andrew Ng): So the poor guy is not at a cocktail party. He's talking to his \\nradio. There's the second recording:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nMicrophone 2:  \\n[Music playing.]  \\nInstructor (Andrew Ng) : Right. And we get this data. It's the same unsupervised \\nlearning algorithm. The algorithm is actually called independent component analysis, and \\nlater in this quarter, you'll see why. And then output's the following:  \\nMicrophone 1:  \\nOne, two, three, four, five, six, seven, eight, nine, ten.  \\nInstructor (Andrew Ng): And that's the second one:  \\nMicrophone 2:  \\n[Music playing.]  \\nInstructor (Andrew Ng): Okay. So it turns out that be yond solving the cocktail party \\nalgorithm, this specific cla ss of unsupervised learning algor ithms are also applied to a \\nbunch of other problems, like in text proces sing or understanding f unctional grading and \\nmachine data, like the magneto-encephalogram would be an EEG data. We'll talk about\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 18}), Document(page_content=\"machine data, like the magneto-encephalogram would be an EEG data. We'll talk about \\nthat more when we go and describe ICA or independent component analysis algorithms, \\nwhich is what you just saw.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 18}), Document(page_content=\"And as an aside, this algorithm I just showed you, it seems like it must be a pretty \\ncomplicated algorithm, right, to take this overlapping audio streams and separate them \\nout. It sounds like a pretty complicated thi ng to do. So you're gonna ask how complicated \\nis it really to implement an  algorithm like this? It turns out if you do it in MATLAB, you \\ncan do it in one line of code.  \\nSo I got this from Samuel Wyse at Toront o, U of Toronto, and the example I showed you \\nactually used a more complicated ICA algorithm than this. But nonetheless, I guess this is \\nwhy for this class I'm going to ask you to  do most of your programming in MATLAB and \\nOctave because if you try to implement the sa me algorithm in C or Java or something, I \\ncan tell you from personal, painful experien ce, you end up writing pages and pages of \\ncode rather than relatively few lines of code. I'll also mention that it did take researchers \\nmany, many years to come up with that one line of code, so this is not easy.  \\nSo that was unsupervised learning, and then the last of the four major topics I wanna tell \\nyou about is reinforcement learning. And this  refers to problems where you don't do one-\\nshot decision-making. So, for example, in  the supervised learning cancer prediction \\nproblem, you have a patient come in, you predict that the cancer is malignant or benign. \\nAnd then based on your prediction, maybe the pa tient lives or dies, and then that's it,\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 19}), Document(page_content=\"And then based on your prediction, maybe the pa tient lives or dies, and then that's it, \\nright? So you make a decision and then there's a consequence. You either got it right or \\nwrong. In reinforcement learning problems, you are usually asked to make a sequence of \\ndecisions over time.  \\nSo, for example, this is something that my students and I work on. If I give you the keys \\nto an autonomous helicopter — we actually ha ve this helicopter here at Stanford, — how \\ndo you write a program to make it fly, ri ght? You notice that if you make a wrong \\ndecision on a helicopter, the consequence of crashing it may not happen until much later. And in fact, usually you need to make a w hole sequence of bad decisions to crash a \\nhelicopter. But conversely, you al so need to make a whole sequence of good decisions in \\norder to fly a helic opter really well.  \\nSo I'm gonna show you some fun videos of lear ning algorithms flying helicopters. This is \\na video of our helicopter at Stanford flying using a contro ller that was learned using a \\nreinforcement learning algorithm. So this wa s done on the Stanford football field, and \\nwe'll zoom out the camera in a second. You'll sort of see th e trees planted in the sky. So \\nmaybe this is one of the most difficult aer obatic maneuvers flown on any helicopter under \\ncomputer control. And this controller, which is very, very hard for a human to sit down \\nand write out, was learned using one of these reinforcement learning algorithms.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 19}), Document(page_content='and write out, was learned using one of these reinforcement learning algorithms.  \\nJust a word about that: The basic idea behi nd a reinforcement learning algorithm is this \\nidea of what\\'s called a reward  function. What we have to think about is imagine you\\'re \\ntrying to train a dog. So every time y our dog does something good, you say, \"Good dog,\" \\nand you reward the dog. Every time your dog does something bad, you go, \"Bad dog,\" right? And hopefully, over time, your dog will lear n to do the right things to get more of \\nthe positive rewards, to get mo re of the \"Good dogs\" and to ge t fewer of the \"Bad dogs.”', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 19}), Document(page_content='So the way we teach a helicopter to fly or any of these robots is sort of the same thing. \\nEvery time the helicopter crashes, we go, \"B ad helicopter,\" and every time it does the \\nright thing, we go, \"Good helicopter, \" and over time it learns how to control itself so as to \\nget more of these positive rewards.  \\nSo reinforcement learning is — I think of it as a way for you to specify what you want \\ndone, so you have to specify what is a \"good dog\" and what is a \"bad dog\" behavior. And \\nthen it\\'s up to the learning algorithm to  figure out how to maximize the \"good dog\" \\nreward signals and minimize the \"bad dog\" punishments.  \\nSo it turns out reinforcement learning is applie d to other problems in robotics. It\\'s applied \\nto things in web crawling and so on. But it\\'s just  cool to show videos, so let me just show \\na bunch of them. This learning algorithm was actually implemented by our head TA, Zico, of programming a four-legged dog. I guess Sam Shriver in this class also worked \\non the project and Peter Renfrew and Mike and a few others. But I guess this really is a \\ngood dog/bad dog since it\\'s a robot dog.  \\nThe second video on the right, some of the st udents, I guess Peter, Zico, Tonca working \\non a robotic snake, again using learning algorith ms to teach a snake robot to climb over \\nobstacles.  \\nBelow that, this is kind of a fun example.  Ashutosh Saxena and Jeff Michaels used \\nlearning algorithms to teach a car how to  drive at reasonably high speeds off roads', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 20}), Document(page_content=\"learning algorithms to teach a car how to  drive at reasonably high speeds off roads \\navoiding obstacles.  \\nAnd on the lower right, that's a robot program med by PhD student Eva Roshen to teach a \\nsort of somewhat strangely configured robot how to get on top of an obstacle, how to get \\nover an obstacle. Sorry. I know the video's kind of small. I hope you can sort of see it. \\nOkay?  \\nSo I think all of these are robots that I thi nk are very difficult to hand-code a controller \\nfor by learning these sorts of l earning algorithms. You can in relatively short order get a \\nrobot to do often pretty amazing things.  \\nOkay. So that was most of what I wanted to say today. Just a couple more last things, but \\nlet me just check what questions you have righ t now. So if there are no questions, I'll just \\nclose with two reminders, which are after class today or as you start to talk with other \\npeople in this class, I just encourage you again to start to form project partners, to try to \\nfind project partners to do your project with. And also, this is a good time to start forming \\nstudy groups, so either talk to your friends  or post in the newsgroup, but we just \\nencourage you to try to star t to do both of those today, okay? Form study groups, and try \\nto find two other project partners.  \\nSo thank you. I'm looking forward to teaching this class, and I'll see you in a couple of \\ndays.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 20}), Document(page_content='[End of Audio]  \\nDuration: 69 minutes', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 21}), Document(page_content=\"MachineLearning-Lecture02  \\nInstructor (Andrew Ng) :All right, good morning, welcom e back. So before we jump \\ninto today's material, I just have one admini strative announcement, which is graders. So I \\nguess sometime next week, we'll hand out the fi rst homework assignment for this class.  \\nIs this loud enough, by the way? Can people in  the back hear me? No. Can you please \\nturn up the mic a bit louder? Is this bette r? Is this okay? This is okay? Great.  \\nSo sometime next week, we'll hand out the firs t problem sets and it'll be two weeks after \\nthat, and the way we grade homework problems in this class is by some combination of \\nTAs and graders, where graders are usually me mbers – students currently in the class.  \\nSo in maybe about a week or so, I'll email the class to solicit applica tions for those of you \\nthat might be interested in becoming graders fo r this class, and ther e's usually sort of a \\nfun thing to do. So four times this quarter, th e TAs, and the graders, and I will spend one \\nevening staying up late and grad ing all the homework problems.  \\nFor those of you who that have never taught a cla ss before, or sort of been a grader, it's an \\ninteresting way for you to see, you know, what  the other half of the teaching experience \\nis. So the students that grade for the first time sort of get to learn about what it is that \\nreally makes a difference between a good so lution and amazing solution. And to give\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 0}), Document(page_content=\"really makes a difference between a good so lution and amazing solution. And to give \\neveryone to just how we do points assignments, or what is it that causes a solution to get \\nfull marks, or just how to write amazing so lutions. Becoming a grad er is usually a good \\nway to do that.  \\nGraders are paid positions and you also get free  food, and it's usually fun for us to sort of \\nhang out for an evening and grade all the a ssignments. Okay, so I will send email. So \\ndon't email me yet if you want to be a grader. I'll send email to the entire class later with \\nthe administrative details and to solicit app lications. So you can email us back then, to \\napply, if you'd be interested in being a grader.  \\nOkay, any questions about that? All right, okay, so let's get started with today's material. \\nSo welcome back to the second lecture. What  I want to do today is talk about linear \\nregression, gradient descent, and the norma l equations. And I should also say, lecture \\nnotes have been posted online and so if some  of the math I go over today, I go over rather \\nquickly, if you want to see every equation wr itten out and work through the details more \\nslowly yourself, go to the course homepage and download detailed lecture notes that \\npretty much describe all the mathematical, te chnical contents I'm going to go over today.  \\nToday, I'm also going to delve into a fair amount  – some amount of linear algebra, and so\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 0}), Document(page_content=\"Today, I'm also going to delve into a fair amount  – some amount of linear algebra, and so \\nif you would like to see a refres her on linear algebra, this w eek's discussion section will \\nbe taught by the TAs and will be a refresher on linear algebra. So if some of the linear \\nalgebra I talk about today sort of seems to be going by pretty quickl y, or if you just want \\nto see some of the things I'm claiming today with our proof, if you wa nt to just see some \\nof those things written out in  detail, you can come to this week's discussion section.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 0}), Document(page_content='So I just want to start by showing you a f un video. Remember at the last lecture, the \\ninitial lecture, I talk ed about supervised learning. And supervised learning was this \\nmachine-learning problem where I said we\\'r e going to tell the algorithm what the close \\nright answer is for a number of examples, a nd then we want the algorithm to replicate \\nmore of the same.  \\nSo the example I had at the first lecture was the problem of predicting housing prices, \\nwhere you may have a training set, and we tell the algorithm what the \"right\" housing \\nprice was for every house in the training set. And then you want the algorithm to learn the \\nrelationship between sizes of houses and the pr ices, and essentially produce more of the \\n\"right\" answer.  \\nSo let me show you a video now. Load the bi g screen, please. So I\\'ll show you a video \\nnow that was from Dean Pomerleau at some work he did at Carnegie Mellon on applied \\nsupervised learning to get a car to drive itself . This is work on a vehicle known as Alvin. \\nIt was done sort of about 15 years ago, and I think it was a very el egant example of the \\nsorts of things you can get supervised or any algorithms to do.  \\nOn the video, you hear Dean Pomerleau\\'s voice mention and algorithm called Neural \\nNetwork. I\\'ll say a little bit about that later, but the essential learni ng algorithm for this is \\nsomething called gradient descent, which I will  talk about later in today\\'s lecture. Let\\'s \\nwatch the video. [Video plays]', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 1}), Document(page_content='something called gradient descent, which I will  talk about later in today\\'s lecture. Let\\'s \\nwatch the video. [Video plays]  \\nInstructor (Andrew Ng) :So two comments, right. One is this is supervised learning \\nbecause it\\'s learning from a human driver, in  which a human driver shows that we\\'re on \\nthis segment of the road, I will steer at this angle. This segment of the road, I\\'ll steer at \\nthis angle. And so the human provides the num ber of \"correct\" steeri ng directions to the \\ncar, and then it\\'s the job of the car to try to learn to produce more of these \"correct\" \\nsteering directions that k eeps the car on the road.  \\nOn the monitor display up here, I just want to tell you a littl e bit about what this display \\nmeans. So on the upper left where the m ouse pointer is moving, this horizontal line \\nactually shows the human steering direction, and this white bar, or this white area right \\nhere shows the steering direction chosen by the human driver, by moving the steering \\nwheel.  \\nThe human is steering a little bit to the left here indicated  by the position of this white \\nregion. This second line here wh ere Mamos is pointing, the sec ond line here is the output \\nof the learning algorithm, and where the learni ng algorithm currently thinks is the right \\nsteering direction. And right now what you\\'re seeing is the learning algorithm just at the \\nvery beginning of training, and so there\\'s just  no idea of where to steer. And so its output,', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 1}), Document(page_content=\"very beginning of training, and so there's just  no idea of where to steer. And so its output, \\nthis little white smear over the en tire range of stee ring directions.  \\nAnd as the algorithm collects more examples a nd learns of a time, you see it start to more \\nconfidently choose a steering direction. So let' s keep watching the video. [Video plays]\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 1}), Document(page_content=\"Instructor (Andrew Ng) :All right, so who thought driving could be that dramatic, right? \\nSwitch back to the chalkboard, please. I s hould say, this work was done about 15 years \\nago and autonomous driving has come a long way. So many of you will have heard of the \\nDARPA Grand Challenge, where one of my colleagues, Sebastian Thrun, the winning \\nteam's drive a car across a desert by itself.  \\nSo Alvin was, I think, absolutely amazing wo rk for its time, but autonomous driving has \\nobviously come a long way since then. So what  you just saw was an example, again, of \\nsupervised learning, and in particular it was an  example of what they  call the regression \\nproblem, because the vehicle is trying to predict a continuous value variables of a \\ncontinuous value steering directions , we call the regression problem.  \\nAnd what I want to do today is talk about our first supervised learning algorithm, and it \\nwill also be to a regression task. So for the running example that I'm going to use \\nthroughout today's lecture, you're going to retu rn to the example of  trying to predict \\nhousing prices. So here's actually a data set collected by TA, Dan Ramage, on housing \\nprices in Portland, Oregon.  \\nSo here's a dataset of a number of houses of different sizes, and here are their asking \\nprices in thousands of dollars, $200,000. And so we  can take this data and plot it, square \\nfeet, best price, and so you make your other dataset like that. And the question is, given a\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 2}), Document(page_content='feet, best price, and so you make your other dataset like that. And the question is, given a \\ndataset like this, or given what we call a tr aining set like this, how  do you learn to predict \\nthe relationship between th e size of the house and th e price of the house?  \\nSo I\\'m actually going to come back and modify th is task a little bit mo re later, but let me \\ngo ahead and introduce some notation, which I\\'ll be using, actually, th roughout the rest of \\nthis course. The first piece of notation is I\\'m going to let the lower case alphabet M \\ndenote the number of training examples, and that just means the number of rows, or the \\nnumber of examples, houses, and prices we have.  \\nAnd in this particular data set, we have, what actually happens, we have 47 training \\nexamples, although I wrote down only five. Oka y, so throughout this quarter, I\\'m going \\nto use the alphabet M to denot e the number of training examples. I\\'m going to use the \\nlower case alphabet X to denote the input variable s, which I\\'ll often also call the features. \\nAnd so in this case, X would denote the size of the house they were looking at.  \\nI\\'m going to use Y to denote the \"output\" va riable, which is sometimes also called a \\ntarget variable, and so one pair, x, y, is wh at comprises one training example. In other \\nwords, one row on the table I drew just now  what would be what I call one training \\nexample, and the Ith training example, in othe r words the Ith row in that table, I\\'m going', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 2}), Document(page_content=\"example, and the Ith training example, in othe r words the Ith row in that table, I'm going \\nto write as XI, Y, I.  \\nOkay, and so in this notation they're going to  use this superscript I is not exponentiation. \\nSo this is not X to the power of IY to the pow er of I. In this notation, the superscript I in \\nparentheses is just sort of an index into the Ith row of my list of training examples.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 2}), Document(page_content=\"So in supervised learning, this is what we 're going to do. We're given a training set, and \\nwe're going to feed our training set, compri sing our M training example, so 47 training \\nexamples, into a learning algorithm. Okay, and our algorithm then has output function \\nthat is by tradition, and for hist orical reasons, is usually de noted lower case alphabet H, \\nand is called a hypothesis. Don't worry too mu ch about whether the term hypothesis has a \\ndeep meaning. It's more a term that's used for historical reasons.  \\nAnd the hypothesis's job is to take this i nput. There's some new [inaudible]. What the \\nhypothesis does is it takes this  input, a new living area in s quare feet saying and output \\nestimates the price of this house. So the hypothesis H maps from inputs X to outputs Y. \\nSo in order to design a learning algorithm, the first thing we have to decide is how we \\nwant to represent the hypothesis, right.  \\nAnd just for this purposes of th is lecture, for the purposes of  our first learning algorithm, \\nI'm going to use a linear representation for the hypothesis. So I'm going to represent my \\nhypothesis as H of X equals theta zero, plus th eta 1X, where X here is  an input feature, \\nand so that's the size of the house we're considering.  \\nAnd more generally, come back to this, mo re generally for many regression problems we \\nmay have more than one input feature. So for example, if instead of  just knowing the size\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 3}), Document(page_content=\"may have more than one input feature. So for example, if instead of  just knowing the size \\nof the houses, if we know also the number of  bedrooms in these houses, let's say, then, so \\nif our training set also has a second feature,  the number of bedrooms in the house, then \\nyou may, let's say X1 denote the size and squa re feet. Let X have script two denote the \\nnumber of bedrooms, and then I would write the hypothesis, H of X,  as theta rho plus \\ntheta 1X1 plus theta 2X2.  \\nOkay, and sometimes when I went to take the hypothesis H, and when I went to make \\nthis dependent on the theta is explicit, I'll sometimes write this as H subscript theta of X. And so this is the price that my hypothesis predicts a house with features X costs. So \\ngiven the new house with features X, a certa in size and a certain number of bedrooms, \\nthis is going to be the price that my hypothe sis predicts this house is going to cost.  \\nOne final piece of notation, so for conciseness, just to write this a bit more compactly I'm \\ngoing to take the convention of defining X0 to be equal to one, and so I can now write H \\nof X to be equal to sum from I equals one to two of theta I, oh sorr y, zero to two, theta I, \\nX I. And if you think of theta as an X, as vect ors, then this is just theta transpose X.  \\nAnd the very final piece of notation is I'm al so going to let lower case N be the number of \\nfeatures in my learning problem . And so this actually becomes a sum from I equals zero\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 3}), Document(page_content=\"features in my learning problem . And so this actually becomes a sum from I equals zero \\nto N, where in this example if you have two features, N would be equal to two.  \\nAll right, I realize that was a fair amount of  notation, and as I proceed through the rest of \\nthe lecture today, or in future weeks as well,  if some day you're looking at me write a \\nsymbol and you're wondering, gee, what was th at simple lower case N again? Or what \\nwas that lower case X again, or whatever, pleas e raise hand and I'll answer. This is a fair\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 3}), Document(page_content=\"amount of notation. We'll probably all get used  to it in a few days and we'll standardize \\nnotation and make a lot of our descripti ons of learning algorithms a lot easier.  \\nBut again, if you see me write some symbol and you don't quite remember what it means, \\nchances are there are others in  this class who've forgotten too. So please raise your hand \\nand ask if you're ever wondering what some  symbol means. Any questions you have \\nabout any of this?  \\nYeah?  \\nStudent: The variable can be anything? [Inaudible]?  \\nInstructor (Andrew Ng) :Say that again.  \\nStudent: [inaudible] zero theta one?  \\nInstructor (Andrew Ng) :Right, so, well let me – this was going to be next, but the theta \\nor the theta Is are called the parameters. Th e thetas are called the parameters of our \\nlearning algorithm and theta zero, theta one, th eta two are just real numbers. And then it \\nis the job of the learning algor ithm to use the training set to choose or to learn appropriate \\nparameters theta.  \\nOkay, is there other questions?  Student: What does [inaudible]?  \\nInstructor (Andrew Ng) :Oh, transpose. Oh yeah, sorr y. When [inaudible] theta and \\ntheta transpose X, theta [inaudible].  \\nStudent: Is this like a [inaudible] hypothesis [inaudible], or would you have higher \\norders? Or would th eta [inaudible]?  \\nInstructor (Andrew Ng) :All great questions. The answer – so the question was, is this a \\ntypical hypothesis or can theta be a function of other variables and so on. And the answer\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 4}), Document(page_content=\"typical hypothesis or can theta be a function of other variables and so on. And the answer \\nis sort of yes. For now, just for this first learning algorithm we'll talk about using a linear \\nhypothesis class. A little bit ac tually later this quarter, we'll talk about much more \\ncomplicated hypothesis classes, and we'll actua lly talk about higher order functions as \\nwell, a little bit later today.  \\nOkay, so for the learning problem then. How do we chose the parameters theta so that our \\nhypothesis H will make accurate predictions ab out all the houses. All right, so one \\nreasonable thing to do seems to be, well, we have a training set. So – and just on the \\ntraining set, our hypothesis will make some prediction, predictions of the housing prices, \\nof the prices of the houses  in the training set.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 4}), Document(page_content=\"So one thing we could do is just try to ma ke the predictions of  a learning algorithm \\naccurate on a training set. So given some feat ures, X, and some correct prices, Y, we \\nmight want to make that thet a square difference between th e prediction of the algorithm \\nand the actual price [inaudible].  \\nSo to choose parameters theta, unless we wa nt to minimize over the parameters theta, so \\nthe squared area between the pred icted price and the actual pri ce. And so going to fill this \\nin. We have M training examples. So the sum from I equals one through M of my M \\ntraining examples, of price predicted on the It h house in my training set. Mine is the \\nactual target variable. Mi ne is actual price on the Ith training example.  \\nAnd by convention, instead of minimizing this sum of the squared differences, I'm just \\ngoing to put a one-half there, which will simplif y some of the math we do later. Okay, \\nand so let me go ahead and define J of theta to be equal to just the same, one-half sum \\nfrom I equals one through M on the number of  training examples, of the value predicted \\nby my hypothesis minus the actual value.  \\nAnd so what we'll do, let's say, is minimize as a function of the parame ters of theta, this \\nquantity J of theta. I should say, to those of you who have taken sort of linear algebra \\nclasses, or maybe basic statistics classes, some of you may have s een things like these \\nbefore and seen least [inaudible] re gression or [inaudible] squares.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 5}), Document(page_content=\"before and seen least [inaudible] re gression or [inaudible] squares.  \\nMany of you will not have seen this before. I think some of you may have seen it before, \\nbut either way, regardless of whether you've s een it before, let's keep going. Just for those \\nof you that have seen it before, I should say eventually, we'll actually show that this \\nalgorithm is a special case of a much broade r class of algorithms. But let's keep going. \\nWe'll get there eventually.  \\nSo I'm going to talk about a couple of different algorithms for performing that \\nminimization over theta of J of theta. The first algorithm I'm going to talk about is a \\nsearch algorithm, where the basic idea is we'll start with some value of my parameter vector theta. Maybe initialize my parameter vect or theta to be the vect or of all zeros, and \\nexcuse me, have to correct th at. I sort of write zero with  an arrow on top to denote the \\nvector of all zeros.  \\nAnd then I'm going to keep changing my para meter vector theta to reduce J of theta a \\nlittle bit, until we hopefully end up at the minimu m with respect to thet a of J of theta. So \\nswitch the laptops please, and lower the big screen. So let me go ahead and show you an \\nanimation of this first algorithm for minimizi ng J of theta, which is an algorithm called \\ngrading and descent.  \\nSo here's the idea. You see on the display a plot and the axes, th e horizontal axes are\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 5}), Document(page_content=\"grading and descent.  \\nSo here's the idea. You see on the display a plot and the axes, th e horizontal axes are \\ntheta zero and theta one. That's usually – mini mize J of theta, which is represented by the \\nheight of this plot. So the surface represents the function J of theta and the axes of this \\nfunction, or the inputs of this function are the parameters theta zero and theta one, written \\ndown here below.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 5}), Document(page_content='So here\\'s the gradient descent algorithm. I\\'m going to choose some initial point. It could \\nbe vector of all zeros or some randomly chosen  point. Let\\'s say we start from that point \\ndenoted by the star, by the cross, and now I wa nt you to imagine that this display actually \\nshows a 3D landscape. Imagine you\\'re all in a hi lly park or something, and this is the 3D \\nshape of, like, a hill in some park.  \\nSo imagine you\\'re actually standing physically at the position of that star, of that cross, \\nand imagine you can stand on that hill, ri ght, and look all 360 degrees around you and \\nask, if I were to take a small step, what would allow me to go downhill the most? Okay, just imagine that this is physically a hill and you\\'re standing there, and would look around \\nask, \"If I take a small step, what is the direc tion of steepest descent, that would take me \\ndownhill as quickly as possible?\"  \\nSo the gradient descent algorith m does exactly that. I\\'m going to take a small step in this \\ndirection of steepest des cent, or the direction that the gr adient turns out to be. And then \\nyou take a small step and you end up at a ne w point shown there, and it would keep \\ngoing. You\\'re now at a new point on this hi ll, and again you\\'re going to look around you, \\nlook all 360 degrees around you, and ask, \"What is the direction that would take me \\ndownhill as quickly as possible?\"  \\nAnd we want to go downhill as quickly as possible, because we want to find the', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 6}), Document(page_content='downhill as quickly as possible?\"  \\nAnd we want to go downhill as quickly as possible, because we want to find the \\nminimum of J of theta. So you do that agai n. You can take another step, okay, and you \\nsort of keep going until you end up at a loca l minimum of this function, J of theta. One \\nproperty of gradient descent is  that where you end up – in this  case, we ended up at this \\npoint on the lower left hand corner of this plot.  \\nBut let\\'s try running gradient descent again fr om a different position. So that was where I \\nstarted gradient descent just now. Let\\'s rer un gradient descent, but using a slightly \\ndifferent initial starting point, so a point sli ghtly further up and furthe r to the right. So it \\nturns out if you run gradient de scent from that point, then if  you take a steepest descent \\ndirection again, that\\'s your first step.  \\nAnd if you keep going, it turns out that with a slightly different ini tial starting point, you \\ncan actually end up at a completely different lo cal optimum. Okay, so this is a property of \\ngradient descent, and we\\'ll come back to it in  a second. So be aware that gradient descent \\ncan sometimes depend on where you initialize yo ur parameters, theta zero and theta one.  \\nSwitch back to the chalkboard, please. Let\\' s go ahead and work out the math of the \\ngradient descent algorithm. Then we\\'ll come b ack and revisit this i ssue of local optimum. \\nSo here\\'s the gradient descent algorithm.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 6}), Document(page_content=\"gradient descent algorithm. Then we'll come b ack and revisit this i ssue of local optimum. \\nSo here's the gradient descent algorithm.  \\nWe're going to take a repeatedly take a step  in the direction of steepest descent, and it \\nturns out that you can write that as [inaudi ble], which is we're going to update the \\nparameters theta as theta I minus the partial de rivative with respect to theta I, J of Theta. \\nOkay, so this is how we're going to update the I parameter, theta I, how we're going to \\nupdate Theta I on each iterati on of gradient descent.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 6}), Document(page_content=\"Just a point of notation, I use this colon equa ls notation to denote setting a variable on the \\nleft hand side equal to the variable on the ri ght hand side. All right, so if I write A colon \\nequals B, then what I'm saying is, this is part of a computer program, or this is part of an \\nalgorithm where we take the value of B, the value on the right hand side, and use that to \\noverwrite the value on the left hand side.  \\nIn contrast, if I write A equals B, then this is an assertion of truth. I'm claiming that the \\nvalue of A is equal to the value of B, wh ereas this is computer operation where we \\noverwrite the value of A. If I write A equals B then I'm asserting that the values of A and \\nB are equal.  \\nSo let's see, this algorithm sort of makes se nse – well, actually let's just move on. Let's \\njust go ahead and take this algorithm and apply it to our problem. And to work out \\ngradient descent, let's take gradient descent and just apply it to our problem, and this \\nbeing the first somewhat mathematical lect ure, I'm going to step through derivations \\nmuch more slowly and carefully than I will later in this quarter. We'll work through the \\nsteps of these in much more detail than I will later in this quarter.  \\nLet's actually work out what this gradient descen t rule is. So – and I'll do this just for the \\ncase of, if we had only one training example. Ok ay, so in this case we need to work out\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 7}), Document(page_content=\"case of, if we had only one training example. Ok ay, so in this case we need to work out \\nwhat the partial derivative with respect to the parameter theta I of J of theta. If we have \\nonly one training example then J of theta is goi ng to be one-half of script theta, of X \\nminus Y, script. So if you have only one training example comprising one pair, X, Y, \\nthen this is what J of theta is going to be.  \\nAnd so taking derivatives, you have one-h alf something squared. So the two comes \\ndown. So you have two times one-half times  theta of X minus Y, and then by the \\n[inaudible] derivatives, we also  must apply this by the deri vative of what's inside the \\nsquare. Right, the two and the one-half cancel. So  this leaves [inaudible] times that, theta \\nzero, X zero plus [inaudible].  \\nOkay, and if you look inside this sum, we're ta king the partial derivative of this sum with \\nrespect to the parameter theta I. But all th e terms in the sum, except for one, do not \\ndepend on theta I. In this sum, the only term  that depends on theta I will be some term \\nhere of theta I, X I. And so we take the partia l derivative with respect to theta I, X I – take \\nthe partial derivative with respec t to theta I of this term thet a I, X I, and so you get that \\ntimes X I.  \\nOkay, and so this gives us our learning rule, ri ght, of theta I gets updated as theta I minus \\nalpha times that. Okay, and this Greek alphabet alpha here is a parameter of the algorithm\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 7}), Document(page_content=\"alpha times that. Okay, and this Greek alphabet alpha here is a parameter of the algorithm \\ncalled the learning rate, and this parameter al pha controls how large a step you take. So \\nyou're standing on the hill. You decided what di rection to take a st ep in, and so this \\nparameter alpha controls how a ggressive – how large a step y ou take in this direction of \\nsteepest descent.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 7}), Document(page_content=\"And so if you – and this is a parameter of the algorithm that's often set by hand. If you \\nchoose alpha to be too small than your steep est descent algorithm will take very tiny \\nsteps and take a long time to converge. If alpha  is too large then the steepest descent may \\nactually end up overshooting the minimum, if  you're taking too a ggressive a step.  \\nYeah?  \\nStudent: [Inaudible].  \\nInstructor (Andrew Ng) :Say that again?  \\nStudent: Isn't there a one over two missing somewhere?  \\nInstructor (Andrew Ng) :Is there a one-half missing?  \\nStudent: I was [inaudible].  \\nInstructor (Andrew Ng) :Thanks. I do make lots of errors  in that. Any questions about \\nthis?  \\nAll right, so let me just wrap this property into an algorithm. So over there I derived the \\nalgorithm where you have just one training example, more generally for M training \\nexamples, gradient descent becomes the following. We're going to repeat until \\nconvergence the following step.  \\nOkay, theta I gets updated as theta I and I'm just writing out the appropriate equation for \\nM examples rather than one example. Theta I gets updated. Theta I minus alpha times the sum from I equals one to M. Okay, and I won't bother to show it, but you can go home \\nand sort of verify for yourself that this summation here, this is indeed the partial \\nderivative with respect to theta I of J of theta, where if you us e the original definition of J \\nof theta for when you have M training examples.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 8}), Document(page_content=\"derivative with respect to theta I of J of theta, where if you us e the original definition of J \\nof theta for when you have M training examples.  \\nOkay, so I'm just going to show – switch back to the laptop display. I'm going to show \\nyou what this looks like when you run the algor ithm. So it turns out that for the specific \\nproblem of linear regression, or ordinary release squares, which is what we're doing \\ntoday, the function J of theta actually does not lo ok like this nasty one that I'll show you \\njust now with a multiple local optima.  \\nIn particular, it turns out for ordi nary release squares, the functi on J of theta is – it's just a \\nquadratic function. And so we'll always ha ve a nice bow shape, like what you see up \\nhere, and only have one global mini mum with no other local optima.  \\nSo when you run gradient descent, here are actu ally the contours of the function J. So the \\ncontours of a bow shaped function like that  are going to be ellipses, and if you run \\ngradient descent on this algorithm, here's what  you might get. Let's see, so I initialize the\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 8}), Document(page_content=\"parameters. So let's say randomly at the positi on of that cross over th ere, right, that cross \\non the upper right.  \\nAnd so after one iteration of gradient descen t, as you change the space of parameters, so \\nif that's the result of one step of gradient desc ent, two steps, three steps, four steps, five \\nsteps, and so on, and it, you know, converges easily, rapidly to the global minimum of \\nthis function J of theta.  \\nOkay, and this is a property of [inaudible] regression with a linear  hypothesis cost. The \\nfunction, J of theta has no local optima. Yes, question?  \\nStudent: Is the alpha changing every time? Becau se the step is not [inaudible].  \\nInstructor (Andrew Ng) :So it turns out that – yes, so it turns out – this was done with a \\n– this is with a fake value of alpha, and one of  the properties of gradie nt descent is that as \\nyou approach the local minimum, it actually ta kes smaller and smaller steps so they'll \\nconverge. And the reason is, the update is – you update theta by subtracting from alpha \\ntimes the gradient. And so as you approach th e local minimum, the gradient also goes to \\nzero.  \\nAs you approach the local minimum, at the lo cal minimum the gradient is zero, and as \\nyou approach the local minimum, the gradient  also gets smaller and smaller. And so \\ngradient descent will automatically take sm aller and smaller steps as you approach the \\nlocal minimum. Make sense?\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 9}), Document(page_content=\"gradient descent will automatically take sm aller and smaller steps as you approach the \\nlocal minimum. Make sense?  \\nAnd here's the same plot – here's actually a pl ot of the housing prices data. So here, lets \\nyou initialize the parameters to the vector of all zeros, and so  this blue line  at the bottom \\nshows the hypothesis with the parameters of in itialization. So initiall y theta zero and theta \\none are both zero, and so your hypothesis predic ts that all prices are equal to zero.  \\nAfter one iteration of gradient descent, that's  the blue line you get. After two iterations, \\nthree, four, five, and after a few more itera tions, excuse me, it converges, and you've now \\nfound the least square fit for the data. Okay, le t's switch back to the chalkboard. Are there \\nquestions about this? Yeah?  \\nStudent: [Inaudible] iteration, do we mean that we  run each sample – all the sample cases \\n[inaudible] the new values?  \\nInstructor (Andrew Ng) :Yes, right.  \\nStudent: And converged means that the value will  be the same [inaudible] roughly the \\nsame?  \\nInstructor (Andrew Ng) :Yeah, so this is sort of a question of how do you test the \\nconvergence. And there's different ways of testing for convergence. One is you can look\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 9}), Document(page_content=\"at two different iterations and see if theta ha s changed a lot, and if  theta hasn't changed \\nmuch within two iterations, you may say it's sort of more or less converged.  \\nSomething that's done maybe slightly more often is look at the value of J of theta, and if J \\nof theta – if the quantity you're  trying to minimize is not changing much anymore, then \\nyou might be inclined to believe it's converged. So  these are sort of standard heuristics, or \\nstandard rules of thumb that are often used to  decide if gradient descent has converged.  \\nYeah?  \\nStudent: I may have missed something, but especial ly in [inaudible] descent. So one \\nfeature [inaudible] curve and can either go this  way or that way. But the math at incline \\n[inaudible] where that co mes in. When do you choose whether you go left, whether \\nyou're going this way or that way?  \\nInstructor (Andrew Ng) :I see. It just turns out that – so the question is, how is gradient \\ndescent looking 360 around you and choosing the direction of steepest descent. So it \\nactually turns out – I'm not su re I'll answer the second part , but it turns out that if you \\nstand on the hill and if you – it turns out th at when you compute the gradient of the \\nfunction, when you compute the derivative of the function, then it just turns out that that \\nis indeed the direction of steepest descent.  \\nBy the way, I just want to point out, you woul d never want to go in the opposite direction\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 10}), Document(page_content=\"is indeed the direction of steepest descent.  \\nBy the way, I just want to point out, you woul d never want to go in the opposite direction \\nbecause the opposite direction woul d actually be the direction of  steepest ascent, right. So \\nas it turns out – maybe the TAs can talk a bit more about this at th e section if there's \\ninterest. It turns out, when you take the de rivative of a function, the derivative of a \\nfunction sort of turns out to just give  you the direction of steepest descent.  \\nAnd so you don't explicitly look all 360 degr ees around you. You sort of just compute the \\nderivative and that turns out to  be the direction of steepest descent. Yeah, maybe the TAs \\ncan talk a bit more about this on Friday.  \\nOkay, let's see, so let me go ahead and give this algorithm a specific name. So this \\nalgorithm here is actually called batch gradient  descent, and the term batch isn't a great \\nterm, but the term batch refers to the fact that on every step of gradient descent you're \\ngoing to look at your entire training set. You're going to perform a sum over your M \\ntraining examples.  \\nSo [inaudible] descent often works very well.  I use it very often, and it turns out that \\nsometimes if you have a really, really large tr aining set, imagine that instead of having 47 \\nhouses from Portland, Oregon in our training set, if you had, say, the U.S. Census Database or something, with U.S. census size databases you often have hundreds of\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 10}), Document(page_content=\"thousands or millions of training examples.  \\nSo if M is a few million then if you're running batch rate and descent,  this means that to \\nperform every step of gradient descent you need  to perform a sum from J equals one to a\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 10}), Document(page_content=\"million. That's sort of a lot of training ex amples where your computer programs have to \\nlook at, before you can even take one st ep downhill on the func tion J of theta.  \\nSo it turns out that when you have very la rge training sets, you should write down an \\nalternative algorithm that is called [inaudible]  gradient descent. Sometimes I'll also call it \\nincremental gradient descent, but the algorith m is as follows. Again, it will repeat until \\nconvergence and will iterate for J equals one to  M, and will perform one of these sort of \\ngradient descent updates using just the J training example.  \\nOh, and as usual, this is really – you update  all the parameters data runs. You perform \\nthis update for all values of I. For I indexe s and the parameter vectors, you just perform \\nthis update, all of your parameters simultaneous ly. And the advantage of this algorithm is \\nthat in order to start learning, in order to start modifying the parameters, you only need to \\nlook at your first training examples.  \\nYou should look at your first training exampl e and perform an update using the derivative \\nof the error with resp ect to just your first training example, and then you look at your \\nsecond training example and perform another u pdate. And you sort of keep adapting your \\nparameters much more quickly without need ing to scan over your entire U.S. Census \\ndatabase before you can even start adapting parameters.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 11}), Document(page_content=\"parameters much more quickly without need ing to scan over your entire U.S. Census \\ndatabase before you can even start adapting parameters.  \\nSo let's see, for launch data sets, so constant ly gradient descent is often much faster, and \\nwhat happens is that constant gradient des cent is that it won't actually converge to the \\nglobal minimum exactly, but if these are the contours of your function, then after you run \\nthe constant gradient descent, you sort of tend to wander around.  \\nAnd you may actually end up going uphill occasionally, but your parameters will sort of \\ntender to wander to the region closest to the global minimum, but sort of keep wandering \\naround a little bit near the region of the global [inaudible]. And often th at's just fine to \\nhave a parameter that wanders around a littl e bit the global minimum. And in practice, \\nthis often works much faster than back gradie nt descent, especially  if you have a large \\ntraining set.  \\nOkay, I'm going to clean a couple of boards.  While I do that, why don't you take a look at \\nthe equations, and after I'm done cleaning the boards, I'll ask what questions you have.  \\nOkay, so what questions do you have about all of this?  \\nStudent: [Inaudible] is it true – are you just so rt of rearranging th e order that you do the \\ncomputation? So do you just use the first trai ning example and update all of the theta Is \\nand then step, and then update with the sec ond training example, a nd update all the theta\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 11}), Document(page_content=\"and then step, and then update with the sec ond training example, a nd update all the theta \\nIs, and then step? And is that why you get sort of this really – ?  \\nInstructor (Andrew Ng) :Let's see, right. So I'm going to look at my first training \\nexample and then I'm going to take a ste p, and then I'm going to perform the second\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 11}), Document(page_content=\"gradient descent updates using my new paramete r vector that has already been modified \\nusing my first training exampl e. And then I keep going.  \\nMake sense? Yeah?  \\nStudent: So in each update of all the theta Is, you're only using –  \\nInstructor (Andrew Ng) :One training example.  \\nStudent: One training example.  \\nStudent: [Inaudible]?  \\nInstructor (Andrew Ng) :Let's see, it's definitely a [ina udible]. I believe this theory that \\nsort of supports that as we ll. Yeah, the theory that suppor ts that, the [inaudible] of \\ntheorem is, I don't remember.  \\nOkay, cool. So in what I've done so far, I've talked about an iterative algorithm for \\nperforming this minimization in terms of J of theta. And it tu rns out that there's another \\nway for this specific problem of least square s regression, of ordinary  least squares. It \\nturns out there's another way to perform this mi nimization of J of thet a that allows you to \\nsolve for the parameters theta in close fo rm, without needing to run an iterative \\nalgorithm.  \\nAnd I know some of you may have seen some of what I'm about to do before, in like an \\nundergraduate linear algebra co urse, and the way it's typica lly done requires [inaudible] \\nprojections, or taking lots of de rivatives and writing lots of algebra. What I'd like to do is \\nshow you a way to derive the closed form solutio n of theta in just a few lines of algebra.  \\nBut to do that, I'll need to in troduce a new notation for matrix  derivatives, and it turns out\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 12}), Document(page_content=\"But to do that, I'll need to in troduce a new notation for matrix  derivatives, and it turns out \\nthat, sort of, the nota tion I'm about to define here ju st in my own personal work has \\nturned out to be one of the mo st useful things that I actually  use all the time, to have a \\nnotation of how to take derivatives with resp ect to matrixes, so that you can solve for the \\nminimum of J of theta with, like, a few lines of algebra rather than  writing out pages and \\npages of matrices and derivatives.  \\nSo then we're going to define this new notati on first and then we'll go ahead and work out \\nthe minimization. Given a function J, since J is a function of a vector of parameters theta, \\nright, I'm going to define the deri vative of the gradient of J with respect to theta, as self of \\nvector. Okay, and so this is going to be an N plus one dimensional vector. Theta is an n \\nplus one dimensional vector with indices ranging from zero to N. And so I'm going to \\ndefine this derivative to be equal to that.  \\nOkay, and so we can actually rewrite the grad ient descent algorithm as follows. This is \\nbatch gradient descent, and we  write gradient descent as updating the para meter vector\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 12}), Document(page_content=\"theta – notice there's no subscript I now – updating the parameter vector theta as the \\nprevious parameter minus alpha times the gradient.  \\nOkay, and so in this equation all of these quanti ties, theta, and this gradient vector, all of \\nthese are n plus one dimensional vectors. I was using the boards out of order, wasn't I? So \\nmore generally, if you have a function F that maps from the space of matrices A, that \\nmaps from, say, the space of N by N matrices  to the space of real numbers. So if you \\nhave a function, F of A, wher e A is an N by N matrix.  \\nSo this function is matched from matrices to real numbers, the function that takes this \\ninput to matrix. Let me define the derivative w ith respect to F of the matrix A. Now, I'm \\njust taking the gradient of F with respect to  its input, which is the matrix. I'm going to \\ndefine this itself to be a matrix.  \\nOkay, so the derivative of F with respect to A is itself a matrix, and the matrix contains \\nall the partial derivatives of F with respect to the elements of  A. One more definition is if \\nA is a square matrix, so if A is an n by n matrix, number of rows  equals number of \\ncolumns, let me define the trace of A to be equal to the sum of A' s diagonal elements. So \\nthis is just sum over I of A, I, I.  \\nFor those of you that haven't seen this sort  of operator notation be fore, you can think of \\ntrace of A as the trace operator applied to the square matrix A, but it's more commonly\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 13}), Document(page_content=\"trace of A as the trace operator applied to the square matrix A, but it's more commonly \\nwritten without the parentheses. So I usually wr ite trace of A like this, and this just means \\nthe sum of diagonal elements.  \\nSo here are some facts about the trace operator and about derivatives, and I'm just going \\nto write these without proof. You can also have the TAs prove some of them in the \\ndiscussion section, or you can actually go home and verify the proofs of all of these.  \\nIt turns out that given two matr ices, A and B, the trace of the matrix A times B is equal to \\nthe trace of B, A. Okay, I'm not going to pr ove this, but you should be able to go home \\nand prove this yourself without too much di fficulty. And similarly, the trace of a product \\nof three matrices, so if you can take the matrix  at the end and cycli cally permeate it to the \\nfront.  \\nSo trace of A times B, times C, is equal to the trace of C, A, B. So take the matrix C at \\nthe back and move it to the front, and this is also equal to the trace of B, C. Take the \\nmatrix B and move it to the front.  \\nOkay, also, suppose you have a function F of A wh ich is defined as a trace of A, B. Okay, \\nso this is, right, the trace is a real number. So  the trace of A, B is a function that takes this \\ninput of matrix A and output a real number. So then the de rivative with respect to the \\nmatrix A of this function of trace A, B, is  going to be B transposed. And this is just\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 13}), Document(page_content=\"matrix A of this function of trace A, B, is  going to be B transposed. And this is just \\nanother fact that you can prove  by yourself by going back and referring to the definitions \\nof traces and matrix derivatives. I'm not goi ng to prove it. You should work it out.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 13}), Document(page_content=\"Okay, and lastly a couple of easy ones. The trace of A is equal to the trace of A \\ntransposed because the trace is just the sum of diagonal elem ents. And so if you transpose \\nthe matrix, the diagonal, then there's no cha nge. And if lower case A is a real number, \\nthen the trace of a real number is just itsel f. So think of a real number as a one by one \\nmatrix. So the trace of a one by one matrix is just whatever that real number is.  \\nAnd lastly, this is a somewhat tricky one. Th e derivative with respect to the matrix A of \\nthe trace of A, B, A, transpose C is equal to C, A, B plus C transposed A, B transposed. \\nAnd I won't prove that either. This is sort  of just algebra. Work it out yourself.  \\nOkay, and so the key facts I'm going to use again about traces and matrix derivatives, I'll \\nuse five. Ten minutes. Okay, so armed with th ese things I'm going to figure out – let's try \\nto come up with a quick deriva tion for how to minimize J of theta as a function of theta in \\nclosed form, and without needing to use an iterative algorithm.  \\nSo work this out. Let me define the matrix X. This is called the design matrix. To be a \\nmatrix containing all the inputs from my traini ng set. So X 1 was the vector of inputs to \\nthe vector of features for my first training ex ample. So I'm going to set X 1 to be the first \\nrow of this matrix X, set my second training example is in place to be the second \\nvariable, and so on.\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 14}), Document(page_content=\"row of this matrix X, set my second training example is in place to be the second \\nvariable, and so on.  \\nAnd I have M training examples, and so that's going to be my design matrix X. Okay, this \\nis defined as matrix capital X as follows, and so now, let me take this matrix X and \\nmultiply it by my parameter vector theta. This derivation will just take two or three sets. \\nSo X times theta – remember how matrix vector multiplication goes. You take this vector \\nand you multiply it by each of the rows of the matrix.  \\nSo X times theta is just going to be X 1 tr ansposed theta, dot, dot, dot, down to X M, \\ntransposed theta. And this is, of course, ju st the predictions of your hypothesis on each of \\nyour M training examples. Then we also defined the Y vector to be the vector of all the \\ntarget values Y1 through YM in my training se t. Okay, so Y vector is an M dimensional \\nvector.  \\nSo X theta minus Y contained the math from th e previous board, this is going to be, right, \\nand now, X theta minus Y, this is a vector. Th is is an M dimensional vector in M training \\nexamples, and so I'm actually going to take th is vector and take th is inner product with \\nitself.  \\nOkay, so we call that if Z is a vector than Z transpose Z is just sum over I, ZI squared. \\nRight, that's how you take the inner product of a vector with a sum. So you want to take \\nthis vector, X theta minus Y, and take the inne r product of this vector with itself, and so\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 14}), Document(page_content=\"this vector, X theta minus Y, and take the inne r product of this vector with itself, and so \\nthat gives me sum from I equals one to M, H, F, X, I, minus Y squared. Okay, since it's \\njust the sum of the squares of the elements of this vector.  \\nAnd put a half there for the emphasis. This is our previous definition of J of theta. Okay, \\nyeah?\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 14}), Document(page_content=\"Student: [Inaudible]?  \\nInstructor (Andrew Ng) :Yeah, I threw a lot of notations  at you today. So M is the \\nnumber of training examples a nd the number of training exam ples runs from one through \\nM, and then is the feature vector that runs from zero through N. Does that make sense?  \\nSo this is the sum from one through M. It's so rt of theta transpose X that's equal to sum \\nfrom J equals zero through N of theta J, X, J. Does that make sense? It's the feature vectors that index from ze ro through N where X, zero is equal to one, whereas the \\ntraining example is actually indexed from one through M.  \\nSo let me clean a few more boards and take an other look at this, make sure it all makes \\nsense. Okay, yeah?  \\nStudent: [Inaudible] the Y inside the parenthe ses, shouldn't that be [inaudible]?  \\nInstructor (Andrew Ng) :Oh, yes, thank you. Oh is that what you meant? Yes, thank \\nyou. Great, I training example. Anything else? Cool. So we're actually nearly done with \\nthis derivation. We wo uld like to minimize J of theta w ith respect to theta and we've \\nwritten J of theta fairly compactly using this matrix vector notation.  \\nSo in order to minimize J of theta with respec t to theta, what we're going to do is take the \\nderivative with respect to theta of J of theta, and set this to zero, and solve for theta. \\nOkay, so we have derivative with respect to theta of that is equa l to – I should mention\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 15}), Document(page_content=\"Okay, so we have derivative with respect to theta of that is equa l to – I should mention \\nthere will be some steps here that I'm just  going to do fairly quickly without proof.  \\nSo is it really true that the derivative of half of that is half of the derivative, and I already \\nexchanged the derivative and the one-half. In terms of the answers, yes, but later on you \\nshould go home and look through the lecture notes and make sure you understand and \\nbelieve why every step is correct. I'm going to  do things relatively quickly here and you \\ncan work through every step yourself more sl owly by referring to the lecture notes.  \\nOkay, so that's equal to – I'm going to expa nd now this quadratic function. So this is \\ngoing to be, okay, and this is ju st sort of taking a quadratic  function and expanding it out \\nby multiplying the [inaudible]. And again, work  through the steps later yourself if you're \\nnot quite sure how I did that.  \\nSo this thing, this vector, vector product, ri ght, this quantity here, th is is just J of theta \\nand so it's just a real numb er, and the trace of a real  number is just itself.  \\nStudent: [Inaudible].  \\nInstructor (Andrew Ng) :Oh, thanks, Dan. Cool, great. So this quantity in parentheses, \\nthis is J of theta and it's just a real number . And so the trace of a real number is just the \\nsame real number. And so you can sort of take a trace operator without changing\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 15}), Document(page_content='same real number. And so you can sort of take a trace operator without changing \\nanything. And this is equal to one-half derivativ e with respect to theta of the trace of – by', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 15}), Document(page_content=\"the second permutation property of trace. You ca n take this theta at the end and move it \\nto the front.  \\nSo this is going to be trace of theta times theta transposed, X transpose X minus \\nderivative with respect to theta of the trace of – I'm going to take that and bring it to the – \\noh, sorry. Actually, this thing he re is also a real number and the transpose of a real \\nnumber is just itself. Right, so take the transpose of a real nu mber without changing \\nanything.  \\nSo let me go ahead and just take the transpose of this. A real number transposed itself is \\njust the same real number. So this is minus  the trace of, taking the transpose of that. \\nHere's Y transpose X theta, then minus [inaudible] theta. Okay, and this last quantity, Y \\ntranspose Y. It doesn't actually depend on theta.  So when I take the derivative of this last \\nterm with respect to theta, it's just zero. So just drop that term.  \\nAnd lastly, well, the derivative with respect to theta of the trace of theta, theta transposed, \\nX transpose X. I'm going to use one of the facts I wrote down earlier without proof, and \\nI'm going to let this be A. There's an identity matrix there, so this is A, B, A transpose C, and using a rule that I've written down previ ously that you'll find in lecture notes, because \\nit's still on one of the boards that you had prev iously, this is just equal to X transpose X \\ntheta.  \\nSo this is C, A, B, which is sort of just the identity matrix, whic h you can ignore, plus X\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 16}), Document(page_content=\"theta.  \\nSo this is C, A, B, which is sort of just the identity matrix, whic h you can ignore, plus X \\ntranspose X theta where this is now C trans pose C, again times the identity which we're \\ngoing to ignore, times B transposed. And the matrix X transpose X is the metric, so C \\ntranspose is equal to C.  \\nSimilarly, the derivative with respect to theta of the trace of Y transpose theta X, this is \\nthe derivative with respect to matrix A of the trace of B, A and this is just X transpose Y. \\nThis is just B transposed, ag ain, by one of the rules that I wrote down earlier. And so if \\nyou plug this back in, we find, therefore, that  the derivative – wow, this board's really \\nbad.  \\nSo if you plug this back into our formula for the derivative of J, you find that the \\nderivative with respect to theta of J of theta is e qual to one-half X transpose theta, plus X \\ntranspose X theta, minus X transpose Y, minus  X transpose Y, which is just X transpose \\nX theta minus X [inaudible].  \\nOkay, so we set this to zero and we get that, which is called a normal equation, and we \\ncan now solve this equation for theta in closed  form. That's X transpose X theta, inverse \\ntimes X transpose Y. And so this gives us a wa y for solving for the least square fit to the \\nparameters in closed form, without need ing to use an [inaudible] descent.  \\nOkay, and using this matrix vector notati on, I think, I don't know, I think we did this\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 16}), Document(page_content=\"Okay, and using this matrix vector notati on, I think, I don't know, I think we did this \\nwhole thing in about ten minut es, which we couldn't have if I was writing out reams of\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 16}), Document(page_content=\"algebra. Okay, some of you look a little bi t dazed, but this is our first learning hour. \\nAren't you excited? Any quick questions a bout this before we close for today?  \\nStudent: [Inaudible].  \\nInstructor (Andrew Ng) :Say that again.  \\nStudent: What you derived, wasn't that just [inaudible] of X?  \\nInstructor (Andrew Ng) :What inverse? \\nStudent: Pseudo inverse.  \\nInstructor (Andrew Ng) :Pseudo inverse?  \\nStudent: Pseudo inverse.  \\nInstructor (Andrew Ng) :Yeah, I turns out that in cases, if X transpose X is not \\ninvertible, than you use the pseudo inverse mi nimized to solve this. But it turns out X \\ntranspose X is not invertible. That usually m eans your features were dependent. It usually \\nmeans you did something like repeat the same feat ure twice in your training set. So if this \\nis not invertible, it turns out the minimu m is obtained by the pseudo inverses of the \\ninverse.  \\nIf you don't know what I just sa id, don't worry about it. It usually won't be a problem. \\nAnything else?  \\nStudent: On the second board [inaudible]?  \\nInstructor (Andrew Ng) :Let me take that off. We're running over. Let's close for today \\nand if they're other questions, I'll take them after.  \\n[End of Audio]  \\nDuration: 79 minutes\", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf', 'page': 17}), Document(page_content='MachineLearning-Lecture03  \\nInstructor (Andrew Ng) :Okay. Good morning and welcome b ack to the third lecture of \\nthis class. So here’s what I want to do t oday, and some of the topics I do today may seem \\na little bit like I’m jumping, sort  of, from topic to topic, but here’s, sort of, the outline for \\ntoday and the illogical flow of ideas. In the last lecture, we  talked about linear regression \\nand today I want to talk about sort of an  adaptation of that called locally weighted \\nregression. It’s very a popular  algorithm that’s actually one of my former mentors \\nprobably favorite machine learning algorithm.  \\nWe’ll then talk about a probabl e second interpretation of linear regression and use that to \\nmove onto our first classification algorithm, which is logistic regr ession; take a brief \\ndigression to tell you about something cal led the perceptron algorithm, which is \\nsomething we’ll come back to, again, later this  quarter; and time allowing I hope to get to \\nNewton’s method, which is an algorithm fo r fitting logistic regression models.  \\nSo this is recap where we’re talking about in the previous lecture, remember the notation \\nI defined was that I used this X superscrip t I, Y superscript I to denote the I training \\nexample. And when we’re talking about linear regression or linear l east squares, we use \\nthis to denote the predicted value of “by my hypothesis H” on the input XI. And my', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 0}), Document(page_content='this to denote the predicted value of “by my hypothesis H” on the input XI. And my \\nhypothesis was franchised by the vector of gram s as theta and so we said that this was \\nequal to some from theta J, si J, and more  theta transpose X. And we had the convention \\nthat X subscript Z is equal to one so this accounts for the intercept term in our linear regression model. And lowercas e n here was the notation I was using for the number of \\nfeatures in my training set. Okay? So in  the example when trying to predict housing \\nprices, we had two features, the size of the house and the number of bedrooms. We had \\ntwo features and there was – li ttle n was equal to two. So just to finish recapping the \\nprevious lecture, we define d this quadratic cos function J of theta equals one-half, \\nsomething I equals one to m, theta of XI mi nus YI squared where this is the sum over our \\nm training examples and my training set. So lowercase m was the notation I’ve been \\nusing to denote the number of training exampl es I have and the size of my training set. \\nAnd at the end of the last lecture, we deri ve the value of theta that minimizes this \\nenclosed form, which was X transpose X inverse X transpose Y. Okay?  \\nSo as we move on in today’s lecture, I’ll cont inue to use this notation and, again, I realize \\nthis is a fair amount of notation to all reme mber, so if partway th rough this lecture you \\nforgot – if you’re having trouble remembering what lowercase m is or what lowercase n', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 0}), Document(page_content='forgot – if you’re having trouble remembering what lowercase m is or what lowercase n \\nis or something please raise your hand and ask. When we talked about linear regression \\nlast time we used two features. One of the f eatures was the size of the houses in square \\nfeet, so the living area of the house, and the other feature was the number of bedrooms in \\nthe house. In general, we apply a machine- learning algorithm to some problem that you \\ncare about. The choice of the features will very much be up to you, right? And the way \\nyou choose your features to give the learning algorithm will often have a large impact on \\nhow it actually does. So just for example, the choice we made last time was X1 equal this \\nsize, and let’s leave this idea of the feature of the number of bedrooms for now, let’s say \\nwe don’t have data that tells us how many bedrooms are in these houses. One thing you', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 0}), Document(page_content='could do is actually define – oh, let’s draw this out. And s o, right? So say that was the \\nsize of the house and that’s th e price of the house. So if you use this as a feature maybe \\nyou get theta zero plus theta 1, X1, this, sort of, linear model. If you choose – let me just copy the same data set over, right? You can de fine the set of features where X1 is equal \\nto the size of the house and X2 is the square of the size of the house. Okay? So X1 is the \\nsize of the house in say square footage and X2 is just take wh atever the square footage of \\nthe house is and just square that number, and this would be another way to come up with \\na feature, and if you do that then the sa me algorithm will end up fitting a quadratic \\nfunction for you. Theta 2, XM squared. Ok ay? Because this is actually X2. And \\ndepending on what the data looks like, maybe th is is a slightly better fit to the data.  \\nYou can actually take this even further, right?  Which is – let’s see. I have seven training \\nexamples here, so you can actually maybe fit up to six for the polynomial. You can \\nactually fill a model theta zero plus theta one , X1 plus theta two, X squared plus up to \\ntheta six. X to the power of six and theta six are the polynomial to  these seven data \\npoints. And if you do that you find that you co me up with a model that fits your data \\nexactly. This is where, I guess, in this exampl e I drew, we have seven data points, so if', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 1}), Document(page_content='exactly. This is where, I guess, in this exampl e I drew, we have seven data points, so if \\nyou fit a six model polynomial you can, sort of, fit a line that  passes through these seven \\npoints perfectly. And you probably find that the curve you get will look something like \\nthat. And on the one hand, this is a great m odel in a sense that it fits your training data \\nperfectly. On the other hand, this is probabl y not a very good model in the sense that \\nnone of us seriously think th at this is a very good predic tor of housing prices as a \\nfunction of the size of the house, right?  \\nSo we’ll actually come back to this later. It  turns out of the models  we have here; I feel \\nlike maybe the quadratic model fits the data  best. Whereas the linear model looks like \\nthere’s actually a bit of  a quadratic component in this data  that the linear function is not \\ncapturing. So we’ll actually come back to this a little bit later and talk about the problems \\nassociated with fitting models that are either  too simple, use two small a set of features, \\nor on the models that are too complex and maybe  use too large a set of features. Just to \\ngive these a name, we call this the problem of underfitting and, very informally, this \\nrefers to a setting where there are obvious patt erns that – where there are patterns in the \\ndata that the algorithm is just failing to f it. And this problem here we refer to as', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 1}), Document(page_content='data that the algorithm is just failing to f it. And this problem here we refer to as \\noverfitting and, again, very informally, this  is when the algorithm is fitting the \\nidiosyncrasies of this specific data set, right ? It just so happens th at of the seven houses \\nwe sampled in Portland, or wherever you collect  data from, that house happens to be a bit \\nmore expensive, that house happened on the less expensive, and by fitting six for the polynomial we’re, sort of, fitting the idiosyncratic properties of this data  set, rather than \\nthe true underlying tren ds of how housing prices vary as  the function of the size of house. \\nOkay?  \\nSo these are two very different problems. We’ ll define them more formally me later and \\ntalk about how to address each of these probl ems, but for now I hope you appreciate that \\nthere is this issue of selecting features. So  if you want to just teach us the learning \\nproblems there are a few ways to do so. We’ll talk about feature sele ction algorithms later \\nthis quarter as well. So automatic algorith ms for choosing what features you use in a', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 1}), Document(page_content='regression problem like this. What I want to do today is talk about a class of algorithms \\ncalled non-parametric learning algorithms that will help to alleviate the need somewhat \\nfor you to choose features very carefully. Okay ? And this leads us in to our discussion of \\nlocally weighted regression. And just to de fine the term, linear regression, as we’ve \\ndefined it so far, is an example of a parame tric learning algorithm. Parametric learning \\nalgorithm is one that’s defined as an algorithm that has a fixed number of parameters that \\nfit to the data. Okay? So in linear regression we  have a fix set of parameters theta, right? \\nThat must fit to the data. In contrast, what  I’m gonna talk about now is our first non-\\nparametric learning algorithm. The formal defi nition, which is not very  intuitive, so I’ve \\nreplaced it with a second, say, more intuitive. The, sort of, formal definition of the non-\\nparametric learning algorithm is that it’s an  algorithm where the number of parameters \\ngoes with M, with the size of the training se t. And usually it’s de fined as a number of \\nparameters grows linearly with the size of the training set. Th is is the formal definition. A \\nslightly less formal definition is that th e amount of stuff that your learning algorithm \\nneeds to keep around will grow linearly with th e training sets or, in another way of saying \\nit, is that this is an algorithm that we’ll n eed to keep around an entire training set, even', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 2}), Document(page_content='it, is that this is an algorithm that we’ll n eed to keep around an entire training set, even \\nafter learning. Okay? So don’t worry too much about this de finition. But what I want to \\ndo now is describe a specific non-parametric  learning algorithm ca lled locally weighted \\nregression. Which also goes by a couple of other names – which also goes by the name of \\nLoess for self-hysterical reasons. Loess is us ually spelled L-O-E-S-S, sometimes spelled \\nlike that, too. I just call it local ly weighted regression. So here ’s the idea. This will be an \\nalgorithm that allows us to worry a little b it less about having to choose features very \\ncarefully. So for my motivating example, let’s say that I have  a training site  that looks \\nlike this, okay? So this is X and that’s Y.  If you run linear regre ssion on this and you fit \\nmaybe a linear function to this and you end up with a more or less flat, straight line, \\nwhich is not a very good fit to  this data. You can sit around a nd stare at this and try to \\ndecide whether the features are used right. So maybe you want to toss in a quadratic \\nfunction, but this isn’t really quadratic eith er. So maybe you want to model this as a X \\nplus X squared plus maybe some function of  sin of X or someth ing. You actually sit \\naround and fiddle with features. And after a wh ile you can probably come up with a set of \\nfeatures that the model is okay, but let’s talk  about an algorithm th at you can use without \\nneeding to do that.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 2}), Document(page_content='features that the model is okay, but let’s talk  about an algorithm th at you can use without \\nneeding to do that.  \\nSo if – well, suppose you want to evaluate  your hypothesis H at a certain point with a \\ncertain query point low K is X. Okay? A nd let’s say you want to know what’s the \\npredicted value of Y at this position of X, right? So for lin ear regression, what we were \\ndoing was we would fit theta to minimize sum over I, YI minus theta, transpose XI \\nsquared, and return theta transpose X. Okay? So that was linear regres sion. In contrast, in \\nlocally weighted linear regression you’re goi ng to do things slightly different. You’re \\ngoing to look at this point X and then I’m going to look in my data set and take into \\naccount only the data points that are, sort of, in the little vicinity of X. Okay? So we’ll \\nlook at where I want to value my hypothesis. I’m  going to look only in the vicinity of this \\npoint where I want to value my hypothesis, and then I’m going to take, let’s say, just \\nthese few points, and I will apply linear regression  to fit a straight line just to this sub-set \\nof the data. Okay? I’m using this sub-term sub- set – well let’s come back to that later. So \\nwe take this data set and I fit a straight line to it and maybe I get a straight line like that.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 2}), Document(page_content='And what I’ll do is then evaluate this particular  value of straig ht line and that will be the \\nvalue I return for my algorithm. I think this would be the predicted value for – this would \\nbe the value of then my hypothesis output s in locally weighted regression. Okay?  \\nSo we’re gonna fall one up. Let me go ahead and formalize that. In locally weighted \\nregression, we’re going to fit theta to minimi ze sum over I to minimize that where these \\nterms W superscript I are called weights. Th ere are many possible choice for ways, I’m \\njust gonna write one down. So this E’s and mi nus, XI minus X squared over two. So let’s \\nlook at what these weights really are, right ? So notice that – suppose you have a training \\nexample XI. So that XI is very close to X. So this is small, right? Then if XI minus X is \\nsmall, so if XI minus X is close to zero, th en this is E’s to the minus zero and E to the \\nzero is one. So if XI is close to X, then WI will be close to one. In other words, the \\nweight associated with the, I training exampl e be close to one if XI and X are close to \\neach other. Conversely if XI minus X is large then – I don’t know, what would WI be?  \\nStudent: Zero.  \\nInstructor (Andrew Ng) :Zero, right. Close to zero. Right. So if XI is very far from X', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 3}), Document(page_content='Student: Zero.  \\nInstructor (Andrew Ng) :Zero, right. Close to zero. Right. So if XI is very far from X \\nthen this is E to the minus of some large number and E to the minus some large number will be close to zero. Okay? So the picture is, if I’m quarrying at a certain point X, shown on the X axis, and if my data set, say, l ook like that, then I’m going to give the points \\nclose to this a large weight and give the points far away a small weight. So for the points \\nthat are far away, WI will be close to zero. A nd so as if for the points that are far away, \\nthey will not contribute much at all to this  summation, right? So I th ink this is sum over I \\nof one times this quadratic term for points by points plus zero times this quadratic term \\nfor faraway points. And so the effect of usi ng this weighting is that locally weighted \\nlinear regression fits a set of parameters thet a, paying much more a ttention to fitting the \\npoints close by accurately. Whereas ignoring the contribution from faraway points. \\nOkay? Yeah?  \\nStudent: Your Y is exponentially [inaudible]?  \\nInstructor (Andrew Ng) :Yeah. Let’s see. So it turns out  there are many other weighting \\nfunctions you can use. It turns out that th ere are definitely different communities of \\nresearchers that tend to choose different c hoices by default. There is somewhat of a \\nliterature on debating what point – exactly what function to us e. This, sort of, exponential', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 3}), Document(page_content='literature on debating what point – exactly what function to us e. This, sort of, exponential \\ndecay function is – this happens to be a reas onably common one that seems to be a more \\nreasonable choice on many problems, but you ca n actually plug in other functions as \\nwell. Did I mention what [inaudible] is it at ? For those of you that are familiar with the \\nnormal distribution, or the Gaussi an distribution, say this – wh at this formula I’ve written \\nout here, it cosmetically looks a bit like a Ga ussian distribution. Okay? But this actually \\nhas absolutely nothing to  do with Gaussian distribution. So this is not that a problem with \\nXI is Gaussian or whatever. This is no su ch interpretation. This is just a convenient \\nfunction that happens to be a bell-shaped f unction, but don’t endow this of any Gaussian \\nsemantics. Okay?', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 3}), Document(page_content='So, in fact – well, if you remember the fam iliar bell-shaped Gaussia n, again, it’s just the \\nways of associating with these points is th at if you imagine putti ng this on a bell-shaped \\nbump, centered around the position of where you want to value your hypothesis H, then \\nthere’s a saying this point here I’ll give a wei ght that’s proportional to the height of the \\nGaussian – excuse me, to the height of the bell-shaped function eval uated at this point. \\nAnd the way to get to this point will be, to th is training example, wi ll be proportionate to \\nthat height and so on. Okay? And so training ex amples that are really far away get a very \\nsmall weight.  \\nOne last small generalization to this is that  normally there’s one other parameter to this \\nalgorithm, which I’ll denote as tow. Again, this  looks suspiciously lik e the variants of a \\nGaussian, but this is not a Gaussian. This  is a convenient form or function. This \\nparameter tow is called the bandwidth paramete r and informally it controls how fast the \\nweights fall of with distance. Okay? So just  copy my diagram from the other side, I \\nguess. So if tow is very small, if that’s  a query X, then you end up choosing a fairly \\nnarrow Gaussian – excuse me, a fairly narrow be ll shape, so that the weights of the points \\nare far away fall off rapidly. Whereas if tow is large then you’d end up choosing a \\nweighting function that falls of relatively sl owly with distance from your query. Okay?', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 4}), Document(page_content='weighting function that falls of relatively sl owly with distance from your query. Okay?  \\nSo I hope you can, therefore, see that if you a pply locally weighted li near regression to a \\ndata set that looks like this, th en to ask what your hypothesis out put is at a point like this \\nyou end up having a straight line making that pr ediction. To ask what kind of class this \\n[inaudible] at that value you put  a straight line there and you pr edict that value. It turns \\nout that every time you try to vary your hypothesis, every time you ask your learning \\nalgorithm to make a prediction for how much a new house costs or whatever, you need to \\nrun a new fitting procedure and th en evaluate this line that yo u fit just at the position of \\nthe value of X. So the positi on of the query where you’re trying to make a prediction. \\nOkay? But if you do this for every point along the X-axis then you find that locally weighted regression is able to  trace on this, sort of, very non-linear curve for a data set \\nlike this. Okay?  \\nSo in the problem set we’re actually gonna let you play around more with this algorithm. \\nSo I won’t say too much more about it here. Bu t to finally move on to the next topic let \\nme check the questions you have. Yeah?  \\nStudent: It seems like you still have the same pr oblem of overfitting and underfitting, like \\nwhen you had a Q’s tow. Like you make it too small in your –  \\nInstructor (Andrew Ng) :Yes, absolutely. Yes. So local ly weighted regression can run', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 4}), Document(page_content='when you had a Q’s tow. Like you make it too small in your –  \\nInstructor (Andrew Ng) :Yes, absolutely. Yes. So local ly weighted regression can run \\ninto – locally weighted regression is not a penancier for the problem  of overfitting or \\nunderfitting. You can still run into the same problems with locally weighted regression. \\nWhat you just said about – and so some of these things I’ll leave you to discover for \\nyourself in the homework problem. You’ll actu ally see what you just mentioned. Yeah?  \\nStudent: It almost seems like you’re not even th oroughly [inaudible] w ith this locally \\nweighted, you had all the data th at you originally had anyway.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 4}), Document(page_content='Instructor (Andrew Ng) :Yeah.  \\nStudent: I’m just trying to think of [inaudi ble] the original data points.  \\nInstructor (Andrew Ng) :Right. So the question is, sort of , this – it’s almost as if you’re \\nnot building a model, because you need the en tire data set. And th e other way of saying \\nthat is that this is a non-parametric learni ng algorithm. So this –I don’t know. I won’t \\ndebate whether, you know, are we really build ing a model or not. But this is a perfectly \\nfine – so if I think when you write a c ode implementing locally weighted linear \\nregression on the data set I think of that code  as a whole – as building your model. So it \\nactually uses – we’ve actually used this quite successfully to model,  sort of, the dynamics \\nof this autonomous helicopter this is. Yeah?  \\nStudent: I ask if this algorithm that lear n the weights based on the data?  \\nInstructor (Andrew Ng) :Learn what weights? Oh, the weights WI.  \\nStudent: Instead of using [inaudible].  \\nInstructor (Andrew Ng) :I see, yes. So it turns out th ere are a few things you can do. One \\nthing that is quite common is how to choos e this band with para meter tow, right? As \\nusing the data. We’ll actually talk about that a bit later when we talk about model \\nselection. Yes? One last question.  \\nStudent: I used [inaudible] Gaussian sometimes if  you [inaudible] Gaussian and then –  \\nInstructor (Andrew Ng) :Oh, I guess. Lt’s see. Boy. The weights are not random', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 5}), Document(page_content='Instructor (Andrew Ng) :Oh, I guess. Lt’s see. Boy. The weights are not random \\nvariables and it’s not, for the pur pose of this algorithm, it is  not useful to endow it with \\nprobable semantics. So you could choose to defi ne things as Gaussian, but it, sort of, \\ndoesn’t lead anywhere. In fact, it turns out th at I happened to choose this, sort of, bell-\\nshaped function to define my weights. It’s act ually fine to choose a function that doesn’t \\neven integrate to one, that integrates to in finity, say, as you’re we ighting function. So in \\nthat sense, I mean, you could force in the defi nition of a Gaussian, but  it’s, sort of, not \\nuseful. Especially since you us e other functions that inte grate to infinity and don’t \\nintegrate to one. Okay? It’s the last question and let’s move on  \\nStudent: Assume that we have a very huge [inaudi ble], for example. A very huge set of \\nhouses and want to predict the linear for each house and so should the end result for each \\ninput – I’m seeing this very constantly for –  \\nInstructor (Andrew Ng) :Yes, you’re right. So because lo cally weighted regression is a \\nnon-parametric algorithm every time you make a prediction you need to fit theta to your \\nentire training set again. So you’re actually  right. If you have a very large training set \\nthen this is a somewhat expensive algorith m to use. Because every time you want to \\nmake a prediction you need to fit a straight li ne to a huge data set again. Turns out there', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 5}), Document(page_content='make a prediction you need to fit a straight li ne to a huge data set again. Turns out there \\nare algorithms that – turns out there are ways to make this much more efficient for large', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 5}), Document(page_content='data sets as well. So don’t want to talk about  that. If you’re interested, look up the work \\nof Andrew Moore on KD-trees. He, sort of, fi gured out ways to fit these models much \\nmore efficiently. That’s not something I want  to go into today. Okay? Let me move one. \\nLet’s take more questions later.  \\nSo, okay. So that’s locally weighted regres sion. Remember the outline I had, I guess, at \\nthe beginning of this lecture. What I want to do now is talk about a probabilistic interpretation of linear regres sion, all right? And in partic ular of the – it’ll be this \\nprobabilistic interpretati on that let’s us move on to talk  about logistic regression, which \\nwill be our first classification algorithm. So le t’s put aside locally weighted regression for \\nnow. We’ll just talk about ordinary unwei ghted linear regression. Let’s ask the question \\nof why least squares, right? Of all the thi ngs we could optimize how do we come up with \\nthis criteria for minimizing the square of  the area between the predictions of the \\nhypotheses and the values Y predicted. So w hy not minimize the absolute value of the \\nareas or the areas to the power of four or something? What I’m going to do now is \\npresent one set of assumptions that will serve to “justify” why we’re minimizing the sum \\nof square zero. Okay?  \\nIt turns out that there are many assumptions th at are sufficient to justify why we do least \\nsquares and this is just one of them. So ju st because I present one set of assumptions', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 6}), Document(page_content='squares and this is just one of them. So ju st because I present one set of assumptions \\nunder which least squares regression make se nse, but this is not the only set of \\nassumptions. So even if the assumptions I de scribe don’t hold, least squares actually still \\nmakes sense in many circumstances. But this sort of new help, you know, give one rationalization, like, one reason for doing le ast squares regression. And, in particular, \\nwhat I’m going to do is endow the least squa res model with probabi listic semantics. So \\nlet’s assume in our example of predicting hous ing prices, that the pr ice of the house it’s \\nsold four, and there’s going to be some linear  function of the features, plus some term \\nepsilon I. Okay? And epsilon I will be our erro r term. You can think of  the error term as \\ncapturing unmodeled effects, like,  that maybe there’s some othe r features of a house, like, \\nmaybe how many fireplaces it ha s or whether there’s a garden or whatever, that there are \\nadditional features that we jut fail to capture or you can th ink of epsilon as random noise. \\nEpsilon is our error term that captures both th ese unmodeled effects. Ju st things we forgot \\nto model. Maybe the function isn’t quite lin ear or something. As well as random noise, \\nlike maybe that day the seller was in a really bad mood and so he sold it, just refused to \\ngo for a reasonable price or something. And now  I will assume that the errors have a', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 6}), Document(page_content='go for a reasonable price or something. And now  I will assume that the errors have a \\nprobabilistic – have a probabil ity distribution. I’ll assume th at the errors epsilon I are \\ndistributed just till they denote epsilon I is distributive according to a probability \\ndistribution. That’s a Gaussian  distribution with mean zer o and variance sigma squared. \\nOkay? So let me just scripts in here, n stands for normal, right? To denote a normal \\ndistribution, also known as the Gaussian di stribution, with mean zero and covariance \\nsigma squared.  \\nActually, just quickly raise your hand if you’ve seen a Gaussian distribution before. \\nOkay, cool. Most of you. Great. Almost everyon e. So, in other words, the density for \\nGaussian is what you’ve seen before. The de nsity for epsilon I would be one over root 2 \\npi sigma, E to the negative, epsilon I squared over 2 sigma squared, right? And the', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 6}), Document(page_content='density of our epsilon I will be  this bell-shaped curve with one standard deviation being \\na, sort of, sigma. Okay? This is form for that  bell-shaped curve. So, let’s see. I can erase \\nthat. Can I erase the board? So this implies that  the probability distri bution of a price of a \\nhouse given in si and the parameters theta, th at this is going to be  Gaussian with that \\ndensity. Okay? In other words, saying goes as that the price of a hous e given the features \\nof the house and my parameters theta, this  is going to be a random variable that’s \\ndistributed Gaussian with mean theta tran spose XI and variance sigma squared. Right? \\nBecause we imagine that the way the housing pr ices are generated is that the price of a \\nhouse is equal to theta transpose XI and th en plus some random Gaussian noise with \\nvariance sigma squared. So the price of a house is going to have mean theta transpose XI, \\nagain, and sigma squared, right? Does this make sense? Raise your hand if this makes \\nsense. Yeah, okay. Lots of you.  \\nIn point of notation – oh, yes?  \\nStudent: Assuming we don’t know anything about th e error, why do you assume here the \\nerror is a Gaussian?  \\nInstructor (Andrew Ng) :Right. So, boy. Why do I see the error as Gaussian? Two \\nreasons, right? One is that it turns out to be mathematically  convenient to do so and the \\nother is, I don’t know, I can also mumble about ju stifications, such as things to the central', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 7}), Document(page_content='other is, I don’t know, I can also mumble about ju stifications, such as things to the central \\nlimit theorem. It turns out that if you, for th e vast majority of problems, if you apply a \\nlinear regression model like this and try to m easure the distribution of the errors, not all \\nthe time, but very often you find that the erro rs really are Gaussian. That this Gaussian \\nmodel is a good assumption for the error in regression proble ms like these. Some of you \\nmay have heard of the central limit theo rem, which says that the sum of many \\nindependent random variables will tend towards a Gaussian. So if the error is caused by many effects, like the mood of th e seller, the mood of the buyer,  some other features that \\nwe miss, whether the place has a garden or not, and if all of these e ffects are independent, \\nthen by the central limit theorem you might be inclined to believe that the sum of all \\nthese effects will be approximately Gaussian. If  in practice, I guess, the two real answers \\nare that, 1.) In practice this is actually a reasonably accurate assumption, and 2.) Is it \\nturns out to be mathematically c onvenient to do so. Okay? Yeah?  \\nStudent: It seems like we’re saying if we assu me that area around model has zero mean, \\nthen the area is centered ar ound our model. Which it seems al most like we’re trying to \\nassume what we’re trying to prove. Instructor? \\nThat’s the [inaudible] but, yes. You are assu ming that the error has zero mean. Which is,', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 7}), Document(page_content='assume what we’re trying to prove. Instructor? \\nThat’s the [inaudible] but, yes. You are assu ming that the error has zero mean. Which is, \\nyeah, right. I think later this quarter we get to some of the other things, but for now just \\nthink of this as a mathematically – it’s act ually not an unreasonabl e assumption. I guess, \\nin machine learning all the assumptions we ma ke are almost never true in the absence \\nsense, right? Because, for instance, housing pric es are priced to dollars and cents, so the \\nerror will be – errors in prices are not c ontinued as value random variables, because \\nhouses can only be priced at a certain number  of dollars and a certa in number of cents \\nand you never have fractions of cents in housing prices. Whereas a Gaussian random', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 7}), Document(page_content='variable would. So in that se nse, assumptions we make are never “absolutely true,” but \\nfor practical purposes this is a accurate enough assumption that  it’ll be useful to make. \\nOkay? I think in a week or two, we’ll actually come back to selected more about the \\nassumptions we make and when they help our  learning algorithms a nd when they hurt our \\nlearning algorithms. We’ll say a bit more about  it when we talk about generative and \\ndiscriminative learning algorithms, like, in a week or two. Okay?  \\nSo let’s point out one bit of notation, which is that when I wrote this down I actually \\nwrote P of YI given XI and then semicolon theta and I’m going to us e this notation when \\nwe are not thinking of theta as a random vari able. So in statistics, though, sometimes it’s \\ncalled the frequentist’s point of view, where you think of there as being some, sort of, \\ntrue value of theta that’s out there that’s generating the data say, but we don’t know what \\ntheta is, but theta is not a ra ndom vehicle, right? So it’s not like there’s some random \\nvalue of theta out there. It’s that theta is – th ere’s some true value of theta out there. It’s \\njust that we don’t know what the true value of theta is. So if theta is not a random \\nvariable, then I’m going to avoid writing P of  YI given XI comma theta, because this \\nwould mean that probably of YI conditio ned on X and theta and you can only condition', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 8}), Document(page_content='would mean that probably of YI conditio ned on X and theta and you can only condition \\non random variables. So at this part of the class where we’re taking sort of frequentist’s \\nviewpoint rather than the Dasi an viewpoint, in this part of  class we’re thinking of theta \\nnot as a random variable, but just as so mething we’re trying to estimate and use the \\nsemicolon notation. So the way to read this is this is the probability  of YI given XI and \\nparameterized by theta. Okay? So you read the semicolon as parameterized by. And in \\nthe same way here, I’ll say YI given XI pa rameterized by theta is distributed Gaussian \\nwith that.  \\nAll right. So we’re gonna make  one more assumption. Let’s assume that the error terms \\nare IID, okay? Which stands for Independently  and Identically Distributed. So it’s going \\nto assume that the error terms are indepe ndent of each other, right? The identically \\ndistributed part just means that I’m a ssuming the outcome for the same Gaussian \\ndistribution or the same varian ce, but the more important part  of is this is that I’m \\nassuming that the epsilon I’s are independent of  each other. Now, let’s talk about how to \\nfit a model. The probability of Y given X parameterized by theta – I’m actually going to give this another name. I’m going to write th is down and we’ll call this the lik elihood of \\ntheta as the probability of Y given X paramete rized by theta. And so this is going to be', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 8}), Document(page_content='theta as the probability of Y given X paramete rized by theta. And so this is going to be \\nthe product over my training set like that. Which is, in turn, going to be a product of those \\nGaussian densities that I wrot e down just now, right? Okay?  \\nThen in parts of notation, I guess, I define th is term here to be the likelihood of theta. \\nAnd the likely of theta is just the probability of the data Y, right? Given X and prioritized \\nby theta. To test the likeli hood and probability are often confused. So the likelihood of \\ntheta is the same thing as the probability of  the data you saw. So likely and probably are, \\nsort of, the same thing. Except that when  I use the term likelihood I’m trying to \\nemphasize that I’m taking this thing and view ing it as a function of  theta. Okay? So \\nlikelihood and for probability, th ey’re really the same thing except that when I want to \\nview this thing as a function of theta holding X and Y fix are then called likelihood. \\nOkay? So hopefully you hear me say the like lihood of the parameters and the probability', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 8}), Document(page_content='of the data, right? Rather than  the likelihood of the data or probability of parameters. So \\ntry to be consistent in that terminology.  \\nSo given that the probability of the data is this and this is also the likelihood of the \\nparameters, how do you estimate the parameters theta? So given a training set, what \\nparameters theta do you want to choose for your model? Well, the principle of maximum \\nlikelihood estimation says that, right? You can choose the valu e of theta that makes the \\ndata as probable as possible, right? So choose theta to maximize the likelihood. Or in \\nother words choose the parameters that make th e data as probable as possible, right? So \\nthis is massive likely your estimation from si x to six. So it’s choose the parameters that \\nmakes it as likely as probable as possible fo r me to have seen the data I just did.  \\nSo for mathematical convenience, let me defi ne lower case l of theta. This is called the \\nlog likelihood function a nd it’s just log of capital L of thet a. So this is log over product \\nover I to find sigma E to that. I won’t bother to write out what’s in the exponent for now. \\nIt’s just saying this from the previous board. Log and a product is the same as the sum of \\nover logs, right? So it’s a sum of the logs of – which simplifies to m times one over root \\ntwo pi sigma plus and then log of explanation ca ncel each other, right? So if log of E of \\nsomething is just whatever’s inside the e xponent. So, you know what, let me write this on', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 9}), Document(page_content='something is just whatever’s inside the e xponent. So, you know what, let me write this on \\nthe next board.  \\nOkay. So maximizing the likelihood or maxi mizing the log likelihood is the same as \\nminimizing that term over there. Well, you ge t it, right? Because th ere’s a minus sign. So \\nmaximizing this because of the minus sign is th e same as minimizing this as a function of \\ntheta. And this is, of course, just the same quadratic cos function that we had last time, J \\nof theta, right? So what we’v e just shown is that the ordina ry least squares algorithm, that \\nwe worked on the previous lecture, is  just maximum likelihood assuming this \\nprobabilistic model, assu ming IID Gaussian errors on our data. Okay?  \\nOne thing that we’ll actually leave is that, in the next lecture no tice that the value of \\nsigma squared doesn’t matter, right? That somehow no matter what the value of sigma \\nsquared is, I mean, sigma squared has to be  a positive number. It ’s a variance of a \\nGaussian. So that no matter what sigma squared is since it’s a positive number the value of theta we end up with will be the same, right? So because minimizing this you get the \\nsame value of theta no matter what sigma squared is. So it’s as if in this model the value \\nof sigma squared doesn’t really  matter. Just remember that for the next lecture. We’ll \\ncome back to this again. Any questions a bout this? Actually, let me clean up another \\ncouple of boards and then I’ll see what questions you have.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 9}), Document(page_content='come back to this again. Any questions a bout this? Actually, let me clean up another \\ncouple of boards and then I’ll see what questions you have.  \\nOkay. Any questions? Yeah?  \\nStudent: You are, I think here you try to measure the likelihood of your nice of theta by a \\nfraction of error, but I think it’s that you measure because it depends on the family of \\ntheta too, for example. If you have a lot of  parameters [inaudible] or fitting in?', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 9}), Document(page_content='Instructor (Andrew Ng) :Yeah, yeah. I mean, you’re asking about overfitting, whether \\nthis is a good model. I thi nk let’s – the thing’s you’re mentioning are maybe deeper \\nquestions about learning algorithms  that we’ll just come back to later, so don’t really \\nwant to get into that right now. Any more questions? Okay.  \\nSo this endows linear regression with a proba bilistic interpretati on. I’m actually going to \\nuse this probabil – use this, sort of, probabilist ic interpretation in order to derive our next \\nlearning algorithm, which will be our first classification algorithm. Okay? So you’ll recall \\nthat I said that regression problems are where the variable Y that you’re trying to predict \\nis continuous values. Now I’m actually gonna ta lk about our first cl assification problem, \\nwhere the value Y you’re trying to predict will be discreet value. You can take on only a \\nsmall number of discrete values and in th is case I’ll talk about binding classification \\nwhere Y takes on only two values, right? So you  come up with classi fication problems if \\nyou’re trying to do, say, a medical diagnosis and try to decide based on some features that \\nthe patient has a disease or does not have a di sease. Or if in the housing example, maybe \\nyou’re trying to decide will this house sell in the next six months or not and the answer is \\neither yes or no. It’ll either be  sold in the next six months or it won’t be. Other standing', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 10}), Document(page_content='either yes or no. It’ll either be  sold in the next six months or it won’t be. Other standing \\nexamples, if you want to build a spam filter. Is this e-mail spam or not? It’s yes or no. Or if you, you know, some of my colleagues sit in  whether predicting whether a computer \\nsystem will crash. So you have a learning algo rithm to predict will this computing cluster \\ncrash over the next 24 hours? And, again, it’s a yes or no answer.  \\nSo there’s X, there’s Y. And in a classifi cation problem Y takes on two values, zero and \\none. That’s it in binding the classification. So what can you do? Well, one thing you \\ncould do is take linear regre ssion, as we’ve described it so far, and apply it to this \\nproblem, right? So you, you know, given this data  set you can fit a st raight line to it. \\nMaybe you get that straight line, right? But th is data set I’ve draw n, right? This is an \\namazingly easy classification probl em. It’s pretty obvious to all of us that, right? The \\nrelationship between X and Y is – well, you ju st look at a value around here and it’s the \\nright is one, it’s the left and Y is zero. So you apply linear regr essions to this data set and \\nyou get a reasonable fit and you can then maybe  take your linear regr ession hypothesis to \\nthis straight line and thres hold it at 0.5. If you do that you ’ll certainly get the right \\nanswer. You predict that if X is to the right of, sort of, the mid-point here then Y is one', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 10}), Document(page_content='answer. You predict that if X is to the right of, sort of, the mid-point here then Y is one \\nand then next to the left of that mid-point then Y is zero.  \\nSo some people actually do this. Apply linear  regression to classi fication problems and \\nsometimes it’ll work okay, but in general it’s actually a pretty bad idea to apply linear \\nregression to classification problems like thes e and here’s why. Let’s say I change my \\ntraining set by giving you just one more tr aining example all the way up there, right? \\nImagine if given this training set is actually  still entirely obvious  what the relationship \\nbetween X and Y is, right? It’s ju st – take this value as greate r than Y is one and it’s less \\nthen Y is zero. By giving you this additiona l training example it really shouldn’t change \\nanything. I mean, I didn’t really convey much  new information. There’s no surprise that \\nthis corresponds to Y equals one. But if you now  fit linear regression to this data set you \\nend up with a line that, I don’t know, maybe  looks like that, right? And now the', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 10}), Document(page_content='predictions of your hypothesis have change d completely if your threshold – your \\nhypothesis at Y equal both 0.5. Okay? So –  \\nStudent: In between there might be an interval wh ere it’s zero, right? For that far off \\npoint?  \\nInstructor (Andrew Ng) :Oh, you mean, like that?  \\nStudent: Right.  \\nInstructor (Andrew Ng) :Yeah, yeah, fine. Yeah, sure. A theta set like that so. So, I \\nguess, these just – yes, you’re right, but this is an example and this example works. This \\n–  \\nStudent: [Inaudible] that will change it even more if you gave it all –  \\nInstructor (Andrew Ng) :Yeah. Then I think this actually  would make it even worse. \\nYou would actually get a lin e that pulls out even further, ri ght? So this is my example. I \\nget to make it whatever I want, right? But th e point of this is that there’s not a deep \\nmeaning to this. The point of this is just that it could be a really bad idea to apply linear \\nregression to classification algorithm. Someti mes it work fine, but usually I wouldn’t do \\nit. So a couple of problems with this. One is that, well – so what do you want to do for \\nclassification? If you know the value of Y lie s between zero and one then to kind of fix \\nthis problem let’s just start by changing the form of our hypothesis so that my hypothesis \\nalways lies in the unit interval between zero a nd one. Okay? So if I know Y is either zero \\nor one then let’s at least not have my hypothe sis predict values much  larger than one and', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 11}), Document(page_content='or one then let’s at least not have my hypothe sis predict values much  larger than one and \\nmuch smaller than zero. And so I’m going to – instead of choosing a linear function for \\nmy hypothesis I’m going to choose something s lightly different. And, in particular, I’m \\ngoing to choose this function, H subscript th eta of X is going to equal to G of theta \\ntranspose X where G is going to be this func tion and so this becomes more than one plus \\ntheta X of theta transpose X. And G of Z is called the sigmoi d function and it is often also \\ncalled the logistic function. It goe s by either of these names.  \\nAnd what G of Z looks like is the followi ng. So when you have your  horizontal axis I’m \\ngoing to plot Z and so G of Z will look like this. Okay? I didn’t draw that very well. \\nOkay. So G of Z tends towards zero as Z becomes very small and G of Z will ascend towards one as Z becomes large and it crosses the vertical axis at 0.5. So this is what \\nsigmoid function, also called the logi stic function of. Yeah? Question?  \\nStudent: What sort of sigmoid in other step five?  \\nInstructor (Andrew Ng) :Say that again.  \\nStudent: Why we cannot chose this at five for so me reason, like, that’s better binary.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 11}), Document(page_content='Instructor (Andrew Ng) :Yeah. Let me come back to that later. So it turns out that Y – \\nwhere did I get this function fr om, right? I just wrote down th is function. It actually turns \\nout that there are two reasons for using this  function that we’ll come to. One is – we \\ntalked about generalized linear m odels. We’ll see that this falls out naturally as part of the \\nbroader class of models. And another reason th at we’ll talk about ne xt week, it turns out \\nthere are a couple of, I think, ve ry beautiful reasons for why we choose logistic functions. \\nWe’ll see that in a little bit. But for now let me just define it  and just take my word for it \\nfor now that this is a reasonable choice. Okay? But notice now th at my – the values \\noutput by my hypothesis will always be between  zero and one. Furthermore, just like we \\ndid for linear regression, I’m going to e ndow the outputs and my hypothesis with a \\nprobabilistic interpretati on, right? So I’m going to assume that the probability that Y is \\nequal to one given X and parameterized by theta that’s equal to H subscript theta of X, all \\nright? So in other words I’m going to imagin e that my hypothesis is outputting all these \\nnumbers that lie between zero and one. I’m going to think of my hypothesis as trying to \\nestimate the probability that Y is equal to one . Okay? And because Y has to be either zero \\nor one then the probability of Y equals zero is  going to be that. All right? So more simply', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 12}), Document(page_content='or one then the probability of Y equals zero is  going to be that. All right? So more simply \\nit turns out – actually, take th ese two equations and write them more compactly. Write P \\nof Y given X parameterized by theta. This is going to be H subscript theta of X to the \\npower of Y times one minus H of X to  the power of one minus Y. Okay?  \\nSo I know this looks somewhat bizarre, but th is actually makes the variation much nicer. \\nSo Y is equal to one then this equation is H of X to the power of one times something to \\nthe power of zero. So anything to the power of zero is just one, righ t? So Y equals one \\nthen this is something to the pow er of zero and so this is just one. So if Y equals one this \\nis just saying P of Y equals one is equal to H subscript theta of X. Okay? And in the same \\nway, if Y is equal to zero then this is P of Y equals zero equals this thing to the power of \\nzero and so this disappears. This is just one times this thing power of one. Okay? So this \\nis a compact way of writing both of these equatio ns to gather them to one line. So let’s \\nhope our parameter fitting, right? And, again, you can ask – well, given this model by \\ndata, how do I fit the parameters theta of my  model? So the likelihood of the parameters \\nis, as before, it’s just the pr obability of theta, right? Which is product over I, PFYI given \\nXI parameterized by theta. Which is – just plugging those in. Okay? I dropped this theta', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 12}), Document(page_content='XI parameterized by theta. Which is – just plugging those in. Okay? I dropped this theta \\nsubscript just so you can write a little bit less. Oh, excuse me. These should be XI’s and \\nYI’s. Okay?  \\nSo, as before, let’s say we want to find a maximum likelihood estimate of the parameters \\ntheta. So we want to find the – setting the parameters theta that maximizes the likelihood \\nL of theta. It turns out that very often – ju st when you work with th e derivations, it turns \\nout that it is often much easier to maximize the log of the likelihood rather than maximize \\nthe likelihood. So the log likelihood L of theta is just log of capital L. This will, therefore, \\nbe sum of this. Okay? And so to fit the para meters theta of our model we’ll find the value \\nof theta that maximizes this log likelihood. Yeah?  \\nStudent: [Inaudible]  \\nInstructor (Andrew Ng) :Say that again.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 12}), Document(page_content='Student: YI is [inaudible].  \\nInstructor (Andrew Ng) :Oh, yes. Thanks. So having maximized this function – well, it \\nturns out we can actually apply the same gradient descent algo rithm that we learned. That \\nwas the first algorithm we used to minimize the quadratic function. And you remember, when we talked about least squares, the first algorithm we used to minimize the quadratic \\nerror function was great in descent. So can actually use exactly the same algorithm to \\nmaximize the log likelihood. And you remember, that algorithm was just repeatedly take \\nthe value of theta and you repla ce it with the previous value of theta plus a learning rate \\nalpha times the gradient of the cos functi on. The log likelihood w ill respect the theta. \\nOkay? One small change is that because pr eviously we were trying to minimize the \\nquadratic error term. Today we’re trying to maximize rather than minimize. So rather \\nthan having a minus sign we have a plus sign. So this is just great in ascents, but for the \\nmaximization rather than the minimization. So we actually call this gradient ascent and \\nit’s really the same algorithm.  \\nSo to figure out what this grad ient – so in order to derive gradient descent, what you need \\nto do is compute the partial de rivatives of your objective func tion with respect to each of \\nyour parameters theta I, right? It turns out  that if you actually compute this partial', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 13}), Document(page_content='your parameters theta I, right? It turns out  that if you actually compute this partial \\nderivative – so you take this formula, this L of theta, which is – oh, got that wrong too. If \\nyou take this lower case l theta,  if you take the log likelihood of  theta, and if you take it’s \\npartial derivative with respect to theta I you find that this is  equal to – let’s see. Okay? \\nAnd, I don’t know, the derivation isn’t terribly co mplicated, but in the interest of saving \\nyou watching me write down a couple of blackbo ards full of math I’ll just write down the \\nfinal answer. But the way you get this is you ju st take those, plug in the definition for F \\nsubscript theta as function of XI, and take derivatives, and work through the algebra it \\nturns out it’ll simplify down to this formula. Okay?  \\nAnd so what that gives you is that gradie nt ascent is the following rule. Theta J gets \\nupdated as theta J plus alpha gives this. Ok ay? Does this look familiar to anyone? Did \\nyou remember seeing this formula at the la st lecture? Right. So when I worked up \\nBastrian descent for least squares regressi on I, actually, wrote down exactly the same \\nthing, or maybe there’s a minus sign and this is  also fit. But I, actually, had exactly the \\nsame learning rule last time for least squares regression, right ? Is this the same learning \\nalgorithm then? So what’s different? How come  I was making all that noise earlier about', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 13}), Document(page_content='algorithm then? So what’s different? How come  I was making all that noise earlier about \\nleast squares regression being a bad idea for classification problems and then I did a \\nbunch of math and I skipped some steps, but I’m, sort of, claiming at the end they’re \\nreally the same learning algorithm?  \\nStudent: [Inaudible] constants?  \\nInstructor (Andrew Ng) :Say that again.  \\nStudent: [Inaudible]  \\nInstructor (Andrew Ng) :Oh, right. Okay, cool.', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 13}), Document(page_content='Student: It’s the lowest it –  \\nInstructor (Andrew Ng) :No, exactly. Right. So zero to the same, this is not the same, \\nright? And the reason is, in logi stic regression this is diffe rent from before, right? The \\ndefinition of this H subscript theta of XI is not the same as the definition I was using in \\nthe previous lecture. And in pa rticular this is no longer thet a transpose XI. This is not a \\nlinear function anymore. This is  a logistic function of theta transpose XI. Okay? So even \\nthough this looks cosmetically similar, even though this is similar on the surface, to the \\nBastrian descent rule I derive d last time for least squares regression this is actually a \\ntotally different learning algorithm. Okay? And it turns out that there’s actually no \\ncoincidence that you ended up with the same l earning rule. We’ll actually talk a bit more \\nabout this later when we talk about generalized linear models. But this is one of the most \\nelegant generalized learning models that we’l l see later. That even though we’re using a \\ndifferent model, you actually ended up with wh at looks like the sa me learning algorithm \\nand it’s actually no coincidence. Cool.  \\nOne last comment as part of a sort of l earning process, over here I said I take the \\nderivatives and I ended up with this line . I didn’t want to make you sit through a long \\nalgebraic derivation, but later t oday or later this week, pleas e, do go home and look at our', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 14}), Document(page_content='algebraic derivation, but later t oday or later this week, pleas e, do go home and look at our \\nlecture notes, where I wrote out the entirety of  this derivation in full, and make sure you \\ncan follow every single step of how we take partial derivatives of this log likelihood to \\nget this formula over here. Okay? By the way, for those who are inte rested in seriously \\nmasking machine learning material, when you go home and look at the lecture notes it \\nwill actually be very easy for most of you to look through the lecture notes and read \\nthrough every line and go yep, that makes sense,  that makes sense, that makes sense, and, \\nsort of, say cool. I see how you get this line. You want to make sure you really \\nunderstand the material. My concrete suggesti on to you would be to you to go home, read \\nthrough the lecture notes, check every line, and then to cover up the derivation and see if \\nyou can derive this example, right? So in general, that’s usually good advice for studying \\ntechnical material like machine learning. Wh ich is if you work through a proof and you \\nthink you understood every line, the way to make sure you rea lly understood it is to cover \\nit up and see if you can rederive the entire th ing itself. This is actually a great way \\nbecause I did this a lot when I was trying to  study various pieces of machine learning \\ntheory and various proofs. And this is actua lly a great way to study because cover up the', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 14}), Document(page_content='theory and various proofs. And this is actua lly a great way to study because cover up the \\nderivations and see if you can do it yourself without looking at the origin al derivation. All \\nright.  \\nI probably won’t get to Newton’s Method toda y. I just want to say – take one quick \\ndigression to talk about one more algorithm, which was the discussion sort of alluding to \\nthis earlier, which is the perceptron algorithm, right? So I’m not gonna say a whole lot \\nabout the perceptron algorithm, but this is some thing that we’ll come back to later. Later \\nthis quarter we’ll talk about le arning theory. So in logistic re gression we said that G of Z \\nare, sort of, my hypothesis output values th at were low numbers between zero and one. \\nThe question is what if you want to force G of  Z to up the value to either zero one? So the \\nperceptron algorithm defines G of Z to be this. So the picture is – or  the cartoon is, rather \\nthan this sigmoid function. E of Z now looks like this step function that you were asking', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 14}), Document(page_content='about earlier. In saying this before, we can use H subscript theta of X equals G of theta \\ntranspose X. Okay? So this is actually – ever ything is exactly the same as before, except \\nthat G of Z is now the step f unction. It turns out there’s this  learning called the perceptron \\nlearning rule that’s actually even the same as the classic gradient ascent for logistic \\nregression. And the learning rule is given by th is. Okay? So it looks just like the classic \\ngradient ascent rule for logist ic regression. So this is very  different flavor of algorithm \\nthan least squares regression and logistic regr ession, and, in particular, because it outputs \\nonly values are either zero or one it turns out  it’s very difficult to  endow this algorithm \\nwith probabilistic semantics. And this is , again, even though – oh, excuse me. Right \\nthere. Okay. And even though this learning ru le looks, again, looks  cosmetically very \\nsimilar to what we have in l ogistics regression this is actual ly a very different type of \\nlearning rule than the others that were seen in this class. So because this is such a simple \\nlearning algorithm, right? It ju st computes theta transpose X and then you threshold and \\nthen your output is zero or one . This is – right. So thes e are a simpler algorithm than \\nlogistic regression, I think. When  we talk about learning theory  later in this class, the \\nsimplicity of this algorithm will let us come back and use it as a building block. Okay?', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 15}), Document(page_content='simplicity of this algorithm will let us come back and use it as a building block. Okay? \\nBut that’s all I want to say about this algorithm for now.  \\nJust for fun, the last thing I’l l do today is show you a histor ical video with – that talks \\nabout the perceptron algorithm. This particular  video comes from a video series titled The \\nMachine that Changed The World and was produced WGBH Television in cooperation \\nwith the BBC, British Broadcasting Corporati on, and it aired on PBS a few years ago. \\nThis shows you what machine learning used to  be like. It’s a f un clip on perceptron \\nalgorithm.  \\nIn the 1950’s and 60’s scientists built a few work ing perceptrons, as these artificial brains \\nwere called. He’s using it to explore the myster ious problem of how the brain learns. This \\nperceptron is being trained to recognize the difference between males and females. It is \\nsomething that all of us can do easily, but fe w of us can explain how. To get a computer \\nto do this it would involve working out ma ny complex rules about faces and writing a \\ncomputer program, but this perceptron was si mply given lots and lots of examples, \\nincluding some with unus ual hairstyles. But when it comes to a beetle the computer looks \\nat facial features and hair out line and takes longer to learn wh at it’s told by Dr. Taylor. \\nAndrew puts on his wig also causes a little part searching. After training on lots of', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 15}), Document(page_content='Andrew puts on his wig also causes a little part searching. After training on lots of \\nexamples, it’s given new faces it has never se en and is able to successfully distinguish \\nmale from female. It has learned.  \\nAll right. Isn’t that gr eat? Okay. That’s it for today. I’ll see you guys at the next lecture.  \\n[End of Audio]  \\nDuration: 75 minutes', metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf', 'page': 15})]\n","page_content=\"MachineLearning-Lecture01  \\nInstructor (Andrew Ng):  Okay. Good morning. Welcome to CS229, the machine \\nlearning class. So what I wanna do today is ju st spend a little time going over the logistics \\nof the class, and then we'll start to  talk a bit about machine learning.  \\nBy way of introduction, my name's  Andrew Ng and I'll be instru ctor for this class. And so \\nI personally work in machine learning, and I' ve worked on it for about 15 years now, and \\nI actually think that machine learning is th e most exciting field of all the computer \\nsciences. So I'm actually always excited about  teaching this class. Sometimes I actually \\nthink that machine learning is not only the most exciting thin g in computer science, but \\nthe most exciting thing in all of human e ndeavor, so maybe a little bias there.  \\nI also want to introduce the TAs, who are all graduate students doing research in or \\nrelated to the machine learni ng and all aspects of machin e learning. Paul Baumstarck \\nworks in machine learning and computer vision.  Catie Chang is actually a neuroscientist \\nwho applies machine learning algorithms to try to understand the human brain. Tom Do \\nis another PhD student, works in computa tional biology and in sort of the basic \\nfundamentals of human learning. Zico Kolter is  the head TA — he's head TA two years \\nin a row now — works in machine learning a nd applies them to a bunch of robots. And \\nDaniel Ramage is — I guess he's not here  — Daniel applies l earning algorithms to\" metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 0}\n","27\n","[Document(page_content='第⼀回：Matplotlib 初相识\\n⼀、认识matplotlib\\nMatplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种静态，动态，\\n交互式的图表。\\nMatplotlib 可⽤于 Python 脚本， Python 和 IPython Shell 、 Jupyter notebook ， Web 应⽤程序服务器和各种图形⽤户界⾯⼯具包等。\\nMatplotlib 是 Python 数据可视化库中的泰⽃，它已经成为 python 中公认的数据可视化⼯具，我们所熟知的 pandas 和 seaborn 的绘图接⼝\\n其实也是基于 matplotlib 所作的⾼级封装。\\n为了对matplotlib 有更好的理解，让我们从⼀些最基本的概念开始认识它，再逐渐过渡到⼀些⾼级技巧中。\\n⼆、⼀个最简单的绘图例⼦\\nMatplotlib 的图像是画在 figure （如 windows ， jupyter 窗体）上的，每⼀个 figure ⼜包含了⼀个或多个 axes （⼀个可以指定坐标系的⼦区\\n域）。最简单的创建 figure 以及 axes 的⽅式是通过 pyplot.subplots命令，创建 axes 以后，可以使⽤ Axes.plot绘制最简易的折线图。\\nimport matplotlib.pyplot as plt\\nimport matplotlib as mpl\\nimport numpy as np\\nfig, ax = plt.subplots()  # 创建⼀个包含⼀个 axes 的 figure\\nax.plot([1, 2, 3, 4], [1, 4, 2, 3]);  # 绘制图像\\nTrick： 在jupyter notebook 中使⽤ matplotlib 时会发现，代码运⾏后⾃动打印出类似 <matplotlib.lines.Line2D at 0x23155916dc0>\\n这样⼀段话，这是因为 matplotlib 的绘图代码默认打印出最后⼀个对象。如果不想显示这句话，有以下三种⽅法，在本章节的代码示例\\n中你能找到这三种⽅法的使⽤。\\n\\x00. 在代码块最后加⼀个分号 ;\\n\\x00. 在代码块最后加⼀句 plt.show()\\n\\x00. 在绘图时将绘图对象显式赋值给⼀个变量，如将 plt.plot([1, 2, 3, 4]) 改成 line =plt.plot([1, 2, 3, 4])\\n和MATLAB 命令类似，你还可以通过⼀种更简单的⽅式绘制图像， matplotlib.pyplot⽅法能够直接在当前 axes 上绘制图像，如果⽤户\\n未指定axes ， matplotlib 会帮你⾃动创建⼀个。所以上⾯的例⼦也可以简化为以下这⼀⾏代码。\\nline =plt.plot([1, 2, 3, 4], [1, 4, 2, 3]) \\n三、Figure 的组成\\n现在我们来深⼊看⼀下 figure 的组成。通过⼀张 figure 解剖图，我们可以看到⼀个完整的 matplotlib 图像通常会包括以下四个层级，这些\\n层级也被称为容器（ container ），下⼀节会详细介绍。在 matplotlib 的世界中，我们将通过各种命令⽅法来操纵图像中的每⼀个部分，\\n从⽽达到数据可视化的最终效果，⼀副完整的图像实际上是各类⼦元素的集合。\\nFigure：顶层级，⽤来容纳所有绘图元素', metadata={'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf', 'page': 0}), Document(page_content='Axes：matplotlib 宇宙的核⼼，容纳了⼤量元素⽤来构造⼀幅幅⼦图，⼀个 figure 可以由⼀个或多个⼦图组成\\nAxis：axes的下属层级，⽤于处理所有和坐标轴，⽹格有关的元素\\nTick：axis的下属层级，⽤来处理所有和刻度有关的元素\\n四、两种绘图接⼝\\nmatplotlib 提供了两种最常⽤的绘图接⼝\\n\\x00. 显式创建figure 和 axes ，在上⾯调⽤绘图⽅法，也被称为 OO 模式（ object-oriented style)\\n\\x00. 依赖pyplot ⾃动创建 figure 和 axes ，并绘图\\n使⽤第⼀种绘图接⼝，是这样的：\\nx = np.linspace(0, 2, 100)\\nfig, ax = plt.subplots()  \\nax.plot(x, x, label=\\'linear\\')  \\nax.plot(x, x**2, label=\\'quadratic\\')  \\nax.plot(x, x**3, label=\\'cubic\\')  \\nax.set_xlabel(\\'x label\\') \\nax.set_ylabel(\\'y label\\') \\nax.set_title(\"Simple Plot\")  \\nax.legend() \\nplt.show()\\n⽽如果采⽤第⼆种绘图接⼝，绘制同样的图，代码是这样的：', metadata={'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf', 'page': 1}), Document(page_content='By Datawhale 数据可视化开源⼩组\\n© Copyright © Copyright 2021.x = np.linspace(0, 2, 100)\\nplt.plot(x, x, label=\\'linear\\') \\nplt.plot(x, x**2, label=\\'quadratic\\')  \\nplt.plot(x, x**3, label=\\'cubic\\')\\nplt.xlabel(\\'x label\\')\\nplt.ylabel(\\'y label\\')\\nplt.title(\"Simple Plot\")\\nplt.legend()\\nplt.show()\\n五、通⽤绘图模板\\n由于matplotlib 的知识点⾮常繁杂，在实际使⽤过程中也不可能将全部 API 都记住，很多时候都是边⽤边查。因此这⾥提供⼀个通⽤的绘\\n图基础模板，任何复杂的图表⼏乎都可以基于这个模板⻣架填充内容⽽成。初学者刚开始学习时只需要牢记这⼀模板就⾜以应对⼤部分\\n简单图表的绘制，在学习过程中可以将这个模板模块化，了解每个模块在做什么，在绘制复杂图表时如何修改，填充对应的模块。\\n# step1 准备数据\\nx = np.linspace(0, 2, 100)\\ny = x**2\\n# step2 设置绘图样式，这⼀模块的扩展参考第五章进⼀步学习，这⼀步不是必须的，样式也可以在绘制图像是进⾏\\n设置\\nmpl.rc(\\'lines\\', linewidth=4, linestyle=\\'-.\\')\\n# step3 定义布局，  这⼀模块的扩展参考第三章进⼀步学习\\nfig, ax = plt.subplots()  \\n# step4 绘制图像，  这⼀模块的扩展参考第⼆章进⼀步学习\\nax.plot(x, y, label=\\'linear\\')  \\n# step5 添加标签，⽂字和图例，这⼀模块的扩展参考第四章进⼀步学习\\nax.set_xlabel(\\'x label\\') \\nax.set_ylabel(\\'y label\\') \\nax.set_title(\"Simple Plot\")  \\nax.legend() ;\\n思考题\\n请思考两种绘图模式的优缺点和各⾃适合的使⽤场景\\n在第五节绘图模板中我们是以 OO 模式作为例⼦展示的，请思考并写⼀个 pyplot 绘图模式的简单模板', metadata={'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf', 'page': 2}), Document(page_content='第⼀回：Matplotlib 初相识\\n⼀、认识matplotlib\\nMatplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种静态，动态，\\n交互式的图表。\\nMatplotlib 可⽤于 Python 脚本， Python 和 IPython Shell 、 Jupyter notebook ， Web 应⽤程序服务器和各种图形⽤户界⾯⼯具包等。\\nMatplotlib 是 Python 数据可视化库中的泰⽃，它已经成为 python 中公认的数据可视化⼯具，我们所熟知的 pandas 和 seaborn 的绘图接⼝\\n其实也是基于 matplotlib 所作的⾼级封装。\\n为了对matplotlib 有更好的理解，让我们从⼀些最基本的概念开始认识它，再逐渐过渡到⼀些⾼级技巧中。\\n⼆、⼀个最简单的绘图例⼦\\nMatplotlib 的图像是画在 figure （如 windows ， jupyter 窗体）上的，每⼀个 figure ⼜包含了⼀个或多个 axes （⼀个可以指定坐标系的⼦区\\n域）。最简单的创建 figure 以及 axes 的⽅式是通过 pyplot.subplots命令，创建 axes 以后，可以使⽤ Axes.plot绘制最简易的折线图。\\nimport matplotlib.pyplot as plt\\nimport matplotlib as mpl\\nimport numpy as np\\nfig, ax = plt.subplots()  # 创建⼀个包含⼀个 axes 的 figure\\nax.plot([1, 2, 3, 4], [1, 4, 2, 3]);  # 绘制图像\\nTrick： 在jupyter notebook 中使⽤ matplotlib 时会发现，代码运⾏后⾃动打印出类似 <matplotlib.lines.Line2D at 0x23155916dc0>\\n这样⼀段话，这是因为 matplotlib 的绘图代码默认打印出最后⼀个对象。如果不想显示这句话，有以下三种⽅法，在本章节的代码示例\\n中你能找到这三种⽅法的使⽤。\\n\\x00. 在代码块最后加⼀个分号 ;\\n\\x00. 在代码块最后加⼀句 plt.show()\\n\\x00. 在绘图时将绘图对象显式赋值给⼀个变量，如将 plt.plot([1, 2, 3, 4]) 改成 line =plt.plot([1, 2, 3, 4])\\n和MATLAB 命令类似，你还可以通过⼀种更简单的⽅式绘制图像， matplotlib.pyplot⽅法能够直接在当前 axes 上绘制图像，如果⽤户\\n未指定axes ， matplotlib 会帮你⾃动创建⼀个。所以上⾯的例⼦也可以简化为以下这⼀⾏代码。\\nline =plt.plot([1, 2, 3, 4], [1, 4, 2, 3]) \\n三、Figure 的组成\\n现在我们来深⼊看⼀下 figure 的组成。通过⼀张 figure 解剖图，我们可以看到⼀个完整的 matplotlib 图像通常会包括以下四个层级，这些\\n层级也被称为容器（ container ），下⼀节会详细介绍。在 matplotlib 的世界中，我们将通过各种命令⽅法来操纵图像中的每⼀个部分，\\n从⽽达到数据可视化的最终效果，⼀副完整的图像实际上是各类⼦元素的集合。\\nFigure：顶层级，⽤来容纳所有绘图元素', metadata={'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf', 'page': 0}), Document(page_content='Axes：matplotlib 宇宙的核⼼，容纳了⼤量元素⽤来构造⼀幅幅⼦图，⼀个 figure 可以由⼀个或多个⼦图组成\\nAxis：axes的下属层级，⽤于处理所有和坐标轴，⽹格有关的元素\\nTick：axis的下属层级，⽤来处理所有和刻度有关的元素\\n四、两种绘图接⼝\\nmatplotlib 提供了两种最常⽤的绘图接⼝\\n\\x00. 显式创建figure 和 axes ，在上⾯调⽤绘图⽅法，也被称为 OO 模式（ object-oriented style)\\n\\x00. 依赖pyplot ⾃动创建 figure 和 axes ，并绘图\\n使⽤第⼀种绘图接⼝，是这样的：\\nx = np.linspace(0, 2, 100)\\nfig, ax = plt.subplots()  \\nax.plot(x, x, label=\\'linear\\')  \\nax.plot(x, x**2, label=\\'quadratic\\')  \\nax.plot(x, x**3, label=\\'cubic\\')  \\nax.set_xlabel(\\'x label\\') \\nax.set_ylabel(\\'y label\\') \\nax.set_title(\"Simple Plot\")  \\nax.legend() \\nplt.show()\\n⽽如果采⽤第⼆种绘图接⼝，绘制同样的图，代码是这样的：', metadata={'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf', 'page': 1}), Document(page_content='By Datawhale 数据可视化开源⼩组\\n© Copyright © Copyright 2021.x = np.linspace(0, 2, 100)\\nplt.plot(x, x, label=\\'linear\\') \\nplt.plot(x, x**2, label=\\'quadratic\\')  \\nplt.plot(x, x**3, label=\\'cubic\\')\\nplt.xlabel(\\'x label\\')\\nplt.ylabel(\\'y label\\')\\nplt.title(\"Simple Plot\")\\nplt.legend()\\nplt.show()\\n五、通⽤绘图模板\\n由于matplotlib 的知识点⾮常繁杂，在实际使⽤过程中也不可能将全部 API 都记住，很多时候都是边⽤边查。因此这⾥提供⼀个通⽤的绘\\n图基础模板，任何复杂的图表⼏乎都可以基于这个模板⻣架填充内容⽽成。初学者刚开始学习时只需要牢记这⼀模板就⾜以应对⼤部分\\n简单图表的绘制，在学习过程中可以将这个模板模块化，了解每个模块在做什么，在绘制复杂图表时如何修改，填充对应的模块。\\n# step1 准备数据\\nx = np.linspace(0, 2, 100)\\ny = x**2\\n# step2 设置绘图样式，这⼀模块的扩展参考第五章进⼀步学习，这⼀步不是必须的，样式也可以在绘制图像是进⾏\\n设置\\nmpl.rc(\\'lines\\', linewidth=4, linestyle=\\'-.\\')\\n# step3 定义布局，  这⼀模块的扩展参考第三章进⼀步学习\\nfig, ax = plt.subplots()  \\n# step4 绘制图像，  这⼀模块的扩展参考第⼆章进⼀步学习\\nax.plot(x, y, label=\\'linear\\')  \\n# step5 添加标签，⽂字和图例，这⼀模块的扩展参考第四章进⼀步学习\\nax.set_xlabel(\\'x label\\') \\nax.set_ylabel(\\'y label\\') \\nax.set_title(\"Simple Plot\")  \\nax.legend() ;\\n思考题\\n请思考两种绘图模式的优缺点和各⾃适合的使⽤场景\\n在第五节绘图模板中我们是以 OO 模式作为例⼦展示的，请思考并写⼀个 pyplot 绘图模式的简单模板', metadata={'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf', 'page': 2}), Document(page_content='第⼆回：艺术画笔⻅乾坤\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nimport matplotlib\\nimport matplotlib.pyplot as plt\\nfrom matplotlib.lines import Line2D   \\nfrom matplotlib.patches import Circle, Wedge\\nfrom matplotlib.collections import PatchCollection\\n⼀、概述\\n1. matplotlib 的三层 api\\nmatplotlib 的原理或者说基础逻辑是，⽤ Artist 对象在画布 (canvas) 上绘制 (Render) 图形。\\n就和⼈作画的步骤类似：\\n\\x00. 准备⼀块画布或画纸\\n\\x00. 准备好颜料、画笔等制图⼯具\\n\\x00. 作画\\n所以matplotlib 有三个层次的 API ：\\nmatplotlib.backend_bases.FigureCanvas 代表了绘图区，所有的图像都是在绘图区完成的\\nmatplotlib.backend_bases.Renderer 代表了渲染器，可以近似理解为画笔，控制如何在  FigureCanvas 上画图。\\nmatplotlib.artist.Artist 代表了具体的图表组件，即调⽤了 Renderer 的接⼝在 Canvas 上作图。\\n前两者处理程序和计算机的底层交互的事项，第三项 Artist 就是具体的调⽤接⼝来做出我们想要的图，⽐如图形、⽂本、线条的设定。\\n所以通常来说，我们95% 的时间，都是⽤来和 matplotlib.artist.Artist 类打交道的。\\n2. Artist的分类\\nArtist有两种类型： primitives 和containers。\\nprimitive是基本要素，它包含⼀些我们要在绘图区作图⽤到的标准图形对象，如 曲线Line2D ，⽂字 text ，矩形 Rectangle ，图像\\nimage等。\\ncontainer是容器，即⽤来装基本要素的地⽅，包括 图形figure 、坐标系 Axes 和坐标轴 Axis。他们之间的关系如下图所示：\\n分类\\n可视化中常⻅的 artist 类可以参考下图这张表格，解释下每⼀列的含义。\\n第⼀列表示 matplotlib 中⼦图上的辅助⽅法，可以理解为可视化中不同种类的图表类型，如柱状图，折线图，直⽅图等，这些图表都可\\n以⽤这些辅助⽅法直接画出来，属于更⾼层级的抽象。\\n第⼆列表示不同图表背后的 artist 类，⽐如折线图⽅法 plot在底层⽤到的就是 Line2D这⼀artist类。\\n第三列是第⼆列的列表容器，例如所有在⼦图中创建的 Line2D对象都会被⾃动收集到 ax.lines返回的列表中。\\n下⼀节的具体案例更清楚地阐释了这三者的关系，其实在很多时候，我们只⽤记住第⼀列的辅助⽅法进⾏绘图即可，⽽⽆需关注具体底\\n层使⽤了哪些类，但是了解底层类有助于我们绘制⼀些复杂的图表，因此也很有必要了解。', metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 0}), Document(page_content=\"Axes helper method Artist Container\\nbar - bar charts Rectangle ax.patches\\nerrorbar - error bar plotsLine2D and Rectangleax.lines and ax.patches\\nfill - shared area Polygon ax.patches\\nhist - histograms Rectangle ax.patches\\nimshow - image data AxesImage ax.images\\nplot - xy plots Line2D ax.lines\\nscatter - scatter chartsPolyCollection ax.collections\\n⼆、基本元素  - primitives\\n各容器中可能会包含多种 基本要素-primitives, 所以先介绍下 primitives ，再介绍容器。\\n本章重点介绍下  primitives 的⼏种类型： 曲线-Line2D ，矩形 -Rectangle ，多边形 -Polygon ，图像 -image\\n1. 2DLines\\n在matplotlib 中曲线的绘制，主要是通过类  matplotlib.lines.Line2D 来完成的。\\nmatplotlib 中线-line的含义：它表示的可以是连接所有顶点的实线样式，也可以是每个顶点的标记。此外，这条线也会受到绘画⻛格的\\n影响，⽐如，我们可以创建虚线种类的线。\\n它的构造函数：\\nclass matplotlib.lines.Line2D(xdata, ydata, linewidth=None, linestyle=None, color=None, marker=None,\\nmarkersize=None, markeredgewidth=None, markeredgecolor=None, markerfacecolor=None,\\nmarkerfacecoloralt='none', fillstyle=None, antialiased=None, dash_capstyle=None, solid_capstyle=None,\\ndash_joinstyle=None, solid_joinstyle=None, pickradius=5, drawstyle=None, markevery=None, **kwargs)\\n其中常⽤的的参数有：\\nxdata:需要绘制的 line 中点的在 x 轴上的取值，若忽略，则默认为 range(1,len(ydata)+1)\\nydata:需要绘制的 line 中点的在 y 轴上的取值\\nlinewidth:线条的宽度\\nlinestyle:线型\\ncolor:线条的颜⾊\\nmarker:点的标记，详细可参考 markers API\\nmarkersize:标记的size\\n其他详细参数可参考 Line2D官⽅⽂档\\na. 如何设置 Line2D 的属性\\n有三种⽅法可以⽤设置线的属性。\\n\\x00. 直接在plot() 函数中设置\\n\\x00. 通过获得线对象，对线对象进⾏设置\\n\\x00. 获得线属性，使⽤ setp() 函数设置\\n# 1) 直接在 plot() 函数中设置\\nx = range(0,5)\\ny = [2,5,7,8,10]\\nplt.plot(x,y, linewidth=10); # 设置线的粗细参数为 10\", metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 1}), Document(page_content=\"# 2) 通过获得线对象，对线对象进⾏设置\\nx = range(0,5)\\ny = [2,5,7,8,10]\\nline, = plt.plot(x, y, '-') # 这⾥等号坐标的 line, 是⼀个列表解包的操作，⽬的是获取 plt.plot 返回列\\n表中的Line2D 对象\\nline.set_antialiased(False); # 关闭抗锯⻮功能\\n# 3) 获得线属性，使⽤ setp() 函数设置\\nx = range(0,5)\\ny = [2,5,7,8,10]\\nlines = plt.plot(x, y)\\nplt.setp(lines, color='r', linewidth=10);\\nb. 如何绘制 lines\\n\\x00. 绘制直线line\\n\\x00. errorbar绘制误差折线图\\n介绍两种绘制直线 line 常⽤的⽅法 :\\nplot⽅法绘制\\nLine2D对象绘制\\n# 1. plot ⽅法绘制\\nx = range(0,5)\\ny1 = [2,5,7,8,10]\\ny2= [3,6,8,9,11]\\nfig,ax= plt.subplots()\\nax.plot(x,y1)\\nax.plot(x,y2)\\nprint(ax.lines); # 通过直接使⽤辅助⽅法画线，打印 ax.lines 后可以看到在 matplotlib 在底层创建了两个\\nLine2D对象\\n[<matplotlib.lines.Line2D object at 0x0000020C25E80940>, <matplotlib.lines.Line2D \\nobject at 0x0000020C25E80CD0>]\", metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 2}), Document(page_content=\"# 2. Line2D 对象绘制\\nx = range(0,5)\\ny1 = [2,5,7,8,10]\\ny2= [3,6,8,9,11]\\nfig,ax= plt.subplots()\\nlines = [Line2D(x, y1), Line2D(x, y2,color='orange')]  # 显式创建 Line2D 对象\\nfor line in lines:\\n    ax.add_line(line) # 使⽤add_line ⽅法将创建的 Line2D 添加到⼦图中\\nax.set_xlim(0,4)\\nax.set_ylim(2, 11);\\n2) errorbar 绘制误差折线图\\npyplot⾥有个专⻔绘制误差线的功能，通过 errorbar类实现，它的构造函数：\\nmatplotlib.pyplot.errorbar(x, y, yerr=None, xerr=None, fmt='', ecolor=None, elinewidth=None, capsize=None,\\nbarsabove=False, lolims=False, uplims=False, xlolims=False, xuplims=False, errorevery=1, capthick=None, *,\\ndata=None, **kwargs)\\n其中最主要的参数是前⼏个 :\\nx：需要绘制的 line 中点的在 x 轴上的取值\\ny：需要绘制的 line 中点的在 y 轴上的取值\\nyerr：指定y轴⽔平的误差\\nxerr：指定x轴⽔平的误差\\nfmt：指定折线图中某个点的颜⾊，形状，线条⻛格，例如 ‘co--ʼ\\necolor：指定error bar 的颜⾊\\nelinewidth：指定error bar 的线条宽度\\n绘制errorbar\\nfig = plt.figure()\\nx = np.arange(10)\\ny = 2.5 * np.sin(x / 20 * np.pi)\\nyerr = np.linspace(0.05, 0.2, 10)\\nplt.errorbar(x,y+3,yerr=yerr,fmt='o-',ecolor='r',elinewidth=2);\", metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 3}), Document(page_content=\"2. patches\\nmatplotlib.patches.Patch 类是⼆维图形类，并且它是众多⼆维图形的⽗类，它的所有⼦类⻅ matplotlib.patches API ，\\nPatch类的构造函数：\\nPatch(edgecolor=None, facecolor=None, color=None, linewidth=None, linestyle=None, antialiased=None,\\nhatch=None, fill=True, capstyle=None, joinstyle=None, **kwargs)\\n本⼩节重点讲述三种最常⻅的⼦类，矩形，多边形和楔形。\\na. Rectangle- 矩形\\nRectangle矩形类在官⽹中的定义是：  通过锚点 xy 及其宽度和⾼度⽣成。  Rectangle 本身的主要⽐较简单，即 xy 控制锚点， width 和\\nheight分别控制宽和⾼。它的构造函数：\\nclass matplotlib.patches.Rectangle(xy, width, height, angle=0.0, **kwargs)\\n在实际中最常⻅的矩形图是 hist直⽅图和bar条形图。\\n1) hist-直⽅图\\nmatplotlib.pyplot.hist(x,bins=None,range=None, density=None, bottom=None, histtype='bar', align='mid', log=False,\\ncolor=None, label=None, stacked=False, normed=None)\\n下⾯是⼀些常⽤的参数：\\nx: 数据集，最终的直⽅图将对数据集进⾏统计\\nbins: 统计的区间分布\\nrange: tuple, 显示的区间， range 在没有给出 bins 时⽣效\\ndensity: bool，默认为 false ，显示的是频数统计结果，为 True 则显示频率统计结果，这⾥需要注意，频率统计结果 = 区间数⽬ /( 总\\n数*区间宽度) ，和 normed 效果⼀致，官⽅推荐使⽤ density\\nhisttype: 可选{'bar', 'barstacked', 'step', 'stepfilled'} 之⼀，默认为 bar ，推荐使⽤默认配置， step 使⽤的是梯状， stepfilled 则会\\n对梯状内部进⾏填充，效果与 bar 类似\\nalign: 可选{'left', 'mid', 'right'} 之⼀，默认为 'mid' ，控制柱状图的⽔平分布， left 或者 right ，会有部分空⽩区域，推荐使⽤默认\\nlog: bool，默认 False, 即 y 坐标轴是否选择指数刻度\\nstacked: bool，默认为 False ，是否为堆积状图\\n# hist绘制直⽅图\\nx=np.random.randint(0,100,100) #⽣成[0-100) 之间的 100 个数据 , 即  数据集  \\nbins=np.arange(0,101,10) #设置连续的边界值，即直⽅图的分布区间 [0,10),[10,20)... \\nplt.hist(x,bins,color='fuchsia',alpha=0.5)#alpha设置透明度， 0 为完全透明  \\nplt.xlabel('scores') \\nplt.ylabel('count') \\nplt.xlim(0,100); #设置x轴分布范围  plt.show()\", metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 4}), Document(page_content=\"# Rectangle 矩形类绘制直⽅图\\ndf = pd.DataFrame(columns = ['data'])\\ndf.loc[:,'data'] = x\\ndf['fenzu'] = pd.cut(df['data'], bins=bins, right = False,include_lowest=True)\\ndf_cnt = df['fenzu'].value_counts().reset_index()\\ndf_cnt.loc[:,'mini'] = df_cnt['index'].astype(str).map(lambda x:re.findall('\\\\\\n[(.*)\\\\,',x)[0]).astype(int)\\ndf_cnt.loc[:,'maxi'] = df_cnt['index'].astype(str).map(lambda x:re.findall('\\\\,\\n(.*)\\\\)',x)[0]).astype(int)\\ndf_cnt.loc[:,'width'] = df_cnt['maxi']- df_cnt['mini']\\ndf_cnt.sort_values('mini',ascending = True,inplace = True)\\ndf_cnt.reset_index(inplace = True,drop = True)\\n#⽤Rectangle 把 hist 绘制出来\\nfig = plt.figure()\\nax1 = fig.add_subplot(111)\\nfor i in df_cnt.index:\\n    rect =  \\nplt.Rectangle((df_cnt.loc[i,'mini'],0),df_cnt.loc[i,'width'],df_cnt.loc[i,'fenzu'])\\n    ax1.add_patch(rect)\\nax1.set_xlim(0, 100)\\nax1.set_ylim(0, 16);\\n2) bar-柱状图\\nmatplotlib.pyplot.bar(left, height, alpha=1, width=0.8, color=, edgecolor=, label=, lw=3)\\n下⾯是⼀些常⽤的参数：\\nleft：x轴的位置序列，⼀般采⽤ range 函数产⽣⼀个序列，但是有时候可以是字符串\\nheight：y轴的数值序列，也就是柱形图的⾼度，⼀般就是我们需要展示的数据；\\nalpha：透明度，值越⼩越透明\\nwidth：为柱形图的宽度，⼀般这是为 0.8 即可；\\ncolor或facecolor：柱形图填充的颜⾊；\\nedgecolor：图形边缘颜⾊\\nlabel：解释每个图像代表的含义，这个参数是为 legend() 函数做铺垫的，表示该次 bar 的标签\\n有两种⽅式绘制柱状图\\nbar绘制柱状图\\nRectangle矩形类绘制柱状图\\n# bar绘制柱状图\\ny = range(1,17)\\nplt.bar(np.arange(16), y, alpha=0.5, width=0.5, color='yellow', edgecolor='red', \\nlabel='The First Bar', lw=3);\", metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 5}), Document(page_content='# Rectangle 矩形类绘制柱状图\\nfig = plt.figure()\\nax1 = fig.add_subplot(111)\\nfor i in range(1,17):\\n    rect =  plt.Rectangle((i+0.25,0),0.5,i)\\n    ax1.add_patch(rect)\\nax1.set_xlim(0, 16)\\nax1.set_ylim(0, 16);\\nb. Polygon- 多边形\\nmatplotlib.patches.Polygon 类是多边形类。它的构造函数：\\nclass matplotlib.patches.Polygon(xy, closed=True, **kwargs)\\nxy是⼀个N×2 的 numpy array ，为多边形的顶点。\\nclosed为True 则指定多边形将起点和终点重合从⽽显式关闭多边形。\\nmatplotlib.patches.Polygon 类中常⽤的是 fill 类，它是基于 xy 绘制⼀个填充的多边形，它的定义：\\nmatplotlib.pyplot.fill(*args, data=None, **kwargs)\\n参数说明 : 关于 x 、 y 和 color 的序列，其中 color 是可选的参数，每个多边形都是由其节点的 x 和 y 位置列表定义的，后⾯可以选择⼀个颜\\n⾊说明符。您可以通过提供多个 x 、 y 、 [ 颜⾊ ] 组来绘制多个多边形。\\n# ⽤fill 来绘制图形\\nx = np.linspace(0, 5 * np.pi, 1000) \\ny1 = np.sin(x)\\ny2 = np.sin(2 * x) \\nplt.fill(x, y1, color = \"g\", alpha = 0.3);\\nc. Wedge- 契形\\nmatplotlib.patches.Wedge 类是楔型类。其基类是 matplotlib.patches.Patch ，它的构造函数：\\nclass matplotlib.patches.Wedge(center, r, theta1, theta2, width=None, **kwargs)\\n⼀个Wedge- 楔型  是以坐标 x,y 为中⼼，半径为 r ，从 θ1 扫到 θ2( 单位是度 ) 。\\n如果宽度给定，则从内半径 r - 宽度到外半径 r 画出部分楔形。 wedge 中⽐较常⻅的是绘制饼状图。\\nmatplotlib.pyplot.pie 语法：', metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 6}), Document(page_content=\"matplotlib.pyplot.pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False,\\nlabeldistance=1.1, startangle=0, radius=1, counterclock=True, wedgeprops=None, textprops=None, center=0, 0,\\nframe=False, rotatelabels=False, *, normalize=None, data=None)\\n制作数据x 的饼图，每个楔⼦的⾯积⽤ x/sum(x) 表示。\\n其中最主要的参数是前 4 个：\\nx：楔型的形状，⼀维数组。\\nexplode：如果不是等于 None ，则是⼀个 len(x) 数组，它指定⽤于偏移每个楔形块的半径的分数。\\nlabels：⽤于指定每个楔型块的标记，取值是列表或为 None 。\\ncolors：饼图循环使⽤的颜⾊序列。如果取值为 None ，将使⽤当前活动循环中的颜⾊。\\nstartangle：饼状图开始的绘制的⻆度。\\n# pie绘制饼状图\\nlabels = 'Frogs', 'Hogs', 'Dogs', 'Logs'\\nsizes = [15, 30, 45, 10] \\nexplode = (0, 0.1, 0, 0) \\nfig1, ax1 = plt.subplots() \\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, \\nstartangle=90) \\nax1.axis('equal'); # Equal aspect ratio ensures that pie is drawn as a circle. \\n# wedge绘制饼图\\nfig = plt.figure(figsize=(5,5))\\nax1 = fig.add_subplot(111)\\ntheta1 = 0\\nsizes = [15, 30, 45, 10] \\npatches = []\\npatches += [\\n    Wedge((0.5, 0.5), .4, 0, 54),           \\n    Wedge((0.5, 0.5), .4, 54, 162),  \\n    Wedge((0.5, 0.5), .4, 162, 324),           \\n    Wedge((0.5, 0.5), .4, 324, 360),  \\n]\\ncolors = 100 * np.random.rand(len(patches))\\np = PatchCollection(patches, alpha=0.8)\\np.set_array(colors)\\nax1.add_collection(p);\\n3. collections\\ncollections类是⽤来绘制⼀组对象的集合， collections 有许多不同的⼦类，如 RegularPolyCollection, CircleCollection, Pathcollection,\\n分别对应不同的集合⼦类型。其中⽐较常⽤的就是散点图，它是属于 PathCollection ⼦类， scatter ⽅法提供了该类的封装，根据 x 与 y 绘\\n制不同⼤⼩或颜⾊标记的散点图。  它的构造⽅法：\", metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 7}), Document(page_content='分别对应不同的集合⼦类型。其中⽐较常⽤的就是散点图，它是属于 PathCollection ⼦类， scatter ⽅法提供了该类的封装，根据 x 与 y 绘\\n制不同⼤⼩或颜⾊标记的散点图。  它的构造⽅法：\\nAxes.scatter(self, x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None,\\nalpha=None, linewidths=None, verts=, edgecolors=None, *, plotnonfinite=False, data=None, **kwargs)', metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 7}), Document(page_content=\"其中最主要的参数是前 5 个：\\nx：数据点x 轴的位置\\ny：数据点y 轴的位置\\ns：尺⼨⼤⼩\\nc：可以是单个颜⾊格式的字符串，也可以是⼀系列颜⾊\\nmarker: 标记的类型\\n# ⽤scatter绘制散点图\\nx = [0,2,4,6,8,10] \\ny = [10]*len(x) \\ns = [20*2**n for n in range(len(x))] \\nplt.scatter(x,y,s=s) ;\\n4. images\\nimages是matplotlib 中绘制 image 图像的类，其中最常⽤的 imshow 可以根据数组绘制成图像，它的构造函数：\\nclass matplotlib.image.AxesImage(ax, cmap=None, norm=None, interpolation=None, origin=None, extent=None,\\nfilternorm=True, filterrad=4.0, resample=False, **kwargs)\\nimshow根据数组绘制图像\\nmatplotlib.pyplot.imshow(X, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None,\\nvmax=None, origin=None, extent=None, shape=, filternorm=1, filterrad=4.0, imlim=, resample=None, url=None, *,\\ndata=None, **kwargs ）\\n使⽤imshow 画图时⾸先需要传⼊⼀个数组，数组对应的是空间内的像素位置和像素点的值， interpolation 参数可以设置不同的差值⽅\\n法，具体效果如下。\\nmethods = [None, 'none', 'nearest', 'bilinear', 'bicubic', 'spline16',\\n           'spline36', 'hanning', 'hamming', 'hermite', 'kaiser', 'quadric',\\n           'catrom', 'gaussian', 'bessel', 'mitchell', 'sinc', 'lanczos']\\ngrid = np.random.rand(4, 4)\\nfig, axs = plt.subplots(nrows=3, ncols=6, figsize=(9, 6),\\n                        subplot_kw={'xticks': [], 'yticks': []})\\nfor ax, interp_method in zip(axs.flat, methods):\\n    ax.imshow(grid, interpolation=interp_method, cmap='viridis')\\n    ax.set_title(str(interp_method))\\nplt.tight_layout();\", metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 8}), Document(page_content='三、对象容器  - Object container\\n容器会包含⼀些 primitives，并且容器还有它⾃身的属性。\\n⽐如Axes Artist，它是⼀种容器，它包含了很多 primitives，⽐如Line2D，Text；同时，它也有⾃身的属性，⽐如 xscal，⽤来控制\\nX轴是linear还是log的。\\n1. Figure容器\\nmatplotlib.figure.Figure是Artist最顶层的 container对象容器，它包含了图表中的所有元素。⼀张图表的背景就是在\\nFigure.patch的⼀个矩形 Rectangle。\\n当我们向图表添加 Figure.add_subplot()或者Figure.add_axes()元素时，这些都会被添加到 Figure.axes列表中。\\nfig = plt.figure()\\nax1 = fig.add_subplot(211) # 作⼀幅2*1 的图，选择第 1 个⼦图\\nax2 = fig.add_axes([0.1, 0.1, 0.7, 0.3]) # 位置参数，四个数分别代表了\\n(left,bottom,width,height)\\nprint(ax1) \\nprint(fig.axes) # fig.axes 中包含了 subplot 和 axes 两个实例 , 刚刚添加的\\nAxesSubplot(0.125,0.536818;0.775x0.343182)\\n[<AxesSubplot:>, <Axes:>]\\n由于Figure维持了current axes，因此你不应该⼿动的从 Figure.axes列表中添加删除元素，⽽是要通过 Figure.add_subplot()、\\nFigure.add_axes()来添加元素，通过 Figure.delaxes()来删除元素。但是你可以迭代或者访问 Figure.axes中的Axes，然后修改这个\\nAxes的属性。\\n⽐如下⾯的遍历 axes ⾥的内容，并且添加⽹格线：\\nfig = plt.figure()\\nax1 = fig.add_subplot(211)\\nfor ax in fig.axes:\\n    ax.grid(True)\\nFigure也有它⾃⼰的 text、line 、 patch 、 image。你可以直接通过 add primitive语句直接添加。但是注意 Figure默认的坐标系是以像\\n素为单位，你可能需要转换成 figure 坐标系： (0,0) 表示左下点， (1,1) 表示右上点。\\nFigure容器的常⻅属性：\\nFigure.patch属性：Figure 的背景矩形\\nFigure.axes属性：⼀个 Axes 实例的列表（包括 Subplot)\\nFigure.images属性：⼀个 FigureImages patch 列表\\nFigure.lines属性：⼀个 Line2D 实例的列表（很少使⽤）\\nFigure.legends属性：⼀个 Figure Legend 实例列表（不同于 Axes.legends)\\nFigure.texts属性：⼀个 Figure Text 实例列表', metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 9}), Document(page_content=\"2. Axes容器\\nmatplotlib.axes.Axes是matplotlib 的核⼼。⼤量的⽤于绘图的 Artist存放在它内部，并且它有许多辅助⽅法来创建和添加 Artist给它\\n⾃⼰，⽽且它也有许多赋值⽅法来访问和修改这些 Artist。\\n和Figure容器类似， Axes包含了⼀个 patch 属性，对于笛卡尔坐标系⽽⾔，它是⼀个 Rectangle；对于极坐标⽽⾔，它是⼀个 Circle。\\n这个patch 属性决定了绘图区域的形状、背景和边框。\\nfig = plt.figure()\\nax = fig.add_subplot(111)\\nrect = ax.patch  # axes的 patch 是⼀个 Rectangle 实例\\nrect.set_facecolor('green')\\nAxes有许多⽅法⽤于绘图，如 .plot() 、 .text() 、 .hist() 、 .imshow()等⽅法⽤于创建⼤多数常⻅的 primitive(如Line2D，\\nRectangle ， Text ， Image等等）。在 primitives中已经涉及，不再赘述。\\nSubplot就是⼀个特殊的 Axes ，其实例是位于⽹格中某个区域的 Subplot 实例。其实你也可以在任意区域创建 Axes ，通过\\nFigure.add_axes([left,bottom,width,height]) 来创建⼀个任意区域的 Axes ，其中 left,bottom,width,height 都是 [0—1] 之间的浮点数，他\\n们代表了相对于 Figure 的坐标。\\n你不应该直接通过 Axes.lines和Axes.patches列表来添加图表。因为当创建或添加⼀个对象到图表中时， Axes会做许多⾃动化的⼯作 :\\n它会设置Artist 中 figure 和 axes 的属性，同时默认 Axes 的转换；\\n它也会检视 Artist 中的数据，来更新数据结构，这样数据范围和呈现⽅式可以根据作图范围⾃动调整。\\n你也可以使⽤ Axes 的辅助⽅法 .add_line()和.add_patch()⽅法来直接添加。\\n另外Axes 还包含两个最重要的 Artist container ：\\nax.xaxis：XAxis对象的实例，⽤于处理 x 轴 tick 以及 label 的绘制\\nax.yaxis：YAxis对象的实例，⽤于处理 y 轴 tick 以及 label 的绘制\\n会在下⾯章节详细说明。\\nAxes容器的常⻅属性有：\\nartists: Artist实例列表\\npatch: Axes所在的矩形实例\\ncollections: Collection 实例\\nimages: Axes图像\\nlegends: Legend 实例\\nlines: Line2D 实例\\npatches: Patch 实例\\ntexts: Text 实例\\nxaxis: matplotlib.axis.XAxis 实例\\nyaxis: matplotlib.axis.YAxis 实例\\n3. Axis容器\\nmatplotlib.axis.Axis实例处理 tick line、grid line、tick label以及axis label的绘制，它包括坐标轴上的刻度线、刻度\\nlabel、坐标⽹格、坐标轴标题。通常你可以独⽴的配置 y 轴的左边刻度以及右边的刻度，也可以独⽴地配置 x 轴的上边刻度以及下边的\\n刻度。\\n刻度包括主刻度和次刻度，它们都是 Tick 刻度对象。\", metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 10}), Document(page_content='label、坐标⽹格、坐标轴标题。通常你可以独⽴的配置 y 轴的左边刻度以及右边的刻度，也可以独⽴地配置 x 轴的上边刻度以及下边的\\n刻度。\\n刻度包括主刻度和次刻度，它们都是 Tick 刻度对象。\\nAxis也存储了⽤于⾃适应，平移以及缩放的 data_interval和view_interval。它还有Locator 实例和 Formatter 实例⽤于控制刻度线的\\n位置以及刻度 label 。\\n每个Axis 都有⼀个 label属性，也有主刻度列表和次刻度列表。这些 ticks是axis.XTick和axis.YTick实例，它们包含着 line\\nprimitive以及text primitive⽤来渲染刻度线以及刻度⽂本。', metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 10}), Document(page_content='刻度是动态创建的，只有在需要创建的时候才创建（⽐如缩放的时候）。 Axis 也提供了⼀些辅助⽅法来获取刻度⽂本、刻度线位置等\\n等：\\n常⻅的如下：\\n# 不⽤print ，直接显示结果\\nfrom IPython.core.interactiveshell import InteractiveShell\\nInteractiveShell.ast_node_interactivity = \"all\"\\nfig, ax = plt.subplots()\\nx = range(0,5)\\ny = [2,5,7,8,10]\\nplt.plot(x, y, \\'-\\')\\naxis = ax.xaxis # axis为 X 轴对象\\naxis.get_ticklocs()     # 获取刻度线位置\\naxis.get_ticklabels()   # 获取刻度 label 列表 ( ⼀个 Text 实例的列表）。  可以通过 minor=True|False\\n关键字参数控制输出 minor 还是 major 的 tick label 。\\naxis.get_ticklines()    # 获取刻度线列表 ( ⼀个 Line2D 实例的列表）。  可以通过 minor=True|False 关\\n键字参数控制输出 minor 还是 major 的 tick line 。\\naxis.get_data_interval()# 获取轴刻度间隔\\naxis.get_view_interval()# 获取轴视⻆（位置）的间隔\\narray([-0.2,  4.2])\\n下⾯的例⼦展示了如何调整⼀些轴和刻度的属性 ( 忽略美观度，仅作调整参考 ) ：\\nfig = plt.figure() # 创建⼀个新图表\\nrect = fig.patch   # 矩形实例并将其设为⻩⾊\\nrect.set_facecolor(\\'lightgoldenrodyellow\\')\\nax1 = fig.add_axes([0.1, 0.3, 0.4, 0.4]) # 创⼀个axes 对象，从 (0.1,0.3) 的位置开始，宽和⾼都为\\n0.4，\\nrect = ax1.patch   # ax1的矩形设为灰⾊\\nrect.set_facecolor(\\'lightslategray\\')\\nfor label in ax1.xaxis.get_ticklabels(): \\n    # 调⽤x 轴刻度标签实例，是⼀个 text 实例\\n    label.set_color(\\'red\\') # 颜⾊\\n    label.set_rotation(45) # 旋转⻆度\\n    label.set_fontsize(16) # 字体⼤⼩\\nfor line in ax1.yaxis.get_ticklines():\\n    # 调⽤y 轴刻度线条实例 , 是⼀个 Line2D 实例\\n    line.set_markeredgecolor(\\'green\\')    # 颜⾊\\n    line.set_markersize(25)    # marker ⼤⼩\\n    line.set_markeredgewidth(2)# marker粗细\\n4. Tick容器\\nmatplotlib.axis.Tick是从Figure到Axes到Axis到Tick中最末端的容器对象。\\nTick包含了tick、grid line实例以及对应的 label。\\n所有的这些都可以通过 Tick的属性获取，常⻅的 tick属性有', metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 11}), Document(page_content='4. Tick容器\\nmatplotlib.axis.Tick是从Figure到Axes到Axis到Tick中最末端的容器对象。\\nTick包含了tick、grid line实例以及对应的 label。\\n所有的这些都可以通过 Tick的属性获取，常⻅的 tick属性有\\nTick.tick1line：Line2D 实例\\nTick.tick2line：Line2D 实例\\nTick.gridline：Line2D 实例\\nTick.label1：Text实例\\nTick.label2：Text实例', metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 11}), Document(page_content=\"By Datawhale 数据可视化开源⼩组\\n© Copyright © Copyright 2021.y轴分为左右两个，因此 tick1 对应左侧的轴； tick2 对应右侧的轴。\\nx轴分为上下两个，因此 tick1 对应下侧的轴； tick2 对应上侧的轴。\\n下⾯的例⼦展示了，如何将 Y 轴右边轴设为主轴，并将标签设置为美元符号且为绿⾊：\\nfig, ax = plt.subplots()\\nax.plot(100*np.random.rand(20))\\n# 设置ticker 的显示格式\\nformatter = matplotlib.ticker.FormatStrFormatter('$%1.2f')\\nax.yaxis.set_major_formatter(formatter)\\n# 设置ticker 的参数，右侧为主轴，颜⾊为绿⾊\\nax.yaxis.set_tick_params(which='major', labelcolor='green',\\n                         labelleft=False, labelright=True);\\n思考题\\nprimitives 和  container 的区别和联系是什么，分别⽤于控制可视化图表中的哪些要素\\n使⽤提供的 drug 数据集，对第⼀列 yyyy 和第⼆列 state 分组求和，画出下⾯折线图。 PA 加粗标⻩，其他为灰⾊。\\n图标题和横纵坐标轴标题，以及线的⽂本暂不做要求。\\n分别⽤⼀组⻓⽅形柱和填充⾯积的⽅式模仿画出下图，函数  y = -1 * (x - 2) * (x - 8) +10 在区间 [2,9] 的积分⾯积\\n参考资料\\n1. matplotlib 设计的基本逻辑\\n2. AI算法⼯程师⼿册\", metadata={'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf', 'page': 12}), Document(page_content=\"第三回：布局格式定⽅圆\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nplt.rcParams['font.sans-serif'] = ['SimHei']   #⽤来正常显示中⽂标签\\nplt.rcParams['axes.unicode_minus'] = False   #⽤来正常显示负号\\n⼀、⼦图\\n1. 使⽤ plt.subplots  绘制均匀状态下的⼦图\\n返回元素分别是画布和⼦图构成的列表，第⼀个数字为⾏，第⼆个为列，不传⼊时默认值都为 1\\nfigsize 参数可以指定整个画布的⼤⼩\\nsharex 和 sharey 分别表示是否共享横轴和纵轴刻度\\ntight_layout 函数可以调整⼦图的相对⼤⼩使字符不会重叠\\nfig, axs = plt.subplots(2, 5, figsize=(10, 4), sharex=True, sharey=True)\\nfig.suptitle('样例1', size=20)\\nfor i in range(2):\\n    for j in range(5):\\n        axs[i][j].scatter(np.random.randn(10), np.random.randn(10))\\n        axs[i][j].set_title('第%d⾏，第%d列'%(i+1,j+1))\\n        axs[i][j].set_xlim(-5,5)\\n        axs[i][j].set_ylim(-5,5)\\n        if i==1: axs[i][j].set_xlabel('横坐标')\\n        if j==0: axs[i][j].set_ylabel('纵坐标')\\nfig.tight_layout()\\nsubplots是基于OO 模式的写法，显式创建⼀个或多个 axes 对象，然后在对应的⼦图对象上进⾏绘图操作。\\n还有种⽅式是使⽤ subplot这样基于pyplot 模式的写法，每次在指定位置新建⼀个⼦图，并且之后的绘图操作都会指向当前⼦图，本质\\n上subplot也是Figure.add_subplot的⼀种封装。\\n在调⽤subplot时⼀般需要传⼊三位数字，分别代表总⾏数，总列数，当前⼦图的 index\\nplt.figure()\\n# ⼦图1\\nplt.subplot(2,2,1) \\nplt.plot([1,2], 'r')\\n# ⼦图2\\nplt.subplot(2,2,2)\\nplt.plot([1,2], 'b')\\n#⼦图3\\nplt.subplot(224)  # 当三位数都⼩于 10 时，可以省略中间的逗号，这⾏命令等价于 plt.subplot(2,2,4) \\nplt.plot([1,2], 'g');\", metadata={'source': 'docs/matplotlib/第三回：布局格式定方圆.pdf', 'page': 0}), Document(page_content=\"除了常规的直⻆坐标系，也可以通过 projection⽅法创建极坐标系下的图表\\nN = 150\\nr = 2 * np.random.rand(N)\\ntheta = 2 * np.pi * np.random.rand(N)\\narea = 200 * r**2\\ncolors = theta\\nplt.subplot(projection='polar')\\nplt.scatter(theta, r, c=colors, s=area, cmap='hsv', alpha=0.75);\\n请思考如何⽤极坐标系画出类似的玫瑰图\\n2. 使⽤ GridSpec  绘制⾮均匀⼦图\\n所谓⾮均匀包含两层含义，第⼀是指图的⽐例⼤⼩不同但没有跨⾏或跨列，第⼆是指图为跨列或跨⾏状态\\n利⽤ add_gridspec 可以指定相对宽度⽐例  width_ratios 和相对⾼度⽐例参数  height_ratios\\nfig = plt.figure(figsize=(10, 4))\\nspec = fig.add_gridspec(nrows=2, ncols=5, width_ratios=[1,2,3,4,5], height_ratios=\\n[1,3])\\nfig.suptitle('样例2', size=20)\\nfor i in range(2):\\n    for j in range(5):\\n        ax = fig.add_subplot(spec[i, j])\\n        ax.scatter(np.random.randn(10), np.random.randn(10))\\n        ax.set_title('第%d⾏，第%d列'%(i+1,j+1))\\n        if i==1: ax.set_xlabel('横坐标')\\n        if j==0: ax.set_ylabel('纵坐标')\\nfig.tight_layout()练⼀练\\uf05a\", metadata={'source': 'docs/matplotlib/第三回：布局格式定方圆.pdf', 'page': 1}), Document(page_content=\"在上⾯的例⼦中出现了  spec[i, j] 的⽤法，事实上通过切⽚就可以实现⼦图的合并⽽达到跨图的共能\\nfig = plt.figure(figsize=(10, 4))\\nspec = fig.add_gridspec(nrows=2, ncols=6, width_ratios=[2,2.5,3,1,1.5,2], \\nheight_ratios=[1,2])\\nfig.suptitle('样例3', size=20)\\n# sub1\\nax = fig.add_subplot(spec[0, :3])\\nax.scatter(np.random.randn(10), np.random.randn(10))\\n# sub2\\nax = fig.add_subplot(spec[0, 3:5])\\nax.scatter(np.random.randn(10), np.random.randn(10))\\n# sub3\\nax = fig.add_subplot(spec[:, 5])\\nax.scatter(np.random.randn(10), np.random.randn(10))\\n# sub4\\nax = fig.add_subplot(spec[1, 0])\\nax.scatter(np.random.randn(10), np.random.randn(10))\\n# sub5\\nax = fig.add_subplot(spec[1, 1:5])\\nax.scatter(np.random.randn(10), np.random.randn(10))\\nfig.tight_layout()\\n⼆、⼦图上的⽅法\\n补充介绍⼀些⼦图上的⽅法\\n常⽤直线的画法为：  axhline, axvline, axline （⽔平、垂直、任意⽅向）\\nfig, ax = plt.subplots(figsize=(4,3))\\nax.axhline(0.5,0.2,0.8)\\nax.axvline(0.5,0.2,0.8)\\nax.axline([0.3,0.3],[0.7,0.7]);\\n使⽤ grid 可以加灰⾊⽹格\", metadata={'source': 'docs/matplotlib/第三回：布局格式定方圆.pdf', 'page': 2}), Document(page_content=\"fig, ax = plt.subplots(figsize=(4,3))\\nax.grid(True)\\n使⽤ set_xscale 可以设置坐标轴的规度（指对数坐标等）\\nfig, axs = plt.subplots(1, 2, figsize=(10, 4))\\nfor j in range(2):\\n    axs[j].plot(list('abcd'), [10**i for i in range(4)])\\n    if j==0:\\n        axs[j].set_yscale('log')\\n    else:\\n        pass\\nfig.tight_layout()\\n思考题\\n墨尔本1981 年⾄ 1990 年的每⽉温度情况\\n数据集来⾃ github 仓库下 data/layout_ex1.csv\\n请利⽤数据，画出如下的图：\\n画出数据的散点图和边际分布\\n⽤ np.random.randn(2, 150) ⽣成⼀组⼆维数据，使⽤两种⾮均匀⼦图的分割⽅法，做出该数据对应的散点图和边际分布图\", metadata={'source': 'docs/matplotlib/第三回：布局格式定方圆.pdf', 'page': 3}), Document(page_content='By Datawhale 数据可视化开源⼩组\\n© Copyright © Copyright 2021.', metadata={'source': 'docs/matplotlib/第三回：布局格式定方圆.pdf', 'page': 4})]\n","page_content='第⼀回：Matplotlib 初相识\\n⼀、认识matplotlib\\nMatplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种静态，动态，\\n交互式的图表。\\nMatplotlib 可⽤于 Python 脚本， Python 和 IPython Shell 、 Jupyter notebook ， Web 应⽤程序服务器和各种图形⽤户界⾯⼯具包等。\\nMatplotlib 是 Python 数据可视化库中的泰⽃，它已经成为 python 中公认的数据可视化⼯具，我们所熟知的 pandas 和 seaborn 的绘图接⼝\\n其实也是基于 matplotlib 所作的⾼级封装。\\n为了对matplotlib 有更好的理解，让我们从⼀些最基本的概念开始认识它，再逐渐过渡到⼀些⾼级技巧中。\\n⼆、⼀个最简单的绘图例⼦\\nMatplotlib 的图像是画在 figure （如 windows ， jupyter 窗体）上的，每⼀个 figure ⼜包含了⼀个或多个 axes （⼀个可以指定坐标系的⼦区\\n域）。最简单的创建 figure 以及 axes 的⽅式是通过 pyplot.subplots命令，创建 axes 以后，可以使⽤ Axes.plot绘制最简易的折线图。\\nimport matplotlib.pyplot as plt\\nimport matplotlib as mpl\\nimport numpy as np\\nfig, ax = plt.subplots()  # 创建⼀个包含⼀个 axes 的 figure\\nax.plot([1, 2, 3, 4], [1, 4, 2, 3]);  # 绘制图像\\nTrick： 在jupyter notebook 中使⽤ matplotlib 时会发现，代码运⾏后⾃动打印出类似 <matplotlib.lines.Line2D at 0x23155916dc0>\\n这样⼀段话，这是因为 matplotlib 的绘图代码默认打印出最后⼀个对象。如果不想显示这句话，有以下三种⽅法，在本章节的代码示例\\n中你能找到这三种⽅法的使⽤。\\n\\x00. 在代码块最后加⼀个分号 ;\\n\\x00. 在代码块最后加⼀句 plt.show()\\n\\x00. 在绘图时将绘图对象显式赋值给⼀个变量，如将 plt.plot([1, 2, 3, 4]) 改成 line =plt.plot([1, 2, 3, 4])\\n和MATLAB 命令类似，你还可以通过⼀种更简单的⽅式绘制图像， matplotlib.pyplot⽅法能够直接在当前 axes 上绘制图像，如果⽤户\\n未指定axes ， matplotlib 会帮你⾃动创建⼀个。所以上⾯的例⼦也可以简化为以下这⼀⾏代码。\\nline =plt.plot([1, 2, 3, 4], [1, 4, 2, 3]) \\n三、Figure 的组成\\n现在我们来深⼊看⼀下 figure 的组成。通过⼀张 figure 解剖图，我们可以看到⼀个完整的 matplotlib 图像通常会包括以下四个层级，这些\\n层级也被称为容器（ container ），下⼀节会详细介绍。在 matplotlib 的世界中，我们将通过各种命令⽅法来操纵图像中的每⼀个部分，\\n从⽽达到数据可视化的最终效果，⼀副完整的图像实际上是各类⼦元素的集合。\\nFigure：顶层级，⽤来容纳所有绘图元素' metadata={'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf', 'page': 0}\n"]}],"source":["from langchain.document_loaders import PyPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","\n","# 加载 PDF\n","loaders = [\n","    # 故意添加重复文档，使数据混乱\n","    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n","    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture01.pdf\"),\n","    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture02.pdf\"),\n","    PyPDFLoader(\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\")\n","]\n","docs = []\n","for loader in loaders:\n","    docs.extend(loader.load())\n","    \n","\n","# 分割文本\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 1500,  # 每个文本块的大小。这意味着每次切分文本时，会尽量使每个块包含 1500 个字符。\n","    chunk_overlap = 150  # 每个文本块之间的重叠部分。\n",")\n","\n","splitted_doc = text_splitter.split_documents(docs)\n","\n","\n","print(len(splitted_doc))\n","print(splitted_doc)\n","print(splitted_doc[0])\n","    \n","    \n","# 下面文档是datawhale官方开源的matplotlib教程链接 https://datawhalechina.github.io/fantastic-matplotlib/index.html ，可在该网站上下载对应的教程\n","# 加载 PDF\n","loaders_chinese = [\n","    # 故意添加重复文档，使数据混乱\n","    PyPDFLoader(\"docs/matplotlib/第一回：Matplotlib初相识.pdf\"),\n","    PyPDFLoader(\"docs/matplotlib/第一回：Matplotlib初相识.pdf\"),\n","    PyPDFLoader(\"docs/matplotlib/第二回：艺术画笔见乾坤.pdf\"),\n","    PyPDFLoader(\"docs/matplotlib/第三回：布局格式定方圆.pdf\")\n","]\n","docs_chinese = []\n","for loader in loaders_chinese:\n","    docs_chinese.extend(loader.load())\n","\n","# 分割文本\n","chinese_text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 1500,  # 每个文本块的大小。这意味着每次切分文本时，会尽量使每个块包含 1500 个字符。\n","    chunk_overlap = 150  # 每个文本块之间的重叠部分。\n",")\n","splitted_chinese = chinese_text_splitter.split_documents(docs_chinese)\n","\n","# 在文档加载后，使用`RecursiveCharacterTextSplitter`(递归字符文本拆分器)来创建块。\n","print(len(splitted_chinese))\n","print(splitted_chinese)\n","print(splitted_chinese[0])"]},{"cell_type":"markdown","id":"848e26fd","metadata":{},"source":["## 三、Embeddings\n","\n","什么是`Embeddings`？\n","\n","在机器学习和自然语言处理（NLP）中，`Embeddings`（嵌入）是一种将类别数据，如单词、句子或者整个文档，转化为实数向量的技术。这些实数向量可以被计算机更好地理解和处理。嵌入背后的主要想法是，相似或相关的对象在嵌入空间中的距离应该很近。\n","\n","举个例子，我们可以使用词嵌入（word embeddings）来表示文本数据。在词嵌入中，每个单词被转换为一个向量，这个向量捕获了这个单词的语义信息。例如，\"king\" 和 \"queen\" 这两个单词在嵌入空间中的位置将会非常接近，因为它们的含义相似。而 \"apple\" 和 \"orange\" 也会很接近，因为它们都是水果。而 \"king\" 和 \"apple\" 这两个单词在嵌入空间中的距离就会比较远，因为它们的含义不同。\n","\n","让我们取出我们的切分部分并对它们进行`Embedding`处理。\n","\n","在使用真实文档数据的例子之前，让我们用几个测试案例的句子来试试，以便了解`embedding`。\n","\n","下面有几个示例句子，其中前两个非常相似，第三个与之无关。然后我们可以使用`embedding`类为每个句子创建一个`embedding`。"]},{"cell_type":"code","execution_count":9,"id":"d9dca7a8","metadata":{"height":47,"tags":[]},"outputs":[],"source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","embedding_model = OpenAIEmbeddings()\n","\n","sentence1 = \"i like dogs\"\n","sentence2 = \"i like canines\"\n","sentence3 = \"the weather is ugly outside\"\n","\n","\n","\n","embedding1 = embedding_model.embed_query(sentence1)\n","embedding2 = embedding_model.embed_query(sentence2)\n","embedding3 = embedding_model.embed_query(sentence3)\n","\n","\n","sentence1_chinese = \"我喜欢狗\"\n","sentence2_chinese = \"我喜欢犬科动物\"\n","sentence3_chinese = \"外面的天气很糟糕\"\n","\n","\n","\n","embedding1_chinese = embedding_model.embed_query(sentence1_chinese)\n","embedding2_chinese = embedding_model.embed_query(sentence2_chinese)\n","embedding3_chinese = embedding_model.embed_query(sentence3_chinese)"]},{"cell_type":"markdown","id":"16c1130d","metadata":{},"source":["然后我们可以使用`numpy`来比较它们，看看哪些最相似。\n","\n","我们期望前两个句子应该非常相似。\n","\n","然后，第一和第二个与第三个相比应该相差很大。\n","\n","我们将使用点积来比较两个嵌入。\n","\n","如果你不知道什么是点积，没关系。你只需要知道的重要一点是，分数越高句子越相似。\n","\n","我们可以看到前两个`embedding`的分数相当高，为0.96。\n","\n","如果我们将第一个`embedding`与第三个`embedding`进行比较，我们可以看到它明显较低，约为0.77。\n","\n","如果我们将第二个`embedding`和第三个`embedding`进行比较，我们可以看到它的分数大约为0.75。"]},{"cell_type":"code","execution_count":10,"id":"abae34b5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.9631853782830291\n","0.7709172071955229\n","0.7595538365168842\n","\n","\n","\n","0.9440615296168726\n","0.7921870036878481\n","0.7804110665851218\n"]}],"source":["import numpy as np\n","\n","\n","print(np.dot(embedding1, embedding2))\n","print(np.dot(embedding1, embedding3))\n","print(np.dot(embedding2, embedding3))\n","print(\"\\n\\n\")\n","\n","\n","# 我们可以看到前两个`embedding`的分数相当高，为0.94。\n","# 如果我们将第一个`embedding`与第三个`embedding`进行比较，我们可以看到它明显较低，约为0.79。\n","# 如果我们将第二个`embedding`和第三个`embedding`进行比较，我们可以看到它的分数大约为0.78。\n","print(np.dot(embedding1_chinese, embedding2_chinese))\n","print(np.dot(embedding1_chinese, embedding3_chinese))\n","print(np.dot(embedding2_chinese, embedding3_chinese))"]},{"cell_type":"markdown","id":"4fc7b24f","metadata":{},"source":["## 四、Vectorstores"]},{"cell_type":"markdown","id":"54776973","metadata":{},"source":["### 4.1 初始化Chroma"]},{"cell_type":"markdown","id":"20916c7c","metadata":{},"source":["Langchain集成了超过30个不同的向量存储库。我们选择Chroma是因为它轻量级且数据存储在内存中，这使得它非常容易启动和开始使用。"]},{"cell_type":"code","execution_count":11,"id":"201e6afa","metadata":{"height":30,"tags":[]},"outputs":[],"source":["from langchain.vectorstores import Chroma\n","persist_directory = 'docs/chroma/cs229_lectures/'\n","persist_directory_chinese = 'docs/chroma/matplotlib/'\n","\n","# # 删除旧的数据库文件（如果文件夹中有文件的话），window电脑请手动删除\n","# !rm -rf './docs/chroma/cs229_lectures'  \n","# !rm -rf './docs/chroma/matplotlib'  # 删除旧的数据库文件（如果文件夹中有文件的话）"]},{"cell_type":"code","execution_count":12,"id":"690efd0a","metadata":{"height":98,"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["209\n","27\n"]}],"source":["vector_db = Chroma.from_documents(\n","    documents=splitted_doc,\n","    embedding=embedding_model,\n","    persist_directory=persist_directory  # 允许我们将persist_directory目录保存到磁盘上\n",")\n","\n","print(vector_db._collection.count())\n","\n","\n","vector_db_chinese = Chroma.from_documents(\n","    documents=splitted_chinese,\n","    embedding=embedding_model,\n","    persist_directory=persist_directory_chinese  # 允许我们将persist_directory目录保存到磁盘上\n",")\n","\n","print(vector_db_chinese._collection.count())\n","\n","# 我们可以看到英文版的长度也是209、中文版的长度也是30，这与我们之前的切分数量是一样的。现在让我们开始使用它。"]},{"cell_type":"markdown","id":"efca7589","metadata":{},"source":["### 4.2 相似性搜索(Similarity Search)"]},{"cell_type":"code","execution_count":13,"id":"3e20837d","metadata":{"height":30,"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["3\n","cs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So \n","rather than sending us email individually, if you send email to this account, it will \n","actually let us get back to you maximally quickly with answers to your questions.  \n","If you're asking questions about homework probl ems, please say in the subject line which \n","assignment and which question the email refers to, since that will also help us to route \n","your question to the appropriate TA or to me  appropriately and get the response back to \n","you quickly.  \n","Let's see. Skipping ahead — let's see — for homework, one midterm, one open and term \n","project. Notice on the honor code. So one thi ng that I think will help you to succeed and \n","do well in this class and even help you to enjoy this cla ss more is if you form a study \n","group.  \n","So start looking around where you' re sitting now or at the end of class today, mingle a \n","little bit and get to know your classmates. I strongly encourage you to form study groups \n","and sort of have a group of people to study with and have a group of your fellow students \n","to talk over these concepts with. You can also  post on the class news group if you want to \n","use that to try to form a study group.  \n","But some of the problems sets in this cla ss are reasonably difficult.  People that have \n","taken the class before may tell you they were very difficult. And just I bet it would be \n","more fun for you, and you'd probably have a be tter learning experience if you form a\n","\n","3\n","第⼀回：Matplotlib 初相识\n","⼀、认识matplotlib\n","Matplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种静态，动态，\n","交互式的图表。\n","Matplotlib 可⽤于 Python 脚本， Python 和 IPython Shell 、 Jupyter notebook ， Web 应⽤程序服务器和各种图形⽤户界⾯⼯具包等。\n","Matplotlib 是 Python 数据可视化库中的泰⽃，它已经成为 python 中公认的数据可视化⼯具，我们所熟知的 pandas 和 seaborn 的绘图接⼝\n","其实也是基于 matplotlib 所作的⾼级封装。\n","为了对matplotlib 有更好的理解，让我们从⼀些最基本的概念开始认识它，再逐渐过渡到⼀些⾼级技巧中。\n","⼆、⼀个最简单的绘图例⼦\n","Matplotlib 的图像是画在 figure （如 windows ， jupyter 窗体）上的，每⼀个 figure ⼜包含了⼀个或多个 axes （⼀个可以指定坐标系的⼦区\n","域）。最简单的创建 figure 以及 axes 的⽅式是通过 pyplot.subplots命令，创建 axes 以后，可以使⽤ Axes.plot绘制最简易的折线图。\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import numpy as np\n","fig, ax = plt.subplots()  # 创建⼀个包含⼀个 axes 的 figure\n","ax.plot([1, 2, 3, 4], [1, 4, 2, 3]);  # 绘制图像\n","Trick： 在jupyter notebook 中使⽤ matplotlib 时会发现，代码运⾏后⾃动打印出类似 <matplotlib.lines.Line2D at 0x23155916dc0>\n","这样⼀段话，这是因为 matplotlib 的绘图代码默认打印出最后⼀个对象。如果不想显示这句话，有以下三种⽅法，在本章节的代码示例\n","中你能找到这三种⽅法的使⽤。\n","\u0000. 在代码块最后加⼀个分号 ;\n","\u0000. 在代码块最后加⼀句 plt.show()\n","\u0000. 在绘图时将绘图对象显式赋值给⼀个变量，如将 plt.plot([1, 2, 3, 4]) 改成 line =plt.plot([1, 2, 3, 4])\n","和MATLAB 命令类似，你还可以通过⼀种更简单的⽅式绘制图像， matplotlib.pyplot⽅法能够直接在当前 axes 上绘制图像，如果⽤户\n","未指定axes ， matplotlib 会帮你⾃动创建⼀个。所以上⾯的例⼦也可以简化为以下这⼀⾏代码。\n","line =plt.plot([1, 2, 3, 4], [1, 4, 2, 3]) \n","三、Figure 的组成\n","现在我们来深⼊看⼀下 figure 的组成。通过⼀张 figure 解剖图，我们可以看到⼀个完整的 matplotlib 图像通常会包括以下四个层级，这些\n","层级也被称为容器（ container ），下⼀节会详细介绍。在 matplotlib 的世界中，我们将通过各种命令⽅法来操纵图像中的每⼀个部分，\n","从⽽达到数据可视化的最终效果，⼀副完整的图像实际上是各类⼦元素的集合。\n","Figure：顶层级，⽤来容纳所有绘图元素\n","\n"]}],"source":["question = \"is there an email i can ask for help\"  # \"有我可以寻求帮助的电子邮件吗\"\n","\n","\n","docs = vector_db.similarity_search(question,k=3)\n","\n","# 如果我们查看第一个文档的内容，我们可以看到它实际上是关于一个电子邮件地址，cs229-qa@cs.stanford.edu。\n","# 这是我们可以向其发送问题的电子邮件，所有的助教都会阅读这些邮件。\n","print(len(docs))\n","print(docs[0].page_content)\n","print()\n","\n","\n","# 如果我们查看第一个文档的内容，我们可以看到它实际上是关于Matplotlib的介绍\n","question_chinese = \"Matplotlib是什么？\" \n","docs_chinese = vector_db_chinese.similarity_search(question_chinese,k=3)\n","print(len(docs_chinese))\n","print(docs_chinese[0].page_content)\n","print()\n","\n","\n","# 在此之后，我们要确保通过运行vectordb.persist来持久化向量数据库，以便我们在未来的课程中使用。\n","\n","# 让我们保存它，以便以后使用！\n","vector_db.persist()\n","vector_db_chinese.persist()"]},{"cell_type":"markdown","id":"cefe9f6a","metadata":{},"source":["## 五、失败的情况(Failure modes)"]},{"cell_type":"markdown","id":"089da0ca","metadata":{},"source":["这看起来很好，基本的相似性搜索很容易就能让你完成80%的工作。\n","\n","但是，可能会出现一些相似性搜索失败的情况。\n","\n","这里有一些可能出现的边缘情况——我们将在下一堂课中修复它们。\n","\n","请注意，我们得到了重复的块（因为索引中有重复的 `MachineLearning-Lecture01.pdf`、`第一回：Matplotlib初相识.pdf`）。\n","\n","语义搜索获取所有相似的文档，但不强制多样性。\n","\n","`docs[0]` 和 `docs[1]` 是完全相同的，以及`docs_chinese[0]` 和 `docs_chinese[1]` 是完全相同的。s"]},{"cell_type":"code","execution_count":14,"id":"df0f29f9","metadata":{"height":30,"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your' metadata={'page': 8, 'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf'}\n","page_content='those homeworks will be done in either MATLA B or in Octave, which is sort of — I \\nknow some people call it a free ve rsion of MATLAB, which it sort  of is, sort of isn\\'t.  \\nSo I guess for those of you that haven\\'t s een MATLAB before, and I know most of you \\nhave, MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to \\nplot data. And it\\'s sort of an extremely easy to  learn tool to use for implementing a lot of \\nlearning algorithms.  \\nAnd in case some of you want to work on your  own home computer or something if you \\ndon\\'t have a MATLAB license, for the purposes of  this class, there\\'s also — [inaudible] \\nwrite that down [inaudible] MATLAB — there\\' s also a software package called Octave \\nthat you can download for free off the Internet. And it has somewhat fewer features than MATLAB, but it\\'s free, and for the purposes of  this class, it will work for just about \\neverything.  \\nSo actually I, well, so yeah, just a side comment for those of you that haven\\'t seen \\nMATLAB before I guess, once a colleague of mine at a different university, not at \\nStanford, actually teaches another machine l earning course. He\\'s taught it for many years. \\nSo one day, he was in his office, and an old student of his from, lik e, ten years ago came \\ninto his office and he said, \"Oh, professo r, professor, thank you so much for your' metadata={'page': 8, 'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf'}\n","\n","page_content='第⼀回：Matplotlib 初相识\\n⼀、认识matplotlib\\nMatplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种静态，动态，\\n交互式的图表。\\nMatplotlib 可⽤于 Python 脚本， Python 和 IPython Shell 、 Jupyter notebook ， Web 应⽤程序服务器和各种图形⽤户界⾯⼯具包等。\\nMatplotlib 是 Python 数据可视化库中的泰⽃，它已经成为 python 中公认的数据可视化⼯具，我们所熟知的 pandas 和 seaborn 的绘图接⼝\\n其实也是基于 matplotlib 所作的⾼级封装。\\n为了对matplotlib 有更好的理解，让我们从⼀些最基本的概念开始认识它，再逐渐过渡到⼀些⾼级技巧中。\\n⼆、⼀个最简单的绘图例⼦\\nMatplotlib 的图像是画在 figure （如 windows ， jupyter 窗体）上的，每⼀个 figure ⼜包含了⼀个或多个 axes （⼀个可以指定坐标系的⼦区\\n域）。最简单的创建 figure 以及 axes 的⽅式是通过 pyplot.subplots命令，创建 axes 以后，可以使⽤ Axes.plot绘制最简易的折线图。\\nimport matplotlib.pyplot as plt\\nimport matplotlib as mpl\\nimport numpy as np\\nfig, ax = plt.subplots()  # 创建⼀个包含⼀个 axes 的 figure\\nax.plot([1, 2, 3, 4], [1, 4, 2, 3]);  # 绘制图像\\nTrick： 在jupyter notebook 中使⽤ matplotlib 时会发现，代码运⾏后⾃动打印出类似 <matplotlib.lines.Line2D at 0x23155916dc0>\\n这样⼀段话，这是因为 matplotlib 的绘图代码默认打印出最后⼀个对象。如果不想显示这句话，有以下三种⽅法，在本章节的代码示例\\n中你能找到这三种⽅法的使⽤。\\n\\x00. 在代码块最后加⼀个分号 ;\\n\\x00. 在代码块最后加⼀句 plt.show()\\n\\x00. 在绘图时将绘图对象显式赋值给⼀个变量，如将 plt.plot([1, 2, 3, 4]) 改成 line =plt.plot([1, 2, 3, 4])\\n和MATLAB 命令类似，你还可以通过⼀种更简单的⽅式绘制图像， matplotlib.pyplot⽅法能够直接在当前 axes 上绘制图像，如果⽤户\\n未指定axes ， matplotlib 会帮你⾃动创建⼀个。所以上⾯的例⼦也可以简化为以下这⼀⾏代码。\\nline =plt.plot([1, 2, 3, 4], [1, 4, 2, 3]) \\n三、Figure 的组成\\n现在我们来深⼊看⼀下 figure 的组成。通过⼀张 figure 解剖图，我们可以看到⼀个完整的 matplotlib 图像通常会包括以下四个层级，这些\\n层级也被称为容器（ container ），下⼀节会详细介绍。在 matplotlib 的世界中，我们将通过各种命令⽅法来操纵图像中的每⼀个部分，\\n从⽽达到数据可视化的最终效果，⼀副完整的图像实际上是各类⼦元素的集合。\\nFigure：顶层级，⽤来容纳所有绘图元素' metadata={'page': 0, 'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf'}\n","page_content='第⼀回：Matplotlib 初相识\\n⼀、认识matplotlib\\nMatplotlib 是⼀个 Python 2D 绘图库，能够以多种硬拷⻉格式和跨平台的交互式环境⽣成出版物质量的图形，⽤来绘制各种静态，动态，\\n交互式的图表。\\nMatplotlib 可⽤于 Python 脚本， Python 和 IPython Shell 、 Jupyter notebook ， Web 应⽤程序服务器和各种图形⽤户界⾯⼯具包等。\\nMatplotlib 是 Python 数据可视化库中的泰⽃，它已经成为 python 中公认的数据可视化⼯具，我们所熟知的 pandas 和 seaborn 的绘图接⼝\\n其实也是基于 matplotlib 所作的⾼级封装。\\n为了对matplotlib 有更好的理解，让我们从⼀些最基本的概念开始认识它，再逐渐过渡到⼀些⾼级技巧中。\\n⼆、⼀个最简单的绘图例⼦\\nMatplotlib 的图像是画在 figure （如 windows ， jupyter 窗体）上的，每⼀个 figure ⼜包含了⼀个或多个 axes （⼀个可以指定坐标系的⼦区\\n域）。最简单的创建 figure 以及 axes 的⽅式是通过 pyplot.subplots命令，创建 axes 以后，可以使⽤ Axes.plot绘制最简易的折线图。\\nimport matplotlib.pyplot as plt\\nimport matplotlib as mpl\\nimport numpy as np\\nfig, ax = plt.subplots()  # 创建⼀个包含⼀个 axes 的 figure\\nax.plot([1, 2, 3, 4], [1, 4, 2, 3]);  # 绘制图像\\nTrick： 在jupyter notebook 中使⽤ matplotlib 时会发现，代码运⾏后⾃动打印出类似 <matplotlib.lines.Line2D at 0x23155916dc0>\\n这样⼀段话，这是因为 matplotlib 的绘图代码默认打印出最后⼀个对象。如果不想显示这句话，有以下三种⽅法，在本章节的代码示例\\n中你能找到这三种⽅法的使⽤。\\n\\x00. 在代码块最后加⼀个分号 ;\\n\\x00. 在代码块最后加⼀句 plt.show()\\n\\x00. 在绘图时将绘图对象显式赋值给⼀个变量，如将 plt.plot([1, 2, 3, 4]) 改成 line =plt.plot([1, 2, 3, 4])\\n和MATLAB 命令类似，你还可以通过⼀种更简单的⽅式绘制图像， matplotlib.pyplot⽅法能够直接在当前 axes 上绘制图像，如果⽤户\\n未指定axes ， matplotlib 会帮你⾃动创建⼀个。所以上⾯的例⼦也可以简化为以下这⼀⾏代码。\\nline =plt.plot([1, 2, 3, 4], [1, 4, 2, 3]) \\n三、Figure 的组成\\n现在我们来深⼊看⼀下 figure 的组成。通过⼀张 figure 解剖图，我们可以看到⼀个完整的 matplotlib 图像通常会包括以下四个层级，这些\\n层级也被称为容器（ container ），下⼀节会详细介绍。在 matplotlib 的世界中，我们将通过各种命令⽅法来操纵图像中的每⼀个部分，\\n从⽽达到数据可视化的最终效果，⼀副完整的图像实际上是各类⼦元素的集合。\\nFigure：顶层级，⽤来容纳所有绘图元素' metadata={'page': 0, 'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf'}\n","\n"]}],"source":["question = \"what did they say about matlab?\"  # \"他们对 matlab 有何评价？\"\n","docs = vector_db.similarity_search(question,k=5)\n","\n","print(docs[0])\n","print(docs[1])\n","print()\n","\n","question_chinese = \"Matplotlib是什么？\"\n","docs_chinese = vector_db_chinese.similarity_search(question_chinese,k=5)\n","\n","print(docs_chinese[0])\n","print(docs_chinese[1])\n","print()"]},{"cell_type":"markdown","id":"3a3a915d","metadata":{},"source":["我们可以看到一种新的失败的情况。\n","\n","下面的问题询问了关于第三讲的问题，但也包括了来自其他讲的结果。"]},{"cell_type":"code","execution_count":15,"id":"b19135e5","metadata":{"height":45,"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["{'page': 0, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n","{'page': 2, 'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf'}\n","{'page': 14, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n","{'page': 0, 'source': 'docs/cs229_lectures/MachineLearning-Lecture02.pdf'}\n","{'page': 6, 'source': 'docs/cs229_lectures/MachineLearning-Lecture03.pdf'}\n","really makes a difference between a good so lution and amazing solution. And to give \n","everyone to just how we do points assignments, or what is it that causes a solution to get \n","full marks, or just how to write amazing so lutions. Becoming a grad er is usually a good \n","way to do that.  \n","Graders are paid positions and you also get free  food, and it's usually fun for us to sort of \n","hang out for an evening and grade all the a ssignments. Okay, so I will send email. So \n","don't email me yet if you want to be a grader. I'll send email to the entire class later with \n","the administrative details and to solicit app lications. So you can email us back then, to \n","apply, if you'd be interested in being a grader.  \n","Okay, any questions about that? All right, okay, so let's get started with today's material. \n","So welcome back to the second lecture. What  I want to do today is talk about linear \n","regression, gradient descent, and the norma l equations. And I should also say, lecture \n","notes have been posted online and so if some  of the math I go over today, I go over rather \n","quickly, if you want to see every equation wr itten out and work through the details more \n","slowly yourself, go to the course homepage and download detailed lecture notes that \n","pretty much describe all the mathematical, te chnical contents I'm going to go over today.  \n","Today, I'm also going to delve into a fair amount  – some amount of linear algebra, and so\n","\n","{'page': 0, 'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf'}\n","{'page': 0, 'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf'}\n","{'page': 9, 'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf'}\n","{'page': 10, 'source': 'docs/matplotlib/第二回：艺术画笔见乾坤.pdf'}\n","{'page': 1, 'source': 'docs/matplotlib/第一回：Matplotlib初相识.pdf'}\n","三、对象容器  - Object container\n","容器会包含⼀些 primitives，并且容器还有它⾃身的属性。\n","⽐如Axes Artist，它是⼀种容器，它包含了很多 primitives，⽐如Line2D，Text；同时，它也有⾃身的属性，⽐如 xscal，⽤来控制\n","X轴是linear还是log的。\n","1. Figure容器\n","matplotlib.figure.Figure是Artist最顶层的 container对象容器，它包含了图表中的所有元素。⼀张图表的背景就是在\n","Figure.patch的⼀个矩形 Rectangle。\n","当我们向图表添加 Figure.add_subplot()或者Figure.add_axes()元素时，这些都会被添加到 Figure.axes列表中。\n","fig = plt.figure()\n","ax1 = fig.add_subplot(211) # 作⼀幅2*1 的图，选择第 1 个⼦图\n","ax2 = fig.add_axes([0.1, 0.1, 0.7, 0.3]) # 位置参数，四个数分别代表了\n","(left,bottom,width,height)\n","print(ax1) \n","print(fig.axes) # fig.axes 中包含了 subplot 和 axes 两个实例 , 刚刚添加的\n","AxesSubplot(0.125,0.536818;0.775x0.343182)\n","[<AxesSubplot:>, <Axes:>]\n","由于Figure维持了current axes，因此你不应该⼿动的从 Figure.axes列表中添加删除元素，⽽是要通过 Figure.add_subplot()、\n","Figure.add_axes()来添加元素，通过 Figure.delaxes()来删除元素。但是你可以迭代或者访问 Figure.axes中的Axes，然后修改这个\n","Axes的属性。\n","⽐如下⾯的遍历 axes ⾥的内容，并且添加⽹格线：\n","fig = plt.figure()\n","ax1 = fig.add_subplot(211)\n","for ax in fig.axes:\n","    ax.grid(True)\n","Figure也有它⾃⼰的 text、line 、 patch 、 image。你可以直接通过 add primitive语句直接添加。但是注意 Figure默认的坐标系是以像\n","素为单位，你可能需要转换成 figure 坐标系： (0,0) 表示左下点， (1,1) 表示右上点。\n","Figure容器的常⻅属性：\n","Figure.patch属性：Figure 的背景矩形\n","Figure.axes属性：⼀个 Axes 实例的列表（包括 Subplot)\n","Figure.images属性：⼀个 FigureImages patch 列表\n","Figure.lines属性：⼀个 Line2D 实例的列表（很少使⽤）\n","Figure.legends属性：⼀个 Figure Legend 实例列表（不同于 Axes.legends)\n","Figure.texts属性：⼀个 Figure Text 实例列表\n"]}],"source":["question = \"what did they say about regression in the third lecture?\"  # \"他们在第三讲中是怎么谈论回归的？\"\n","docs = vector_db.similarity_search(question,k=5)\n","for doc in docs:\n","    print(doc.metadata)\n","print(docs[3].page_content)\n","print()\n","\n","question_chinese = \"他们在第二讲中对Figure说了些什么？\" \n","docs_chinese = vector_db_chinese.similarity_search(question_chinese,k=5)\n","for doc_chinese in docs_chinese:\n","    print(doc_chinese.metadata)\n","print(docs_chinese[2].page_content)"]},{"cell_type":"markdown","id":"c3dbca56","metadata":{},"source":["在下一讲中讨论的方法可以用来解决这两个问题！"]}],"metadata":{"kernelspec":{"display_name":"auto-gpt","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
