{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarforever/LangChain-Tutorials/blob/main/LangChain_Expression_Language.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHpaChk4b5UN"
      },
      "source": [
        "# LangChain的新特性 - Expression Language\n",
        "\n",
        "`LangChain Expression Language` 是一种以声明式方法，轻松地将链或组件组合在一起的机制。通过利用管道操作符，构建的任何链将自动具有完整的同步、异步和流式支持。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAzbVA50cVt1"
      },
      "source": [
        "## Python管道 - Pipe\n",
        "\n",
        "`Python` 的 `Pipe` 提供了管道实现。请参考 [https://github.com/JulienPalard/Pipe](https://github.com/JulienPalard/Pipe)。 来看几个例子"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL8UBSkOkAVC"
      },
      "source": [
        "### 安装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzj4WvdHbeRn",
        "outputId": "8ccdd25b-9056-4002-9f70-daf8d3d8d8fa"
      },
      "outputs": [],
      "source": [
        "# !pip install -U pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cVz9DNDkEYr"
      },
      "source": [
        "### 最简单一个例子"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9a39Tepgy58",
        "outputId": "7119d313-7b21-42d4-ea61-866737ce02b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[4, 8]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pipe import select, where\n",
        "\n",
        "numbers = [1, 2, 3, 4, 5]\n",
        "result = list(numbers | where(lambda x: x % 2 == 0) | select(lambda x: x * 2))\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6hYiTkYkJ-L"
      },
      "source": [
        "### 小小进阶\n",
        "\n",
        "自定义管道 `uppercase` - 接受一个 `iterable` 参数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwhNhNzSkOHC",
        "outputId": "6a69c76c-ac24-49bc-e06f-cfb8c7d659d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['red', 'green', 'blue', 'yellow']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pipe import Pipe\n",
        "\n",
        "uppercase = Pipe(lambda iterable: (x.lower() for x in iterable))\n",
        "\n",
        "words = ['red', 'green', 'blue', 'YELLOW']\n",
        "\n",
        "uppercase_words = list(words | uppercase)\n",
        "\n",
        "uppercase_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c8vpUz3lSEt"
      },
      "source": [
        "## LangChain Expression Language与管道\n",
        "\n",
        "`LEL` 通过管道定义操作序列，帮助程序员以更加优雅简洁的编码方式构建功能逻辑。我们来看看如何通过表达式来重构几个经典的LangChain实例。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcAYqmcAmLiY"
      },
      "source": [
        "### 安装\n",
        "\n",
        "我们需要安装最新版本的 `langchain` 以确保具有 `LEL` 功能的支持。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QKFeM3t0mbQQ"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U langchain openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIiNbIYimmSa"
      },
      "source": [
        "### 提示词模版与模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xCEJ_9XEs6jA"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = '您的有效openai api key'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from langchain.llms import ChatGLM\n",
        "# from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "# template = \"\"\"{question}\"\"\"  \n",
        "# prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "# # llm = ChatGLM(endpoint_url=\"http://localhost:8000\") \n",
        "# llm = ChatGLM(endpoint_url=\"http://10.26.128.38:8595\")\n",
        "\n",
        "\n",
        "# llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "# question = \"What is the capital of China?\"\n",
        "# llm_chain.run(question)\n",
        "\n",
        "\n",
        "'''\n",
        "Author: DingWang wangding19@mails.ucas.ac.cn\n",
        "Date: 2023-08-04 09:52:47\n",
        "LastEditors: DingWang wangding19@mails.ucas.ac.cn\n",
        "LastEditTime: 2023-08-04 11:37:16\n",
        "FilePath: /ChatGLM2-6B/LLMForLangChain.py\n",
        "Description: \n",
        "\n",
        "Copyright (c) 2023 by ${git_name_email}, All Rights Reserved. \n",
        "'''\n",
        "import time\n",
        "import logging\n",
        "import requests\n",
        "from typing import Optional, List, Dict, Mapping, Any\n",
        "\n",
        "import langchain\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "# 启动llm的缓存\n",
        "langchain.llm_cache = InMemoryCache()\n",
        "\n",
        "class ChatGLM(LLM):\n",
        "    \n",
        "    # 模型服务url\n",
        "    url = \"http://10.26.128.38:8595/chat\"\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"chatglm\"\n",
        "\n",
        "    def _construct_query(self, prompt: str) -> Dict:\n",
        "        \"\"\"构造请求体\n",
        "        \"\"\"\n",
        "        query = {\n",
        "            \"human_input\": prompt\n",
        "        }\n",
        "        return query\n",
        "\n",
        "    @classmethod\n",
        "    def _post(cls, url: str,\n",
        "        query: Dict) -> Any:\n",
        "        \"\"\"POST请求\n",
        "        \"\"\"\n",
        "        _headers = {\"Content_Type\": \"application/json\"}\n",
        "        with requests.session() as sess:\n",
        "            resp = sess.post(url, \n",
        "                json=query, \n",
        "                headers=_headers, \n",
        "                timeout=60)\n",
        "        return resp\n",
        "\n",
        "    \n",
        "    def _call(self, prompt: str, \n",
        "        stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"_call\n",
        "        \"\"\"\n",
        "        # construct query\n",
        "        query = self._construct_query(prompt=prompt)\n",
        "\n",
        "        # post\n",
        "        resp = self._post(url=self.url,\n",
        "            query=query)\n",
        "        \n",
        "        if resp.status_code == 200:\n",
        "            resp_json = resp.json()\n",
        "            predictions = resp_json[\"response\"]\n",
        "            return predictions\n",
        "        else:\n",
        "            return f\"请求模型 in {resp.status_code}\" \n",
        "    \n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\n",
        "        \"\"\"\n",
        "        _param_dict = {\n",
        "            \"url\": self.url\n",
        "        }\n",
        "        return _param_dict\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "    \n",
        "#     # import \n",
        "    \n",
        "#     llm = ChatGLM()\n",
        "#     while True:\n",
        "#         human_input = input(\"Human: \")\n",
        "\n",
        "#         begin_time = time.time() * 1000\n",
        "#         # 请求模型\n",
        "#         response = llm(human_input, stop=[\"you\"])\n",
        "#         end_time = time.time() * 1000\n",
        "#         used_time = round(end_time - begin_time, 3)\n",
        "#         logging.info(f\"chatGLM process time: {used_time}ms\")\n",
        "\n",
        "#         print(f\"ChatGLM: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivCfgUk5rFn6"
      },
      "source": [
        "#### 提示词模板与模型的传统用法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "liG2Oc3ZrHWw",
        "outputId": "98b2fbb4-de33-4cbc-b1a7-4427af5f704a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The HEX code of color red is: #FF0000.'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    HumanMessage\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "# llm = \n",
        "\n",
        "human_template=\"Show me the HEX code of color {color_name}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([human_message_prompt])\n",
        "chain = LLMChain(llm=ChatGLM(), prompt=chat_prompt)\n",
        "\n",
        "chain.run(\"RED\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skH8OpgIrKP7"
      },
      "source": [
        "#### 通过 `LEL` 连接提示词模板与模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml1opV6bmq9z",
        "outputId": "b05b45af-5f6f-4f9a-f6ec-c1f40e9cee46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The HEX code of color red is: #FF0000.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatGLM()\n",
        "prompt = ChatPromptTemplate.from_template(\"Show me the HEX code of color {color_name}\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "chain.invoke({\"color_name\": \"RED\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuKN7l61uTRO"
      },
      "source": [
        "### 一个稍稍复杂的例子\n",
        "\n",
        "现在我们给刚才搭建的管道追加一些环节"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBJtYNrMwk8k"
      },
      "source": [
        "#### 添加标准输出解析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eEDRWAYxuaIj",
        "outputId": "814a655d-6aca-44f2-ca96-3b7e48882e33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The HEX code of color red is: #FF0000.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke({\"color_name\": \"RED\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2YheMkRxGrg"
      },
      "source": [
        "#### 添加函数调用\n",
        "\n",
        "我们来给管道中的模型添加一些函数调用。注，我们并不真正调用函数，只解析出函数调用的数据。\n",
        "\n",
        "`JsonOutputFunctionsParser` 用来将函数调用的回复解析为JSON格式，请参考[API 文档](https://api.python.langchain.com/en/latest/output_parsers/langchain.output_parsers.openai_functions.JsonKeyOutputFunctionsParser.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obpu_uLAyGS5",
        "outputId": "c8c45e4f-c159-4fe3-fd00-9d21b3b55f46"
      },
      "outputs": [
        {
          "ename": "OutputParserException",
          "evalue": "This output parser can only be used with a chat generation.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 25\u001b[0m\n\u001b[0;32m      3\u001b[0m functions \u001b[39m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     {\n\u001b[0;32m      5\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msave_color_code\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     }\n\u001b[0;32m     22\u001b[0m   ]\n\u001b[0;32m     23\u001b[0m chain \u001b[39m=\u001b[39m prompt \u001b[39m|\u001b[39m model\u001b[39m.\u001b[39mbind(function_call \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msave_color_code\u001b[39m\u001b[39m\"\u001b[39m}, functions \u001b[39m=\u001b[39m functions) \u001b[39m|\u001b[39m JsonOutputFunctionsParser()\n\u001b[1;32m---> 25\u001b[0m chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mcolor_name\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mRED\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\schema\\runnable.py:273\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps:\n\u001b[1;32m--> 273\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[0;32m    274\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    275\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m    276\u001b[0m             _patch_config(config, run_manager\u001b[39m.\u001b[39;49mget_child()),\n\u001b[0;32m    277\u001b[0m         )\n\u001b[0;32m    278\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\schema\\output_parser.py:47\u001b[0m, in \u001b[0;36mBaseGenerationOutputParser.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     39\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_result(\n\u001b[0;32m     40\u001b[0m             [ChatGeneration(message\u001b[39m=\u001b[39minner_input)]\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m     )\n\u001b[0;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_config(\n\u001b[0;32m     48\u001b[0m         \u001b[39mlambda\u001b[39;49;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_result([Generation(text\u001b[39m=\u001b[39;49minner_input)]),\n\u001b[0;32m     49\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m     50\u001b[0m         config,\n\u001b[0;32m     51\u001b[0m         run_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mparser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     52\u001b[0m     )\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\schema\\runnable.py:182\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type)\u001b[0m\n\u001b[0;32m    176\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    177\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    178\u001b[0m     \u001b[39minput\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39minput\u001b[39m},\n\u001b[0;32m    179\u001b[0m     run_type\u001b[39m=\u001b[39mrun_type,\n\u001b[0;32m    180\u001b[0m )\n\u001b[0;32m    181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     output \u001b[39m=\u001b[39m func(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    183\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    184\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\schema\\output_parser.py:48\u001b[0m, in \u001b[0;36mBaseGenerationOutputParser.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[0;32m     39\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_result(\n\u001b[0;32m     40\u001b[0m             [ChatGeneration(message\u001b[39m=\u001b[39minner_input)]\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m     )\n\u001b[0;32m     46\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_config(\n\u001b[1;32m---> 48\u001b[0m         \u001b[39mlambda\u001b[39;00m inner_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_result([Generation(text\u001b[39m=\u001b[39;49minner_input)]),\n\u001b[0;32m     49\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m     50\u001b[0m         config,\n\u001b[0;32m     51\u001b[0m         run_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparser\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m     )\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\output_parsers\\openai_functions.py:41\u001b[0m, in \u001b[0;36mJsonOutputFunctionsParser.parse_result\u001b[1;34m(self, result)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_result\u001b[39m(\u001b[39mself\u001b[39m, result: List[Generation]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m---> 41\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mparse_result(result)\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs_only:\n\u001b[0;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mloads(func)\n",
            "File \u001b[1;32md:\\Anaconda3\\envs\\LLM\\lib\\site-packages\\langchain\\output_parsers\\openai_functions.py:23\u001b[0m, in \u001b[0;36mOutputFunctionsParser.parse_result\u001b[1;34m(self, result)\u001b[0m\n\u001b[0;32m     21\u001b[0m generation \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mraise\u001b[39;00m OutputParserException(\n\u001b[0;32m     24\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis output parser can only be used with a chat generation.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     26\u001b[0m message \u001b[39m=\u001b[39m generation\u001b[39m.\u001b[39mmessage\n\u001b[0;32m     27\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "\u001b[1;31mOutputParserException\u001b[0m: This output parser can only be used with a chat generation."
          ]
        }
      ],
      "source": [
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "\n",
        "functions = [\n",
        "    {\n",
        "      \"name\": \"save_color_code\",\n",
        "      \"description\": \"Save the HEX code of color and its name\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"hex_code\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The HEX code of the color\"\n",
        "          },\n",
        "          \"color\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The color name\"\n",
        "          }\n",
        "        },\n",
        "        \"required\": [\"hex_code\", \"color\"]\n",
        "      }\n",
        "    }\n",
        "  ]\n",
        "chain = prompt | model.bind(function_call = {\"name\": \"save_color_code\"}, functions = functions) | JsonOutputFunctionsParser()\n",
        "\n",
        "chain.invoke({\"color_name\": \"RED\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNczSLtaIeZJPFolwCjTueW",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
